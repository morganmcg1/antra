{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v0.3 - Exp2 - AdaHessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/2 BASELINE, 42: 22.33, 0.4723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *\n",
    "from fastai2.callback.all import *\n",
    "from fastai2.basics import *\n",
    "import seaborn as sns\n",
    "import sacrebleu\n",
    "from nlp import load_dataset\n",
    "\n",
    "from einops import rearrange\n",
    "import gc\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/morgan/AdaHessian\" target=\"_blank\">https://app.wandb.ai/morgan/AdaHessian</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/morgan/AdaHessian/runs/ri2ud90n\" target=\"_blank\">https://app.wandb.ai/morgan/AdaHessian/runs/ri2ud90n</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/morgan/AdaHessian/runs/ri2ud90n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.wandb_config import ConfigError\n",
    "\n",
    "wandb.init(project='AdaHessian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.callback.wandb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data/irish/parallel_corpora/paracrawl')\n",
    "fn = 'para_crawl_huggingface_clean_v02_20200723.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired t-test approach\n",
    "\n",
    "https://forums.fast.ai/t/how-to-do-statistical-test-to-show-significant-improvement/76038/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42  # 1918, 139\n",
    "#seed=1918\n",
    "#seed=87\n",
    "\n",
    "def seed_everything(seed):\n",
    "    # Python\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    #pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    #numpy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # fastai\n",
    "    set_seed(seed)\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357399\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ga</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Among the French PIM , in 2013, it is only 9 islands that have been chiroptérologiques inventories .</td>\n",
       "      <td>I measc na PIM Fraince, i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Among the French PIM, in 2013, it is only 9 islands that have been chiroptérologiques inventories.</td>\n",
       "      <td>I measc na PIM Fraince , i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Among the French PIM, in 2013, it is only 9 islands that have been chiroptérologiques inventories.</td>\n",
       "      <td>I measc na PIM Fraince, i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As you can see, so get to show off the spacious shapes in 3D (red and blue).</td>\n",
       "      <td>Mar is féidir leat a fheiceáil, a fháil mar sin a thaispeáint as na cruthanna mhór i 3D (dearg agus gorm).</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Equation Solving – Traditional, simple</td>\n",
       "      <td>Ligningsløsning – Traidisiúnta, simplí</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     en  \\\n",
       "0  Among the French PIM , in 2013, it is only 9 islands that have been chiroptérologiques inventories .   \n",
       "1    Among the French PIM, in 2013, it is only 9 islands that have been chiroptérologiques inventories.   \n",
       "2    Among the French PIM, in 2013, it is only 9 islands that have been chiroptérologiques inventories.   \n",
       "3                          As you can see, so get to show off the spacious shapes in 3D (red and blue).   \n",
       "4                                                                Equation Solving – Traditional, simple   \n",
       "\n",
       "                                                                                                           ga  \\\n",
       "0                       I measc na PIM Fraince, i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail.   \n",
       "1                     I measc na PIM Fraince , i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail .   \n",
       "2                       I measc na PIM Fraince, i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail.   \n",
       "3  Mar is féidir leat a fheiceáil, a fháil mar sin a thaispeáint as na cruthanna mhór i 3D (dearg agus gorm).   \n",
       "4                                                                      Ligningsløsning – Traidisiúnta, simplí   \n",
       "\n",
       "   clean  \n",
       "0   True  \n",
       "1   True  \n",
       "2   True  \n",
       "3   True  \n",
       "4   True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(path/fn)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAISE BUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data/irish/parallel_corpora/paracrawl')\n",
    "fn = 'para_crawl_huggingface_clean_v02_20200723.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    }
   ],
   "source": [
    "# ds_dict = load_dataset('csv', data_files=str(path/fn),\n",
    "#                   description='en-ga Paracrawl data from HuggingFace, clean for suspect translations',\n",
    "#                       download_mode='force_redownload', version='0.0.2')\n",
    "ds_dict = load_dataset('csv', data_files=str(path/fn),\n",
    "                  description='en-ga Paracrawl data from HuggingFace, clean for suspect translations',\n",
    "                    version='0.0.1')\n",
    "ds=ds_dict['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that were identified as noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357399\n",
      "355837\n"
     ]
    }
   ],
   "source": [
    "def is_clean(example): return example['clean']\n",
    "print(len(ds))\n",
    "ds = ds.filter(is_clean)\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "**Remove long texts to make things easier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ga_len'] = df['ga'].str.split().str.len()\n",
    "# df['en_len'] = df['en'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sample lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(features: {'clean': Value(dtype='bool', id=None), 'en': Value(dtype='string', id=None), 'en_len': Value(dtype='int64', id=None), 'ga': Value(dtype='string', id=None), 'ga_len': Value(dtype='int64', id=None)}, num_rows: 355837)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_lens(example, lang):\n",
    "    example[f'{lang}_len'] = len(example[lang].split())\n",
    "    return example\n",
    "\n",
    "ds = ds.map(partial(get_lens, lang='ga'))\n",
    "ds = ds.map(partial(get_lens, lang='en'))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49.0, 47.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word count 90th percentile\n",
    "np.percentile([o for o in ds['ga_len']], 90), np.percentile([o for o in ds['en_len']], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.axes._subplots.AxesSubplot at 0x7f70f8309d90>, 18.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(ds['ga_len']), np.median(ds['ga_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.axes._subplots.AxesSubplot at 0x7f70e9f45e50>, 17.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(ds['en_len']), np.median(ds['en_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355837\n",
      "334244\n"
     ]
    }
   ],
   "source": [
    "def len_filter(example,col,l): \n",
    "    return example[col] <= l\n",
    "\n",
    "print(len(ds))\n",
    "ds = ds.filter(partial(len_filter, col='ga_len', l=60))\n",
    "ds = ds.filter(partial(len_filter, col='en_len', l=60))\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Removing {len(df.query(\"en_len > 60\"))} EN samples where len was > 60')\n",
    "# print(len(df))\n",
    "# df=df[~df.index.isin(df.query(\"en_len > 60\").index)]\n",
    "# print(len(df))\n",
    "      \n",
    "# print(f'Removing {len(df.query(\"ga_len > 60\"))} FR samples where len was > 60')\n",
    "# print(len(df))\n",
    "# df=df[~df.index.isin(df.query(\"ga_len > 60\").index)]\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.axes._subplots.AxesSubplot at 0x7f70e9dbe250>, 17.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(ds['ga_len']), np.median(ds['ga_len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lowercase everything**\n",
    "\n",
    "NOT Done as it is appliced in the rules below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['en'] = df['en'].apply(lambda x:x.lower())\n",
    "# df['ga'] = df['ga'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lowercase_all(example, lang):\n",
    "#     example[lang] = example[lang].lower()\n",
    "#     return example\n",
    "\n",
    "# ds = ds.map(partial(lowercase_all, lang='ga'))\n",
    "# ds = ds.map(partial(lowercase_all, lang='en'))\n",
    "# ds['ga'][400:420]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules used as part of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai2.text.core.fix_html(x)>,\n",
       " <function fastai2.text.core.replace_rep(t)>,\n",
       " <function fastai2.text.core.replace_wrep(t)>,\n",
       " <function fastai2.text.core.spec_add_spaces(t)>,\n",
       " <function fastai2.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai2.text.core.replace_all_caps(t)>,\n",
       " <function fastai2.text.core.replace_maj(t)>,\n",
       " functools.partial(<function lowercase at 0x7f712b6be8c0>, add_eos=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_rules=defaults.text_proc_rules[:-1] + [partial(lowercase, add_eos=True)]\n",
    "proc_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load vocab to speed up data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>en</th>\n",
       "      <th>en_len</th>\n",
       "      <th>ga</th>\n",
       "      <th>ga_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>Among the French PIM , in 2013, it is only 9 islands that have been chiroptérologiques inventories .</td>\n",
       "      <td>18</td>\n",
       "      <td>I measc na PIM Fraince, i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail.</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>Among the French PIM, in 2013, it is only 9 islands that have been chiroptérologiques inventories.</td>\n",
       "      <td>16</td>\n",
       "      <td>I measc na PIM Fraince , i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail .</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>Among the French PIM, in 2013, it is only 9 islands that have been chiroptérologiques inventories.</td>\n",
       "      <td>16</td>\n",
       "      <td>I measc na PIM Fraince, i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail.</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>As you can see, so get to show off the spacious shapes in 3D (red and blue).</td>\n",
       "      <td>17</td>\n",
       "      <td>Mar is féidir leat a fheiceáil, a fháil mar sin a thaispeáint as na cruthanna mhór i 3D (dearg agus gorm).</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>Equation Solving – Traditional, simple</td>\n",
       "      <td>5</td>\n",
       "      <td>Ligningsløsning – Traidisiúnta, simplí</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clean  \\\n",
       "0   True   \n",
       "1   True   \n",
       "2   True   \n",
       "3   True   \n",
       "4   True   \n",
       "\n",
       "                                                                                                     en  \\\n",
       "0  Among the French PIM , in 2013, it is only 9 islands that have been chiroptérologiques inventories .   \n",
       "1    Among the French PIM, in 2013, it is only 9 islands that have been chiroptérologiques inventories.   \n",
       "2    Among the French PIM, in 2013, it is only 9 islands that have been chiroptérologiques inventories.   \n",
       "3                          As you can see, so get to show off the spacious shapes in 3D (red and blue).   \n",
       "4                                                                Equation Solving – Traditional, simple   \n",
       "\n",
       "   en_len  \\\n",
       "0      18   \n",
       "1      16   \n",
       "2      16   \n",
       "3      17   \n",
       "4       5   \n",
       "\n",
       "                                                                                                           ga  \\\n",
       "0                       I measc na PIM Fraince, i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail.   \n",
       "1                     I measc na PIM Fraince , i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail .   \n",
       "2                       I measc na PIM Fraince, i 2013, tá sé ach 9 oileáin a bhí chiroptérologiques fardail.   \n",
       "3  Mar is féidir leat a fheiceáil, a fháil mar sin a thaispeáint as na cruthanna mhór i 3D (dearg agus gorm).   \n",
       "4                                                                      Ligningsløsning – Traidisiúnta, simplí   \n",
       "\n",
       "   ga_len  \n",
       "0      16  \n",
       "1      18  \n",
       "2      16  \n",
       "3      21  \n",
       "4       4  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=ds.data.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified Tokenizer.from_df to take a lang arguemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(Transform):\n",
    "    \"Provides a consistent `Transform` interface to tokenizers operating on `DataFrame`s and folders\"\n",
    "    input_types = (str, list, L, tuple, Path)\n",
    "    def __init__(self, tok, rules=None, counter=None, lengths=None, mode=None, sep=' '):\n",
    "        store_attr(self, 'tok,counter,lengths,mode,sep')\n",
    "        self.rules = defaults.text_proc_rules if rules is None else rules\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(tokenize_df, keep=True)\n",
    "    def from_df(cls, text_cols, lang='en', tok=None, rules=None, sep=' ', **kwargs):\n",
    "        if tok is None: tok = WordTokenizer(lang=lang)   # <--- MODIFIED HERE\n",
    "        res = cls(tok, rules=rules, mode='df')\n",
    "        res.kwargs,res.train_setup = merge({'tok': tok}, kwargs),False\n",
    "        res.text_cols,res.sep = text_cols,sep\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(tokenize_folder, keep=True)\n",
    "    def from_folder(cls, path, tok=None, rules=None, **kwargs):\n",
    "        path = Path(path)\n",
    "        if tok is None: tok = WordTokenizer()\n",
    "        output_dir = tokenize_folder(path, tok=tok, rules=rules, **kwargs)\n",
    "        res = cls(tok, counter=(output_dir/fn_counter_pkl).load(),\n",
    "                  lengths=(output_dir/fn_lengths_pkl).load(), rules=rules, mode='folder')\n",
    "        res.path,res.output_dir = path,output_dir\n",
    "        return res\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if not self.mode == 'df' or not isinstance(dsets.items, pd.DataFrame): return\n",
    "        dsets.items,count = tokenize_df(dsets.items, self.text_cols, rules=self.rules, **self.kwargs)\n",
    "        if self.counter is None: self.counter = count\n",
    "        return dsets\n",
    "\n",
    "    def encodes(self, o:Path):\n",
    "        if self.mode=='folder' and str(o).startswith(str(self.path)):\n",
    "            tok = self.output_dir/o.relative_to(self.path)\n",
    "            return L(tok.read().split(' '))\n",
    "        else: return self._tokenize1(o.read())\n",
    "\n",
    "    def encodes(self, o:str): return self._tokenize1(o)\n",
    "    def _tokenize1(self, o): return first(self.tok([compose(*self.rules)(o)]))\n",
    "\n",
    "    def get_lengths(self, items):\n",
    "        if self.lengths is None: return None\n",
    "        if self.mode == 'df':\n",
    "            if isinstance(items, pd.DataFrame) and 'text_lengths' in items.columns: return items['text_length'].values\n",
    "        if self.mode == 'folder':\n",
    "            try:\n",
    "                res = [self.lengths[str(Path(i).relative_to(self.path))] for i in items]\n",
    "                if len(res) == len(items): return res\n",
    "            except: return None\n",
    "\n",
    "    def decodes(self, o): return TitledStr(self.sep.join(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "# at 30k tokens per vocab sometimes this works, sometimes it doesn't\n",
    "\n",
    "# Couldnt process 30k tokens until I added the 'hi' below, it was getting stuck at 94.87%, no idea why\n",
    "@Numericalize\n",
    "def encodes(self, o): \n",
    "    print('hi')\n",
    "    return TensorText(tensor([self.o2i  [o_] for o_ in o]))\n",
    "\n",
    "class floatify_tfm(Transform):\n",
    "    def encodes(self,o): return o.float()\n",
    "    def decodes(self,o): return o.long()\n",
    "\n",
    "max_vocab=30000\n",
    "#splits = ColSplitter()(df) \n",
    "splits = RandomSplitter(valid_pct=0.2, seed=42)(df)\n",
    "\n",
    "tfms = [[Tokenizer.from_df(text_cols='en' , rules=proc_rules), attrgetter(\"text\"), Numericalize(max_vocab=max_vocab)], \n",
    "       [Tokenizer.from_df(text_cols='ga', lang='ga', rules=proc_rules), attrgetter(\"text\"), Numericalize(max_vocab=max_vocab)]]\n",
    "\n",
    "dl = partial(SortedDL, shuffle=True, res=df.ga_len.values)\n",
    "\n",
    "dsets = Datasets(df, tfms, splits=splits, dl_type=dl)\n",
    "\n",
    "# remove the print from Numericalize\n",
    "@Numericalize\n",
    "def encodes(self, o): return TensorText(tensor([self.o2i  [o_] for o_ in o]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_vocab=[]\n",
    "# ga_vocab=[]\n",
    "# with open(f'paracrawl_vocab_en_exp{exp}.csv', newline='') as csvfile:\n",
    "#     v_reader = csv.reader(csvfile, delimiter=',')\n",
    "#     for row in v_reader:\n",
    "#         en_vocab.append(row[0])\n",
    "        \n",
    "# with open(f'paracrawl_vocab_ga_exp{exp}.csv', newline='') as csvfile:\n",
    "#     v_reader = csv.reader(csvfile, delimiter=',')\n",
    "#     for row in v_reader:\n",
    "#         ga_vocab.append(row[0])\n",
    "        \n",
    "# len(en_vocab), len(ga_vocab), en_vocab[:10], ga_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,\n",
       " ((#800) [326,273,176,961,727,190,701,714,373,314...],\n",
       "  (#200) [542,618,816,68,94,215,60,585,942,165...]),\n",
       " 24,\n",
       " 23,\n",
       " (TensorText([  2,   8, 478,   9,   8, 381,   7, 479,  13,  16, 549,  13,  41,  29,\n",
       "          227, 107, 862,  35,  56,  66, 863, 864,  15,   3]),\n",
       "  TensorText([  2,  18, 510,  15,   7, 511,   8, 351,  10,  18, 581,  10,  65,  39,\n",
       "           75, 115, 891,   9, 105, 892, 893,  14,   3])))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsets), splits, len(dsets[2][0]), len(dsets[2][1]), dsets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos ( 1 ) xxmaj subject to section 131 of the xxmaj united xxmaj states xxmaj internal xxmaj revenue xxmaj code as in effect on the day on which this xxmaj convention shall have come into effect , xxmaj irish tax shall be allowed as a credit against xxmaj united xxmaj states tax . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos ( 1 ) xxmaj faoi réir alt 131 de xxmaj chód xxmaj ioncaim xxmaj intíre na xxmaj stát xxmaj aontaithe mar a bheidh éifeacht aige an lá a bheidh an xxmaj coinbhinsiún seo tar éis teacht in éifeacht , lamhálfar cáin éireannach mar chreidmheas i gcoinne cáin na xxmaj stát xxmaj aontaithe . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos \" i xxunk xxunk that my xxmaj department has been able to support this initiative and enable two xxunk national institutions to co - operate together to produce this xxunk . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos \" tá an - xxunk xxunk gur xxunk le mo xxmaj roinn tacú leis an xxunk seo agus go raibh ar xxunk dhá threoir - institiúid náisiúnta an taispeántas seo a dhéanamh i xxunk le chéile . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos \" it has been a xxunk to take part in the xxmaj xxunk to xxmaj xxunk xxunk under xxmaj xxunk xxmaj day for xxmaj europe xxunk . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos \" ba mhór an xxunk dom xxunk a ghlacadh sa xxunk xxmaj ar xxmaj ais ar xxmaj xxunk faoi xxunk xxup lá xxmaj xxunk na heorpa xxunk . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxmaj when a xxunk has 3 xxunk with the same size , xxunk xxunk for xxunk . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos xxmaj nuair a bhfuil xxunk 3 xxunk leis an méid céanna , xxunk den xxunk xxunk . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos ( 10 ) xxmaj sections 5 and 7 of the xxmaj xxunk xxmaj act , xxunk , shall apply to the determination of an appeal under this section as they apply to appeals under that xxmaj act . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos ( 10 ) xxmaj beidh feidhm ag ailt 5 agus 7 den xxmaj acht xxmaj xxunk , xxunk , maidir le cinneadh achomhairc faoin alt seo mar atá feidhm acu maidir le xxunk faoin xxmaj acht sin . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos ( 1 ) by the substitution for “ section xxunk of the xxmaj income xxmaj tax xxmaj act , 1967 ” of “ section xxunk ( 5 ) of the xxmaj corporation xxmaj tax xxmaj act , 1976 ” , and xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos ( 1 ) trí “ xxunk ( 5 ) den xxmaj acht xxmaj cánach xxmaj corparáide , 1976 ” a chur in ionad “ xxunk den xxmaj acht xxmaj cánach xxmaj ioncaim , 1967 ” , agus xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos \" processing and public access to xxunk documents \" xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos \" próiseáil agus rochtain phoiblí ar xxunk xxunk \" xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos ( 1 ) a person guilty of an offence under this xxmaj act shall be liable — xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos ( 1 ) xxmaj aon duine a bheidh ciontach i gcion faoin xxmaj acht seo , dlífear — xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos ( 1 ) xxmaj paragraph 3 is xxunk by the following xxunk : xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos ( 1 ) xxmaj xxunk an téacs seo a leanas in ionad mhír 3 : xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs,sl = 32, 512\n",
    "dls = dsets.dataloaders(bs=bs, seq_len=sl, before_batch=partial(pad_input, pad_fields=[0,1]))\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save vocab to speed up data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'models/paracrawl_vocab_en_v0.2_exp{exp}.csv', 'w', newline='') as csvfile:\n",
    "#     v_writer = csv.writer(csvfile, delimiter=',')\n",
    "#     for l in dls.vocab[0]:\n",
    "#         v_writer.writerow([l])\n",
    "        \n",
    "# with open(f'models/paracrawl_vocab_ga_v0.2_exp{exp}.csv', 'w', newline='') as csvfile:\n",
    "#     v_writer = csv.writer(csvfile, delimiter=',')\n",
    "#     for l in dls.vocab[1]:\n",
    "#         v_writer.writerow([l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 25, 7)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train_ds)+len(dls.valid_ds), len(dls.train), len(dls.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab lengths are : (1112, 1208)\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab lengths are : {len(dls.vocab[0]), len(dls.vocab[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 62]),\n",
       " torch.Size([32, 67]),\n",
       " (TensorText([[  2,  11,  25,  ...,   1,   1,   1],\n",
       "          [  2,  19, 123,  ...,   1,   1,   1],\n",
       "          [  2,  19,  41,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  2,  19, 711,  ...,   1,   1,   1],\n",
       "          [  2,  11,  21,  ...,   1,   1,   1],\n",
       "          [  2,  11,  25,  ...,   1,   1,   1]], device='cuda:0'),\n",
       "  TensorText([[  2,  12,  26,  ...,   1,   1,   1],\n",
       "          [  2,  20,  65,  ...,   1,   1,   1],\n",
       "          [  2,  20, 359,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  2,   8,  29,  ...,   1,   1,   1],\n",
       "          [  2,  12,  21,  ...,   1,   1,   1],\n",
       "          [  2,  12,  26,  ...,   1,   1,   1]], device='cuda:0')))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o=dls.one_batch(); o[0].size(), o[1].size(), o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Encode the position with a sinusoid.\"\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))\n",
    "    \n",
    "    def forward(self, pos):\n",
    "        #inp = torch.ger(pos, self.freq)\n",
    "        inp = torch.ger(pos, self.freq.float())\n",
    "        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)\n",
    "        return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tst_encoding = PositionalEncoding(20)\n",
    "# res = tst_encoding(torch.arange(0,100).float())\n",
    "# _, ax = plt.subplots(1,1)\n",
    "# for i in range(1,5): ax.plot(res[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    \"Embedding + positional encoding + dropout\"\n",
    "    def __init__(self, vocab_sz, emb_sz, inp_p=0.):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        self.embed = Embedding(vocab_sz, emb_sz)\n",
    "        self.pos_enc = PositionalEncoding(emb_sz)\n",
    "        self.drop = nn.Dropout(inp_p)\n",
    "    \n",
    "    def forward(self, inp): \n",
    "        #pos = torch.arange(0, inp.size(1), device=inp.device).float()     \n",
    "        pos = torch.arange(0, inp.size(1), device=inp.device).float()     \n",
    "        return self.drop(self.embed(inp) * math.sqrt(self.emb_sz) + self.pos_enc(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Transformer Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: [src/tgt/memory]_mask should be filled with float(‘-inf’) for the masked positions and float(0.0) else. These masks ensure that predictions for position i depend only on the unmasked positions j and are applied identically for each sequence in a batch. \n",
    "\n",
    "[src/tgt/memory]_key_padding_mask should be a ByteTensor where True values are positions that should be masked with float(‘-inf’) and False values will be unchanged. This mask ensures that no information will be taken from position i if it is masked, and has a separate mask for each sequence in a batch.\n",
    "\n",
    "attn mask with -inf\n",
    "key_padding mask with True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pt_Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_half(b):\n",
    "    \"Recursively map lists of tensors in `b ` to FP16.\"\n",
    "    return apply(lambda x: x.half() if torch.is_floating_point(x) else x, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class pt_Transformer(Module):\n",
    "    def __init__(self, src_vcbsz, trg_vcbsz, n_enc_layers=6, n_dec_layers=6, n_heads=8, d_model=256, d_head=32, \n",
    "                 d_inner=1024, p=0.1, bias=True, scale=True, double_drop=True, pad_idx=1, fp16=True):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.enc_tfmr_emb = TransformerEmbedding(src_vcbsz, d_model, p)\n",
    "        self.dec_tfmr_emb = TransformerEmbedding(trg_vcbsz, d_model, 0.)        \n",
    "        self.final = nn.Linear(d_model, trg_vcbsz)\n",
    "        self.fp16 = fp16\n",
    "        # !!!\n",
    "        #self.final.weight = self.dec_tfmr_emb.embed.weight    # !! Ties weights\n",
    "        \n",
    "        self.transformer_model=torch.nn.Transformer(d_model=d_model, nhead=n_heads, num_encoder_layers=n_enc_layers, \n",
    "                                   num_decoder_layers=n_dec_layers, dim_feedforward=d_inner, dropout=p, \n",
    "                                   activation='relu', custom_encoder=None, custom_decoder=None)\n",
    "    \n",
    "    \n",
    "    def forward(self, src, trg, src_mask=None, tgt_mask=None, memory_mask=None, \n",
    "                        src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \n",
    "        enc_emb, dec_emb = self.enc_tfmr_emb(src), self.dec_tfmr_emb(trg)\n",
    "        \n",
    "        # Test whether fp16 is being used or not\n",
    "#         if not isinstance(model.transformer_model.encoder.layers[0].self_attn.out_proj.weight,\n",
    "#                           torch.cuda.FloatTensor):\n",
    "        if model.transformer_model.encoder.layers[0].self_attn.out_proj.weight.dtype == torch.float16:\n",
    "            enc_emb=to_half(enc_emb)\n",
    "            dec_emb=to_half(dec_emb)\n",
    "        \n",
    "        if src.device.type == 'cuda':\n",
    "            src_mask=self.transformer_model.generate_square_subsequent_mask(src.size(1)).cuda()\n",
    "            trg_mask=self.transformer_model.generate_square_subsequent_mask(trg.size(1)).cuda()\n",
    "        else:\n",
    "            src_mask=self.transformer_model.generate_square_subsequent_mask(src.size(1)).cpu()\n",
    "            trg_mask=self.transformer_model.generate_square_subsequent_mask(trg.size(1)).cpu()\n",
    "        \n",
    "        dec_out = self.transformer_model(enc_emb.permute(1,0,2), dec_emb.permute(1,0,2),\n",
    "                                         src_mask=src_mask, tgt_mask=trg_mask, memory_mask=None, \n",
    "                        src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "        \n",
    "        out=self.final(dec_out)\n",
    "        \n",
    "        return out.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "#### Present Input and Target in a single tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineInputOutputCallback(Callback):\n",
    "    '''Callback to combine the input and target text into self.xb'''\n",
    "    def __init__(self): pass\n",
    "    def begin_batch(self): \n",
    "        self.learn.xb = (self.xb[0], self.yb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifting and masking of y, from [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html#training):\n",
    "\n",
    "> We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shifting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target shift/offset explained\n",
    "\n",
    "**Taken from [@bentrevett's brilliant github repo \"pytorch-seq2seq\" tutorials](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb):**\n",
    "\n",
    "As we want our model to predict the <eos> token but not have it be an input into our model we simply slice the <eos> token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes **actual** target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the <eos> token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes **predicted** target sequence element. We then calculate our loss using the original trg tensor with the <sos> token sliced off the front, leaving the <eos> token:\n",
    "\n",
    "$$\\begin{align*} \\text{output} &= [y_1, y_2, y_3, eos]\\\\ \\text{trg[1:]} &= [x_1, x_2, x_3, eos] \\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard.\n",
    "    \n",
    "    \n",
    "We don't want to punish the model for not translating the 'sos' token, but we do need it to predict/define the end of the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RemoveEOSCallback** \n",
    "\n",
    "Cut the *EOS* token token from the **output_x** presented to the model as we are trying to predict the next word. Therefore don't want to model to try anything after the *EOS* token. So the last token given to the model will be the token before *EOS*. This callback modifies the second element of our learn.xb, (which is the *copied* yb)\n",
    "\n",
    "But this should also ignore padding, as otherwise we'll be just cutting the last padding token and not the EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveEOSCallback(Callback):\n",
    "    '''\n",
    "        Shift the target presented to the model during training to remove the \"eos\" token as \n",
    "        we don't want the model to learn to translate EOS when it sees EOS.\n",
    "        \n",
    "        In practice we actually mask the EOS token as due to batching the last token will often be a <pad> token,\n",
    "        not EOS\n",
    "    '''\n",
    "    def __init__(self, eos_idx): self.eos_idx=eos_idx\n",
    "    def begin_batch(self):        \n",
    "        eos_mask=(self.learn.xb[1]!=self.eos_idx)\n",
    "        sz=torch.tensor(self.learn.xb[1].size())\n",
    "        sz[1]=sz[1]-1\n",
    "        self.learn.xb = (self.learn.xb[0], self.learn.xb[1][eos_mask].view((sz[0],sz[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LossTargetShiftCallback:** Shift the target shown to the loss to exclude the \"eos\" token, as translating \"bos\" is not part of our language translation objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossTargetShiftCallback(Callback):\n",
    "    '''\n",
    "        Shift the target shown to the loss to exclude the \"bos\" token as the first token we want predicted\n",
    "        should be an actual word, not the \"bos\" token (as we have already given the model \"bos\" )\n",
    "    '''\n",
    "    def __init__(self): pass\n",
    "    def after_pred(self): \n",
    "        self.learn.yb = (self.learn.yb[0][:,1:],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer size from Annotated Transformer:\n",
    "\n",
    "N=6, d_model=512, d_ff=2048, h=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pad_idx=1\n",
    "assert dls.vocab[1][pad_idx] == 'xxpad' \n",
    "n_x_vocab, n_y_vocab = len(dls.vocab[0]), len(dls.vocab[1])\n",
    "d_model=512\n",
    "n_heads=8 #12\n",
    "d_inner=2048  #1024\n",
    "\n",
    "n_enc_layers=2\n",
    "n_dec_layers=2\n",
    "\n",
    "model=pt_Transformer(src_vcbsz=n_x_vocab, trg_vcbsz=n_y_vocab, d_model=d_model, d_inner=d_inner,\n",
    "                    n_enc_layers=n_enc_layers, n_dec_layers=n_dec_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaiming_Normal works terrribly, at least if you apply it to everything except LayerNorm...\n",
    "\n",
    "DistilBERT works ok\n",
    "\n",
    "Could try xavier:\n",
    "\n",
    "```\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "model.apply(initialize_weights);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DistilBERT initialisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilERT HF init weights https://github.com/huggingface/transformers/blob/31e67dd19f1b3fe2bc9a13f86d814f3f7bba48e4/src/transformers/modeling_distilbert.py\n",
    "\n",
    "def distil_apply_leaf(m, f):\n",
    "    \"Apply `f` to children of `m`.\"\n",
    "    c = m.children()\n",
    "    if isinstance(m, nn.Module): f(m)\n",
    "    for l in c: apply_leaf(l,f)\n",
    "\n",
    "\n",
    "def _distilbert_init_weights(module):\n",
    "    \"\"\" Initialize the weights.\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Embedding):\n",
    "        if module.weight.requires_grad:\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02) #std=self.config.initializer_range)\n",
    "    if isinstance(module, nn.Linear):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02) #self.config.initializer_range)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "distil_apply_leaf(model, _distilbert_init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,522,424 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbLogMore(Callback):\n",
    "    run_after=WandbCallback\n",
    "    def begin_fit(self):\n",
    "        wandb.config.update({'n_enc_layers':n_enc_layers, 'n_dec_layers':n_dec_layers})\n",
    "\n",
    "# Make sure model config is logged each time        \n",
    "WandbCallback._wandb_watch_called=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sqr_grad(p, sqr_mom, dampening=True, sqr_avg=None, **kwargs):\n",
    "    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n",
    "    damp = 1-sqr_mom if dampening else 1.\n",
    "    sqr_avg.mul_(sqr_mom).addcmul_(p.grad.data, p.grad.data, value=damp)\n",
    "    # above eqv to:\n",
    "    # sqr_avg = (sqr_mom * sqr_avg) + (p.grad.data * p.grad.data * damp)  \n",
    "    return {'sqr_avg': sqr_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_step(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, **kwargs):\n",
    "    \"Step for Adam with `lr` on `p`\"\n",
    "    debias1 = debias(mom,     1-mom,     step)\n",
    "    debias2 = debias(sqr_mom, 1-sqr_mom, step)\n",
    "    p.data.addcdiv_(grad_avg, (sqr_avg/debias2).sqrt() + eps, value = -lr / debias1)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_step._defaults = dict(eps=1e-5)\n",
    "\n",
    "# Cell\n",
    "@log_args(to_return=True, but_as=Optimizer.__init__)\n",
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, decouple_wd=True):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaHessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params, grads = [], []\n",
    "# for param in learn.model.parameters():\n",
    "#     if not param.requires_grad:\n",
    "#         continue\n",
    "#     params.append(param)\n",
    "#     grads.append(0. if param.grad is None else param.grad + 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#learn.show_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GetParamsGrad(Callback):\n",
    "#     \"\"\"\n",
    "#         get model parameters and corresponding gradients\n",
    "#     \"\"\"\n",
    "#     run_after=MixedPrecision\n",
    "#     def after_backward(self):\n",
    "#         params = []\n",
    "#         grads = []\n",
    "#         for param in self.model.parameters():\n",
    "#             if not param.requires_grad:\n",
    "#                 continue\n",
    "#             params.append(param)\n",
    "#             grads.append(0. if param.grad is None else param.grad + 0.)\n",
    "#         self._params = params\n",
    "#         self._grads = grads\n",
    "#         #return params, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         loss.backward(create_graph=True)\n",
    "#         _, gradsH = get_params_grad(model)\n",
    "#         optimizer.step(gradsH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tst_param(val, grad=None):\n",
    "#     \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "#     res = tensor([val]).float()\n",
    "#     res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "#     return res\n",
    "\n",
    "# r = L.range(4)\n",
    "# def tst_params(): return r.map(tst_param)\n",
    "\n",
    "# params = tst_params()\n",
    "# opt = Optimizer(params, sgd_step, lr=0.1)\n",
    "# print(params)\n",
    "# print()\n",
    "# print(opt.all_params())\n",
    "# print()\n",
    "# print(opt.state)\n",
    "# opt.step()\n",
    "# print()\n",
    "# print(opt.all_params())\n",
    "# print()\n",
    "# print(opt.all_params(with_grad=True))\n",
    "# print()\n",
    "# #print(opt.state)\n",
    "\n",
    "# test_close([p.item() for p in params], r.map(mul(0.99)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def tst_param(val, grad=None):\n",
    "#     \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "#     res = tensor([val]).float()\n",
    "#     res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "#     return res\n",
    "\n",
    "# r = L.range(4)\n",
    "\n",
    "# def tst_params(): return r.map(tst_param)\n",
    "\n",
    "# params = tst_params()\n",
    "\n",
    "# grads = []\n",
    "# for p in params:\n",
    "#     p.requires_grad = True\n",
    "#     grads.append(0. if p.grad is None else p.grad + 0.)\n",
    "    \n",
    "# grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NllLossBackward object at 0x7f709145db50>\n",
      "\n",
      "[Parameter containing:\n",
      "tensor([[-0.5158,  0.0392],\n",
      "        [-0.3330,  0.4913]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4275, -0.6492], requires_grad=True)]\n",
      "\n",
      "[tensor([[-0.8369, -0.2466],\n",
      "        [ 0.8369,  0.2466]], grad_fn=<AddBackward0>), tensor([ 0.6518, -0.6518], grad_fn=<AddBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "lin = nn.Linear(2, 2) # your model or manual operations\n",
    "out = lin(x)\n",
    "\n",
    "trg = torch.ones(out.size()[1]).long()\n",
    "\n",
    "loss = criterion(out, trg)\n",
    "\n",
    "print(loss.grad_fn)\n",
    "print()\n",
    "loss.backward(create_graph=True)\n",
    "\n",
    "params = []\n",
    "grads = []\n",
    "for param in lin.parameters():\n",
    "    if not param.requires_grad:\n",
    "        continue\n",
    "    params.append(param)\n",
    "    grads.append(0. if param.grad is None else param.grad + 0.)\n",
    "    \n",
    "print(params)\n",
    "print()      \n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8369, -0.2466],\n",
       "        [ 0.8369,  0.2466]], grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #for p,g in zip(params, grads):\n",
    "# for p in params:\n",
    "#     print(get_hutchinson_trace(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = Optimizer(params, hutchinson_trace, lr=0.1)\n",
    "# opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hutchinson_trace(p, **kwargs):\n",
    "#     \"\"\"\n",
    "#         compute the Hessian vector product with a random vector v, at the current gradient point,\n",
    "#         i.e., compute the gradient of <gradsH,v>.\n",
    "#         :param gradsH: a list of torch variables\n",
    "#         :return: a list of torch tensors\n",
    "#     \"\"\"\n",
    "# #     params = p\n",
    "# #     # Get gradsH\n",
    "# #     params = []\n",
    "# #     grads = []\n",
    "# #     for param in self.model.parameters():\n",
    "# #         if not param.requires_grad:\n",
    "# #             continue\n",
    "# #         #params.append(param)\n",
    "# #         grads.append(0. if param.grad is None else param.grad + 0.)\n",
    "# #     #self._params = params\n",
    "# #     gradsH = grads\n",
    "\n",
    "#     #params = self.param_groups[0]['params']\n",
    "#     #params = [p]\n",
    "# #     if gradsH is None:\n",
    "# #         print('yep')\n",
    "#     #print(p.requires_grad)\n",
    "#     #gradsH = [0. if p.grad is None else p.grad + 0.]\n",
    "#     #gradsH = [torch.tensor(0. if p.grad is None else p.grad.clone() + 0., requires_grad=True)]\n",
    "#     if p.grad is None: gradsH = torch.tensor(0. , requires_grad=True)\n",
    "#     else: gradsH = p.grad.clone().detach().requires_grad_(True) + 0.\n",
    "#     #gradsH =  = torch.tensor(0. if p.grad is None else p.grad.clone() + 0., requires_grad=True)\n",
    "#     #torch.tensor(b.grad.clone(), requires_grad=True)\n",
    "#     print(gradsH)\n",
    "\n",
    "#         #gradsH.append(0. if p.grad is None else p.grad + 0.)\n",
    "# #         print(gradsH)\n",
    "# #         for param in self.model.parameters():\n",
    "# # #         if not param.requires_grad:\n",
    "# # #             continue\n",
    "# # #         #params.append(param)\n",
    "# # #         grads.append(0. if param.grad is None else param.grad + 0.)\n",
    "# #     else: gradsH = [gradsH]\n",
    "    \n",
    "#     #v = [torch.randint_like(_p, high=2, device='cuda') for _p in params]\n",
    "#     #v = [torch.randint_like(_p, high=2, device='cuda') for _p in params]\n",
    "#     v = torch.randint_like(p, high=2, device='cuda')\n",
    "    \n",
    "#     for v_i in v:\n",
    "#         v_i[v_i == 0] = -1\n",
    "    \n",
    "#     hvs = torch.autograd.grad(\n",
    "#         list(gradsH),\n",
    "#         list(p),\n",
    "# #         gradsH,\n",
    "# #         params,\n",
    "#         grad_outputs=list(v),\n",
    "#         only_inputs=True,\n",
    "#         retain_graph=True,\n",
    "#         allow_unused=True)\n",
    "#     print(hvs)\n",
    "    \n",
    "#     hutchinson_trace = []\n",
    "#     for hv, vi in zip(hvs, v):\n",
    "#         param_size = hv.size()\n",
    "#         #print(param_size)\n",
    "#         if len(param_size) <= 2:  # for 0/1/2D tensor\n",
    "#             tmp_output = torch.abs(hv * vi)\n",
    "#             #hutchinson_trace.append(tmp_output) # Hessian diagonal block size is 1 here.\n",
    "#             hutchinson_trace = tmp_output\n",
    "#         elif len(param_size) == 4:  # Conv kernel\n",
    "#             tmp_output = torch.abs(torch.sum(torch.abs(hv * vi), dim=[2, 3], keepdim=True)) / vi[0, 1].numel() # Hessian diagonal block size is 9 here: torch.sum() reduces the dim 2/3.\n",
    "#             #hutchinson_trace.append(tmp_output)\n",
    "#             hutchinson_trace = tmp_output\n",
    "    \n",
    "#     #print(len(hutchinson_trace))\n",
    "#     return {'hutchinson_trace':hutchinson_trace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sqr_diag_hessian(p, sqr_mom, dampening=True, sqr_avg_diag_hessian=None, **kwargs):\n",
    "#     print(kwargs)\n",
    "#     print(hutchinson_trace)\n",
    "    if sqr_avg_diag_hessian is None: sqr_avg_diag_hessian = torch.zeros_like(p.grad.data)\n",
    "    damp = 1-sqr_mom if dampening else 1.\n",
    "    #sqr_avg_diag_hessian.mul_(sqr_mom).addcmul_(p.grad.data, p.grad.data, value=damp)\n",
    "    \n",
    "    sqr_avg_diag_hessian = (sqr_mom * sqr_avg_diag_hessian) + (damp * hutchinson_trace * hutchinson_trace)\n",
    "    return {'sqr_avg_diag_hessian': sqr_avg_diag_hessian}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adahessian_step(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg_diag_hessian, eps, **kwargs):\n",
    "    \"Step for Adam with `lr` on `p`\"\n",
    "    debias1 = debias(mom,     1-mom,     step)\n",
    "    debias2 = debias(sqr_mom, 1-sqr_mom, step)\n",
    "    p.data.addcdiv_(grad_avg, (sqr_avg_diag_hessian/debias2).sqrt() + eps, value = -lr / debias1)\n",
    "    \n",
    "    # WHERE DOES THE POWER GO??\n",
    "    #p.data = p.data + (grad_avg / (sqr_avg_diag_hessian/debias2).sqrt() + eps) * (-lr / debias1)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if self.hessian_power < 1:\n",
    "#     denom = ((exp_hessian_diag_sq.sqrt() / math.sqrt(bias_correction2)) ** self.hessian_power).add_(group['eps'])\n",
    "# else:\n",
    "#     denom = (exp_hessian_diag_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr=0.15, betas=(0.9, 0.999), eps=1e-4, weight_decay=0, hessian_power=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam_step._defaults = dict(eps=1e-5)\n",
    "\n",
    "# Cell\n",
    "@log_args(to_return=True, but_as=Optimizer.__init__)\n",
    "def AdaHessian(params, lr, hessian_power=1, hutchinson_trace=None, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.0, decouple_wd=True):\n",
    "    \n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "#     cbs += [partial(average_grad, dampening=True), average_sqr_diag, step_stat, adam_step]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_diag_hessian, adahessian_step, step_stat]\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, hessian_power=hessian_power, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def one_batch(self:Learner, i, b, create_graph=True):\n",
    "    self.iter = i\n",
    "    try:\n",
    "        self._split(b);                                  self('begin_batch')\n",
    "        self.pred = self.model(*self.xb);                self('after_pred')\n",
    "        if len(self.yb) == 0: return\n",
    "        self.loss = self.loss_func(self.pred, *self.yb); self('after_loss')\n",
    "        if not self.training: return\n",
    "        self.loss.backward(create_graph=create_graph);   self('after_backward')    # <---CHANGED HERE\n",
    "        self.opt.step();                                 self('after_step')\n",
    "        self.opt.zero_grad()\n",
    "    except CancelBatchException:                         self('after_cancel_batch')\n",
    "    finally:                                             self('after_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HutchinsonTrace(Callback):\n",
    "    runs_after=MixedPrecision\n",
    "    \n",
    "    def _get_params_grads(self):\n",
    "        params = []\n",
    "        gradsH = []\n",
    "        for p in self.model.parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            gradsH.append(0. if p.grad is None else p.grad + 0.)\n",
    "            params.append(p)\n",
    "        return params,gradsH\n",
    "    \n",
    "    def after_backward(self):\n",
    "        \"\"\"\n",
    "            compute the Hessian vector product with a random vector v, at the current gradient point,\n",
    "            i.e., compute the gradient of <gradsH,v>.\n",
    "            :param gradsH: a list of torch variables\n",
    "            :return: a list of torch tensors\n",
    "        \"\"\"\n",
    "        params,gradsH = self._get_params_grads()\n",
    "\n",
    "        v = [torch.randint_like(p, high=2, device='cuda') for p in params]\n",
    "        for v_i in v:\n",
    "            v_i[v_i == 0] = -1\n",
    "        \n",
    "        hvs = torch.autograd.grad(\n",
    "            gradsH,\n",
    "            params,\n",
    "            grad_outputs=v,\n",
    "            only_inputs=True,\n",
    "            retain_graph=True,\n",
    "            #allow_unused=True\n",
    "        )\n",
    "        #print(hvs)\n",
    "\n",
    "        hutchinson_trace = []\n",
    "        for hv, vi in zip(hvs, v):\n",
    "            param_size = hv.size()\n",
    "            #print(param_size)\n",
    "            if len(param_size) <= 2:  # for 0/1/2D tensor\n",
    "                tmp_output = torch.abs(hv * vi)\n",
    "                hutchinson_trace.append(tmp_output) # Hessian diagonal block size is 1 here.\n",
    "                #hutchinson_trace = tmp_output\n",
    "            elif len(param_size) == 4:  # Conv kernel\n",
    "                tmp_output = torch.abs(torch.sum(torch.abs(hv * vi), dim=[2, 3], keepdim=True)) / vi[0, 1].numel() # Hessian diagonal block size is 9 here: torch.sum() reduces the dim 2/3.\n",
    "                hutchinson_trace.append(tmp_output)\n",
    "                #hutchinson_trace = tmp_output\n",
    "\n",
    "        #print(len(hutchinson_trace))\n",
    "        self.learn.opt.state['hutchinson_trace'] = hutchinson_trace\n",
    "        \n",
    "        print(f'hutch state: {self.learn.opt.state[\"hutchinson_trace\"]}')\n",
    "        \n",
    "        #print(hutchinson_trace)\n",
    "        #return {'hutchinson_trace':hutchinson_trace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cbs = [CombineInputOutputCallback, RemoveEOSCallback(eos_idx=3), LossTargetShiftCallback, HutchinsonTrace,\n",
    "       WandbLogMore, WandbCallback(log_preds=False)] \n",
    "\n",
    "pad_idx=1\n",
    "assert dls.vocab[1][pad_idx] == 'xxpad' \n",
    "loss_func = CrossEntropyLossFlat(ignore_index=pad_idx)\n",
    "\n",
    "opt_func=AdaHessian\n",
    "\n",
    "# No accuracy metric\n",
    "learn = Learner(dls, model, metrics=[Perplexity(), CorpusBLEUMetric(vocab_sz=n_y_vocab)], opt_func=opt_func,\n",
    "                cbs=cbs, loss_func=loss_func).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#learn.load('paracrawl_en_ga_5e_5e-4_5e_1e-5_v0.2_exp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai2.fp16_utils import convert_network\n",
    "# learn.model = convert_network(learn.model, dtype=torch.float32)\n",
    "# learn.save('paracrawl_en_ga_5e_5e-4_5e_1e-5_v0.2_exp4_no_opt', with_opt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn=learn.to_fp32()\n",
    "# learn.save('paracrawl_en_ga_5e_5e-4_5e_1e-5_v0.2_exp4_no_opt', with_opt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hutch state: [tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), tensor([[inf, inf, inf,  ..., inf, inf, inf],\n",
      "        [inf, inf, inf,  ..., inf, nan, inf],\n",
      "        [inf, nan, nan,  ..., inf, nan, inf],\n",
      "        ...,\n",
      "        [inf, inf, inf,  ..., inf, inf, inf],\n",
      "        [nan, inf, inf,  ..., nan, inf, inf],\n",
      "        [inf, inf, inf,  ..., inf, inf, inf]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([   inf,    inf,    inf,  ...,    inf,    inf, 60704.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([   inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf, 53824.,    inf, 48672.,\n",
      "           inf,  7376.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    nan,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    nan,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 14056.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf, 12544., 56256., 17568.,  2252.,    inf,    inf,\n",
      "           inf,    inf, 10816.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 45152.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 57504.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 49632.,    inf,    inf,    inf,    inf, 52736.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "        49920.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 42944.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    nan,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    nan,    inf,    inf,    inf,    inf, 35840.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    nan,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    nan, 40512.,    inf,    inf,\n",
      "           inf, 51136.,    inf,    inf,    inf,    inf,    inf,    inf, 11568.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           nan,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf, 38176.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "        30560.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf, 33344.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf, 36448.,\n",
      "        16544.,    inf,    inf,    inf, 56704.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([   inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf, 58944.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 59392.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 56992.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 13480.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf],\n",
      "       device='cuda:0', dtype=torch.float16)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hutch state: [tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), tensor([[   inf,    inf,    inf,  ...,    inf,    inf,    inf],\n",
      "        [   inf,    inf,    inf,  ...,    inf,    inf,  8288.],\n",
      "        [   nan,    inf,    nan,  ...,    nan,    inf,    inf],\n",
      "        ...,\n",
      "        [   inf,    inf,    inf,  ...,    inf,    inf,  8624.],\n",
      "        [   inf,    inf,    inf,  ...,    inf,    inf, 36064.],\n",
      "        [   nan,    inf, 17888.,  ...,    nan,    inf, 32720.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([inf, inf, inf,  ..., inf, inf, inf], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([   inf,    inf, 63104.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "        54496.,    inf,    inf, 38816.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf, 28736.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 24192., 53376.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf, 16544.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,  6756.,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf, 36512.,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf, 55168.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf, 18000.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 63584.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 18256., 12544.,    inf,    inf,    inf,\n",
      "        14976.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,  8528.,    inf,    inf,    inf, 49440.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf, 45344.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf, 64288.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf, 57568.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,  3032.,\n",
      "           inf,    inf,    inf, 37632.,    inf,    inf,    inf,    inf,    inf,\n",
      "        22832.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "         2518.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 39552.,    inf,    inf,    inf, 19984.,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf, 37792.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 40064., 26144.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf, 43776.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 28400.,    inf, 58912.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 49760.,    inf,    inf, 26640.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([   inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 17120.,    inf,    inf,    inf, 30368.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,  2948.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf, 60736.,    inf,\n",
      "        39488.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 14960.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf, 45056.,    inf,\n",
      "           inf, 26976.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 13960.,    inf,    inf,    inf, 57344.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "         1248.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf, 47488.,    inf,    inf,    inf,    inf,    inf,\n",
      "        36512.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf, 12752.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf, 43744.,    inf,    inf,\n",
      "           inf,    inf,  2564.,    inf,    inf,    inf,    inf,    inf, 31424.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 53216.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf],\n",
      "       device='cuda:0', dtype=torch.float16)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hutch state: [tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), tensor([[   inf,    inf,    inf,  ...,    inf,    inf,    inf],\n",
      "        [42656., 21856.,    inf,  ...,  2556.,    inf,    inf],\n",
      "        [   inf, 20448.,    inf,  ...,    inf,    inf,    inf],\n",
      "        ...,\n",
      "        [   inf, 28832.,    inf,  ...,    inf,    inf, 52608.],\n",
      "        [60896., 28192.,    inf,  ...,  5840.,    inf,    inf],\n",
      "        [45120.,  8296.,    inf,  ...,  2776.,    inf,    inf]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([      inf,  736.0000,       inf,  ...,       inf,  444.5000,\n",
      "        3638.0000], device='cuda:0', dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([       inf, 56928.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 48288.0000,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf, 47200.0000, 18720.0000,\n",
      "        56256.0000,        inf, 29392.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf, 31632.0000,        inf,\n",
      "               inf, 12552.0000,        inf, 18016.0000,        inf,        inf,\n",
      "               inf,        inf, 62560.0000,   168.0000,        inf,        inf,\n",
      "        43136.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        11408.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 35616.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 15608.0000,\n",
      "               inf, 55904.0000,        inf,        inf, 18256.0000,        inf,\n",
      "               inf,        inf,        inf, 56000.0000,        inf,  1498.0000,\n",
      "               inf,        inf,        inf,        inf, 13000.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 22816.0000,        inf, 34336.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "         4612.0000,        inf,        inf, 20832.0000,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 37248.0000, 22256.0000,        inf,        inf,\n",
      "        49888.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        13336.0000,        inf, 37216.0000, 56864.0000,        inf,        inf,\n",
      "               inf,        inf,  1310.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "        13872.0000, 57792.0000,        inf,        inf, 29968.0000, 56064.0000,\n",
      "               inf,        inf,        inf, 65312.0000,        inf,        inf,\n",
      "               inf,  2422.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,  5608.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 46944.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 35392.0000,  9968.0000,        inf,\n",
      "               inf, 63424.0000,        inf, 57472.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 51328.0000,        inf, 49984.0000,\n",
      "               inf,        inf, 13072.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,   989.5000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 11632.0000,  3726.0000,        inf,\n",
      "        26912.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 26896.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 47200.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 29808.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 28352.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf, 54080.0000,        inf,\n",
      "        48224.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        13496.0000,        inf,        inf,        inf, 51264.0000,        inf,\n",
      "               inf,        inf,        inf,        inf, 43520.0000, 31104.0000,\n",
      "               inf,        inf, 43392.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,  2436.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,  7816.0000,        inf,\n",
      "        53344.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        33696.0000,        inf,        inf,        inf, 21728.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 26768.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 47360.0000,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf, 31584.0000,        inf,\n",
      "               inf,        inf, 37152.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 20928.0000,\n",
      "               inf,        inf,        inf,        inf, 41472.0000,        inf,\n",
      "               inf, 25184.0000,        inf,        inf, 49248.0000,        inf,\n",
      "               inf,        inf, 20400.0000,        inf,        inf,        inf,\n",
      "        56256.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf, 15440.0000, 63392.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 42368.0000,\n",
      "               inf, 42656.0000,        inf,        inf, 37632.0000,        inf,\n",
      "               inf, 24480.0000, 48480.0000,        inf,        inf,        inf,\n",
      "        65440.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,  4740.0000], device='cuda:0', dtype=torch.float16), tensor([   inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf, 44192.,\n",
      "           inf,    inf,    inf, 32528.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 24832.,    inf,    inf, 57152.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "        37792.,    inf,    inf,    inf,    inf,    inf,    inf, 10872.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "         8480.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 28768., 41504.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,  6568.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,  1680.,    inf,  5360., 60832.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf, 38144.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf, 14976.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,  5264.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "          302.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf, 51648.,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf, 46880.,\n",
      "           inf,  4256.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,  2870.,    inf,    inf,\n",
      "           inf,    inf,    inf, 25408., 46496.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,  2830.,    inf,    inf,    inf,\n",
      "           inf, 44576.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 41792.,    inf,    inf,    inf,    inf, 24384.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,  7376.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf, 11784.,    inf,    inf,\n",
      "           inf,    inf,    inf, 13072.,    inf,    inf,    inf, 16224.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 31776.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 45664.,    inf,    inf, 36992.,    inf,\n",
      "        34336.,    inf,    inf,    inf,    inf,    inf, 61952.,    inf,    inf,\n",
      "           inf,    inf, 42816.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf, 59680.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,  5504.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 25328.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf],\n",
      "       device='cuda:0', dtype=torch.float16)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hutch state: [tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), tensor([[   inf,    inf,    inf,  ...,    inf,    inf,    inf],\n",
      "        [   inf, 17056., 35904.,  ...,  6784., 15640.,    inf],\n",
      "        [   inf,    inf,    inf,  ...,    inf, 10648.,    inf],\n",
      "        ...,\n",
      "        [59456.,  9936., 26720.,  ...,   928., 15656.,    inf],\n",
      "        [15488., 24320., 60800.,  ..., 57280., 11216.,    inf],\n",
      "        [34304., 26256., 43520.,  ..., 26448.,  5480.,    inf]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([ 3158., 39712., 13936.,  ..., 33152., 33440.,  1356.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([   inf, 34784.,    inf,    inf, 53888.,    inf, 11560.,    inf,    inf,\n",
      "        33408., 37856., 23936.,    inf,    inf,    inf, 35488.,    inf,  5844.,\n",
      "           inf,    inf, 39552.,    inf,    inf,    inf,    inf,    inf, 47552.,\n",
      "           inf,    inf,    inf,    inf, 11872.,    inf,    inf, 28480., 10424.,\n",
      "           inf, 44288., 26992.,    inf, 48448., 21984.,    inf,    inf,    inf,\n",
      "           inf, 49408.,    inf, 30752.,    inf,    inf,    inf,    inf,   491.,\n",
      "        36544.,    inf, 14984., 36480.,    inf,    inf,    inf, 55808., 18208.,\n",
      "           inf,  4788., 33696.,    inf,    inf,    inf, 22512.,    inf,    inf,\n",
      "         7584.,    inf,    inf,    inf, 43744., 55968.,    inf, 54080.,    inf,\n",
      "        35424., 48096.,    inf, 43008., 17744.,    inf, 22096.,    inf,    inf,\n",
      "           inf,    inf, 31216.,    inf,    inf,    inf, 55264.,    inf,    inf,\n",
      "        23024.,    inf,    inf,    inf,    inf, 63136., 19824.,    inf, 41920.,\n",
      "        49056.,    inf,    inf,  6648.,    inf,    inf,    inf, 36960.,    inf,\n",
      "        49216., 52128., 23888.,    inf, 53856.,    inf,    inf, 13992.,  8352.,\n",
      "           inf,    inf,    inf,    inf, 34912.,    inf,    inf,  7540.,    inf,\n",
      "           inf, 53376.,  8904., 56320.,    inf, 22448.,    inf, 38688.,    inf,\n",
      "           inf, 56064., 53728.,    inf,    inf,    inf,    inf, 23168.,  8440.,\n",
      "         7552.,    inf,    inf,    inf, 44224., 52160., 12832., 23424., 25072.,\n",
      "         9064.,    inf,    inf, 21824.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 16752.,    inf,  2496.,    inf, 31200., 40704.,    inf,    inf,\n",
      "        52608., 26864.,    inf,    inf,    inf,    inf, 23040., 14480.,    inf,\n",
      "        28992.,    inf,    inf,    inf,    inf, 28976.,    inf, 14568.,    inf,\n",
      "           inf, 57792., 24016.,    inf, 52192.,    inf,    inf,    inf, 34688.,\n",
      "           inf,    inf,  6304.,    inf, 43264.,    inf,  3912.,    inf,    inf,\n",
      "           inf, 53312.,    inf,    inf,    inf, 20432.,    inf, 40768.,    inf,\n",
      "        28096., 53728.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf, 30768.,  1528.,    inf, 32096., 11296., 10352.,\n",
      "           inf,    inf,    inf, 16272.,    inf,    inf,    inf,    inf, 16640.,\n",
      "           inf,    inf,  7504.,    inf,    inf,    inf,    inf,    inf, 58880.,\n",
      "         8600., 30064.,    inf,    inf,    inf, 13064.,    inf, 57664.,  2590.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 12904.,    inf,    inf,    inf,    inf, 18544.,    inf,    inf,\n",
      "        18160.,    inf,    inf,    inf,    inf, 15352.,    inf, 13664.,    inf,\n",
      "        28160., 15184.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 39072.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 15016.,    inf, 21376.,    inf, 19760.,\n",
      "           inf,    inf,    inf, 54528., 62240.,    inf,    inf, 37408., 57664.,\n",
      "           inf,    inf,    inf, 28896., 51616., 22832.,    inf,    inf,    inf,\n",
      "         5256.,    inf,  3500.,    inf, 10312., 30160.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf, 12096.,  4860., 22208.,\n",
      "        62336.,    inf,    inf,    inf,    inf, 63168.,    inf,  8176.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf, 28160.,\n",
      "        44000.,    inf,    inf,    inf, 45920.,    inf,    inf, 12584.,    inf,\n",
      "           inf,    inf, 27632.,    inf, 50784.,    inf,    inf, 43616.,    inf,\n",
      "           inf,    inf,  3328.,    inf, 42592.,    inf,    inf, 50240., 42656.,\n",
      "           inf,    inf,    inf,  5208.,  4044.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "        10848., 25232., 61664., 49600.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 56160., 25728.,    inf, 17568.,    inf,    inf, 29392.,\n",
      "           inf,    inf,    inf,  6812.,    inf, 18656.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,  9576.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 45216.,    inf,    inf, 19472.,    inf,    inf,    inf,    inf,\n",
      "           inf, 51712., 56320.,    inf,    inf,    inf,    inf,  4020.,    inf,\n",
      "         3574.,    inf,    inf,    inf, 57248.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,  5392.,    inf,    inf,  1076.,\n",
      "           inf, 54272.,    inf,    inf,    inf,    inf,    inf,    inf],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([   inf, 53024.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "        54112., 10416., 40544.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf, 51328.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 59136.,    inf,    inf,    inf, 30192.,\n",
      "           inf, 18288., 21856.,    inf,  9384.,    inf,    inf,    inf,    inf,\n",
      "           inf, 16128.,    inf,    inf,    inf,    inf,    inf,    inf, 58272.,\n",
      "        50304.,    inf,    inf, 40384., 19296.,    inf,    inf, 39520., 54688.,\n",
      "           inf,    inf, 33312.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 28640.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf, 15016.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf, 56960.,    inf,    inf,\n",
      "           inf,    inf, 23184.,    inf,    inf,    inf,    inf,    inf, 19952.,\n",
      "        48928.,    inf,    inf,    inf,    inf,    inf, 21808.,    inf, 34432.,\n",
      "        49952.,    inf, 22880.,    inf, 59328.,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf, 42912.,    inf,\n",
      "           inf,    inf,    inf, 26272.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,  9264., 43936.,    inf,\n",
      "           inf, 18960.,  5456.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "        14256.,    inf,    inf,    inf,    inf,    inf,    inf,    inf, 14456.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf, 15312., 52352.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf, 12096., 55488.,    inf,    inf,\n",
      "           inf, 25136.,    inf,    inf, 41984., 34560.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 20304.,    inf,    inf,    inf, 24848.,    inf, 36864.,    inf,\n",
      "           inf,    inf, 46784.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf, 33504.,    inf,  5196.,    inf,\n",
      "           inf,    inf, 35616.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf, 28832.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf, 25280.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,  4468.,    inf,    inf,    inf,    inf, 45600.,    inf,    inf,\n",
      "           inf,  2124.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 57536.,    inf,    inf,    inf,    inf, 44000.,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf, 46464.,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "        51424.,    inf,    inf,    inf,    inf,    inf,    inf, 41792.,    inf,\n",
      "        33664.,    inf, 38016.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 60512.,    inf,    inf,    inf,    inf, 59520.,    inf,\n",
      "           inf,    inf,    inf, 13512.,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf, 58240.,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf, 61760., 33728.,    inf,    inf,    inf,    inf,    inf, 50688.,\n",
      "        18848.,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "           inf,  5952., 18560.,    inf,    inf,    inf,    inf,    inf, 34400.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf, 35680.,    inf,    inf,\n",
      "           inf, 34528., 42848.,    inf,    inf,    inf,    inf, 12944.,    inf,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,    inf,\n",
      "        63904.,    inf,    inf,    inf,    inf,    inf,    inf,    inf, 61568.,\n",
      "           inf,    inf,    inf,    inf,    inf,    inf, 37184.,    inf],\n",
      "       device='cuda:0', dtype=torch.float16)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hutch state: [tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16), tensor([[   inf,    inf,    inf,  ...,    inf,    inf,    inf],\n",
      "        [   inf, 27152., 25248.,  ...,  3976., 14440.,  3650.],\n",
      "        [   inf,    inf,    inf,  ..., 26560., 36288.,  8272.],\n",
      "        ...,\n",
      "        [   inf, 23040., 26080.,  ..., 10808.,  9288.,  2712.],\n",
      "        [   inf, 34336., 26560.,  ..., 13408., 18048.,  2380.],\n",
      "        [   inf, 28416., 33184.,  ...,  3616., 10048.,  3270.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([20064.,  9648., 20128.,  ...,  2544.,  1419., 12480.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([       inf, 2.0096e+04, 5.4496e+04,        inf,        inf, 8.3600e+03,\n",
      "        1.8352e+04, 6.0544e+04, 5.7248e+04, 3.8225e+02,        inf,        inf,\n",
      "               inf, 5.2800e+03, 5.7024e+04, 4.8608e+04, 4.7200e+03, 6.0160e+04,\n",
      "               inf, 2.1856e+04, 3.6736e+04, 1.3192e+04, 4.8384e+04, 4.1632e+04,\n",
      "               inf, 4.9728e+04,        inf, 4.8128e+04, 4.1312e+04, 5.8208e+04,\n",
      "        2.1168e+04, 9.0560e+03, 4.8880e+03, 1.1592e+04, 5.8752e+04, 4.4000e+04,\n",
      "               inf,        inf, 2.7280e+04, 2.3136e+04,        inf,        inf,\n",
      "        4.0576e+04,        inf, 1.5304e+04,        inf, 4.6432e+04, 1.3592e+04,\n",
      "               inf,        inf, 4.8064e+04, 8.8800e+03, 9.4240e+03, 5.1104e+04,\n",
      "        7.4480e+03,        inf, 4.8600e+03,        inf, 5.4880e+04, 4.2592e+04,\n",
      "        7.9280e+03,        inf, 1.8272e+04, 3.8304e+04, 5.2700e+02, 4.4672e+04,\n",
      "               inf, 4.4704e+04, 1.4056e+04, 4.6960e+03,        inf,        inf,\n",
      "        6.4320e+04, 2.1312e+04, 1.4576e+04,        inf, 3.7940e+03,        inf,\n",
      "        4.0520e+03,        inf, 4.6720e+04, 1.9040e+04,        inf, 1.7808e+04,\n",
      "        2.7408e+04, 1.0648e+04,        inf,        inf, 2.8540e+03, 7.3760e+03,\n",
      "        1.7120e+04,        inf,        inf,        inf, 1.2590e+03,        inf,\n",
      "        2.2736e+04, 6.0224e+04,        inf, 5.0048e+04, 5.0464e+04, 5.2128e+04,\n",
      "        4.5088e+04, 2.2620e+03, 3.9296e+04, 5.3696e+04, 4.5472e+04, 5.5840e+04,\n",
      "               inf, 4.2016e+04,        inf,        inf,        inf, 1.3744e+04,\n",
      "        4.3456e+04, 3.5232e+04, 5.1584e+04, 3.8176e+04, 5.2640e+04, 2.9808e+04,\n",
      "               inf, 6.0384e+04, 3.6608e+04, 2.5296e+04,        inf, 3.7536e+04,\n",
      "               inf,        inf, 6.2336e+04,        inf, 2.1152e+04, 3.8208e+04,\n",
      "               inf, 6.2688e+04,        inf,        inf,        inf, 3.0320e+04,\n",
      "        2.5760e+04,        inf, 6.4224e+04, 2.2096e+04, 1.9872e+04, 8.0320e+03,\n",
      "               inf, 9.6200e+02, 6.0550e+02,        inf,        inf, 4.9040e+03,\n",
      "        3.5520e+04, 2.0784e+04, 3.1280e+04, 3.5136e+04, 4.2050e+02, 6.4000e+04,\n",
      "        2.4880e+04, 1.9056e+04, 1.0016e+04, 3.5904e+04, 3.2496e+04, 4.2592e+04,\n",
      "               inf, 6.5504e+04, 2.5648e+04, 8.5040e+03,        inf,        inf,\n",
      "        5.3856e+04,        inf, 2.9860e+03, 5.2384e+04, 5.3824e+04, 5.3504e+04,\n",
      "               inf,        inf, 1.2352e+04, 4.3480e+03,        inf,        inf,\n",
      "               inf, 9.9440e+03,        inf,        inf, 1.8608e+04,        inf,\n",
      "        6.3264e+04, 1.5072e+04, 2.8032e+04,        inf, 6.0288e+04, 1.1216e+04,\n",
      "               inf, 5.5616e+04, 1.8781e+01, 4.6592e+04, 2.2320e+04,        inf,\n",
      "        3.4304e+04, 4.3808e+04,        inf, 2.8608e+04, 3.8592e+04, 2.4512e+04,\n",
      "        7.3550e+02, 5.0144e+04, 1.6096e+04, 2.6176e+04, 2.2048e+04, 2.9248e+04,\n",
      "        4.4000e+03,        inf,        inf, 2.4360e+03, 5.1232e+04,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 4.3424e+04,\n",
      "               inf, 1.7360e+04, 1.9840e+04, 1.4776e+04, 8.2640e+03,        inf,\n",
      "               inf, 1.6650e+03,        inf,        inf, 6.1568e+04, 1.2232e+04,\n",
      "               inf,        inf, 1.7488e+04,        inf,        inf,        inf,\n",
      "        2.0336e+04, 6.5360e+03, 1.3624e+04, 5.8496e+04, 2.2304e+04, 1.7376e+04,\n",
      "        5.6448e+04, 6.4960e+04, 1.1280e+04,        inf,        inf, 3.8560e+04,\n",
      "        8.0200e+02, 4.4960e+04, 4.1080e+03,        inf,        inf, 4.6016e+04,\n",
      "               inf, 4.2368e+04, 5.2224e+04,        inf,        inf,        inf,\n",
      "        1.4792e+04, 8.0480e+03, 4.6400e+04,        inf, 5.0464e+04, 3.5872e+04,\n",
      "               inf, 9.2880e+03, 4.4768e+04, 6.2496e+04, 1.9936e+04, 1.6896e+04,\n",
      "               inf, 1.8672e+04, 5.0208e+04,        inf,        inf, 5.0016e+04,\n",
      "               inf, 3.4432e+04, 1.0256e+04,        inf, 6.3680e+04, 1.9520e+04,\n",
      "        6.3264e+04,        inf,        inf, 3.6192e+04,        inf, 2.9952e+04,\n",
      "               inf, 3.0368e+04,        inf,        inf, 2.6600e+03,        inf,\n",
      "        6.3808e+04,        inf,        inf, 4.5800e+02, 7.9080e+03,        inf,\n",
      "               inf, 4.0032e+04, 1.6576e+04,        inf, 4.0736e+04,        inf,\n",
      "               inf,        inf,        inf, 9.9120e+03, 3.6000e+04, 6.1696e+04,\n",
      "               inf, 1.2928e+04,        inf, 2.2032e+04, 2.5140e+03, 3.7664e+04,\n",
      "               inf,        inf,        inf, 4.9216e+04, 2.6864e+04, 2.5600e+04,\n",
      "               inf,        inf, 4.2272e+04,        inf, 5.6288e+04, 5.4560e+04,\n",
      "        4.7680e+04, 2.6640e+03, 2.8928e+04, 5.9440e+03, 4.4320e+04,        inf,\n",
      "        1.6512e+04, 1.8704e+04, 7.8480e+03, 4.5792e+04, 5.0816e+04, 2.2656e+04,\n",
      "               inf, 4.9600e+03,        inf,        inf,        inf, 3.6608e+04,\n",
      "        2.1168e+04,        inf,        inf, 2.0096e+04, 4.2208e+04, 2.1536e+04,\n",
      "               inf, 6.1184e+04, 3.6896e+04,        inf, 2.2944e+04, 3.6600e+03,\n",
      "        1.9712e+04,        inf, 2.4016e+04, 6.1632e+04,        inf,        inf,\n",
      "               inf, 3.2144e+04,        inf,        inf, 4.4672e+04, 5.3472e+04,\n",
      "               inf, 6.7160e+03,        inf,        inf, 2.9456e+04,        inf,\n",
      "        3.7480e+03,        inf,        inf, 5.3920e+04, 1.1792e+04,        inf,\n",
      "               inf, 4.7232e+04, 1.8496e+04,        inf, 2.1808e+04, 6.3552e+04,\n",
      "               inf,        inf, 2.9280e+04,        inf, 3.8272e+04,        inf,\n",
      "        3.7952e+04,        inf, 4.7456e+04, 6.2336e+04, 1.3928e+04, 6.4640e+04,\n",
      "        5.4656e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 2.1696e+04, 4.8544e+04, 4.2688e+04, 5.1424e+04, 3.7460e+03,\n",
      "        2.1952e+04,        inf, 4.7072e+04, 5.1008e+04, 9.4640e+03,        inf,\n",
      "        4.0256e+04, 3.1792e+04, 4.6200e+03, 1.1090e+03, 2.4144e+04, 1.6512e+04,\n",
      "        3.6040e+03,        inf,        inf, 6.5400e+03, 8.3440e+03, 9.3040e+03,\n",
      "        2.4448e+04,        inf, 3.9200e+04,        inf, 1.6896e+04,        inf,\n",
      "               inf,        inf, 2.1376e+04, 2.5408e+04, 1.7104e+04, 1.1424e+04,\n",
      "        7.9920e+03,        inf, 3.9840e+04, 5.8464e+04, 3.8976e+04,        inf,\n",
      "               inf, 1.2104e+04,        inf,        inf, 5.5360e+04, 4.0416e+04,\n",
      "               inf, 1.6240e+04, 2.0128e+04, 5.3568e+04,        inf,        inf,\n",
      "               inf, 4.5536e+04,        inf,        inf,        inf,        inf,\n",
      "               inf, 1.8192e+04, 2.7952e+04, 2.5100e+03, 5.0680e+03,        inf,\n",
      "               inf, 5.5712e+04, 6.2176e+04, 1.2100e+02, 3.1168e+04,        inf,\n",
      "               inf,        inf,        inf, 2.5360e+04, 7.6880e+03, 2.2016e+04,\n",
      "               inf, 2.0368e+04, 5.6544e+04,        inf,        inf, 6.1376e+04,\n",
      "        1.5440e+04, 1.3656e+04,        inf, 3.0976e+04,        inf,        inf,\n",
      "        2.4000e+04, 2.2576e+04, 4.1680e+03, 2.1000e+03, 4.6592e+04, 2.3760e+04,\n",
      "        2.1056e+04, 1.8064e+04], device='cuda:0', dtype=torch.float16), tensor([       inf,        inf,        inf,        inf,        inf, 58784.0000,\n",
      "        49824.0000,        inf,        inf,        inf,        inf, 54688.0000,\n",
      "               inf,  1397.0000, 19344.0000,        inf,   937.0000,        inf,\n",
      "               inf,        inf,        inf,  7472.0000,        inf, 45152.0000,\n",
      "               inf,  5804.0000,        inf,        inf, 35936.0000, 33248.0000,\n",
      "               inf, 27584.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 16624.0000,        inf, 44352.0000,        inf,\n",
      "        50752.0000,        inf,        inf,        inf, 14880.0000,        inf,\n",
      "               inf,        inf, 18112.0000,  1607.0000,  3450.0000,        inf,\n",
      "        29552.0000,        inf,        inf,        inf,        inf, 11560.0000,\n",
      "        34080.0000,        inf, 11192.0000,        inf,        inf,        inf,\n",
      "               inf, 61664.0000,        inf,        inf,        inf, 65376.0000,\n",
      "        45120.0000, 52768.0000,        inf,        inf, 54464.0000,        inf,\n",
      "               inf, 21296.0000,        inf,        inf,        inf,        inf,\n",
      "        65120.0000, 46080.0000,        inf,        inf,        inf, 36416.0000,\n",
      "               inf,        inf,  4164.0000,        inf, 35648.0000,        inf,\n",
      "        18400.0000, 31104.0000,        inf,  3252.0000, 24256.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 18176.0000,        inf,        inf,        inf, 56960.0000,\n",
      "               inf,        inf, 34144.0000,        inf,        inf, 51296.0000,\n",
      "         2566.0000,        inf,        inf, 16336.0000,        inf,        inf,\n",
      "         5788.0000, 15200.0000,  8256.0000,        inf,        inf, 40320.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 61696.0000, 41824.0000,        inf, 31952.0000,\n",
      "               inf,        inf,        inf, 53248.0000,        inf, 21760.0000,\n",
      "               inf,        inf, 17792.0000,        inf, 53184.0000, 16912.0000,\n",
      "               inf,        inf,        inf, 12136.0000, 56032.0000,        inf,\n",
      "               inf,        inf,        inf,  4168.0000,        inf,        inf,\n",
      "               inf,        inf, 25648.0000, 25888.0000,        inf,        inf,\n",
      "        44096.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        41792.0000, 22592.0000,        inf,        inf,  3008.0000,        inf,\n",
      "               inf, 17776.0000, 10280.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf, 40640.0000,        inf,\n",
      "               inf, 54656.0000,        inf,        inf,        inf, 19472.0000,\n",
      "               inf,        inf,        inf, 39872.0000, 31152.0000, 30128.0000,\n",
      "          379.0000,        inf, 60384.0000, 24096.0000,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,  7104.0000, 53344.0000,        inf, 25952.0000,        inf,\n",
      "               inf,        inf,        inf,        inf, 36992.0000,        inf,\n",
      "               inf,        inf,        inf,        inf, 52416.0000,        inf,\n",
      "        12200.0000,        inf,        inf, 55040.0000, 56000.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 19904.0000, 58656.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 17728.0000,        inf,        inf,\n",
      "         7428.0000,        inf, 62464.0000,        inf,        inf,        inf,\n",
      "               inf, 38880.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 56576.0000,        inf, 47936.0000,        inf, 21888.0000,\n",
      "               inf,        inf,        inf,        inf, 17984.0000,  3388.0000,\n",
      "        51808.0000,        inf, 38432.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 52064.0000, 51392.0000, 65120.0000,\n",
      "               inf,        inf,        inf, 26528.0000,        inf,        inf,\n",
      "               inf, 45760.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 18784.0000,        inf, 57888.0000,\n",
      "               inf, 29968.0000,        inf, 17216.0000,        inf,        inf,\n",
      "               inf, 27648.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 26592.0000, 34496.0000,        inf, 23424.0000, 53216.0000,\n",
      "               inf,        inf,  3460.0000,        inf,        inf,        inf,\n",
      "               inf, 65120.0000,        inf, 42304.0000,        inf,        inf,\n",
      "               inf, 58848.0000,        inf, 57056.0000,        inf,  7248.0000,\n",
      "        17712.0000,        inf,        inf, 39520.0000,        inf,        inf,\n",
      "               inf,        inf,   372.5000, 61536.0000, 62752.0000, 29152.0000,\n",
      "        53120.0000,        inf,   562.0000, 22640.0000,        inf,        inf,\n",
      "               inf, 65152.0000,        inf, 56736.0000,        inf,        inf,\n",
      "               inf,  9536.0000,        inf,        inf,        inf,        inf,\n",
      "        40352.0000,        inf,        inf, 62048.0000,        inf, 62976.0000,\n",
      "               inf, 17280.0000, 43808.0000,        inf, 27152.0000,        inf,\n",
      "               inf,        inf, 44864.0000,        inf, 25664.0000,        inf,\n",
      "          815.0000,        inf, 63232.0000,        inf,        inf, 39264.0000,\n",
      "               inf,        inf, 61728.0000,        inf,        inf,        inf,\n",
      "               inf, 11056.0000,        inf, 15672.0000, 52256.0000, 26128.0000,\n",
      "               inf,        inf,        inf,        inf, 61184.0000,        inf,\n",
      "               inf,        inf, 57056.0000,        inf,        inf, 58784.0000,\n",
      "        52320.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        32800.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 63968.0000,        inf, 59264.0000,        inf,\n",
      "        33920.0000,  8328.0000,        inf,        inf, 31584.0000,        inf,\n",
      "               inf,        inf,        inf,        inf, 48480.0000, 35904.0000,\n",
      "               inf, 40640.0000,  3140.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 23248.0000, 36448.0000,        inf, 36960.0000,        inf,\n",
      "               inf,        inf,        inf, 40320.0000, 14040.0000,        inf,\n",
      "               inf,        inf, 37728.0000, 47552.0000, 41408.0000,  3156.0000,\n",
      "               inf,        inf, 60000.0000,        inf,        inf,   701.0000,\n",
      "         1574.0000,        inf,        inf, 27328.0000, 37120.0000,        inf,\n",
      "        44704.0000,        inf, 60832.0000, 30800.0000, 59776.0000, 24368.0000,\n",
      "        14032.0000, 63968.0000], device='cuda:0', dtype=torch.float16)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hutch state: [tensor([[ 4480.,    inf,    inf,  ...,    inf,    inf,    inf],\n",
      "        [   inf,    inf, 35904.,  ...,    inf,    inf,    inf],\n",
      "        [39232., 41536.,    inf,  ..., 25184.,    inf, 36384.],\n",
      "        ...,\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.],\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.],\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([[   inf,    inf,    inf,  ..., 19616.,    inf,    inf],\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.],\n",
      "        [   inf,    inf,  3810.,  ..., 61888.,    inf,    inf],\n",
      "        ...,\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.],\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.],\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([[   inf,    inf,    inf,  ...,    inf,    inf,    inf],\n",
      "        [11120.,  4776.,  3514.,  ..., 24192.,  1441.,    51.],\n",
      "        [  112.,   448., 16008.,  ..., 45568.,  7216.,  2328.],\n",
      "        ...,\n",
      "        [ 4872.,  2038.,  8368.,  ..., 10792.,  2622.,  2750.],\n",
      "        [12688.,  5928.,  1156.,  ..., 30496.,  2674.,   193.],\n",
      "        [ 1336.,  1165.,  6248.,  ..., 15168.,  3256.,  1883.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([  594.5000,  5716.0000, 17088.0000,  ...,  9088.0000,  8984.0000,\n",
      "         6952.0000], device='cuda:0', dtype=torch.float16), tensor([[2.0438e+01, 2.0200e+02, 6.3400e+02,  ..., 1.6230e+03, 2.5140e+03,\n",
      "         2.5780e+03],\n",
      "        [1.8088e+02, 6.1625e+01, 7.1875e+01,  ..., 1.2690e+03, 1.6360e+03,\n",
      "         1.4450e+03],\n",
      "        [2.2531e+01, 8.5750e+01, 3.3050e+02,  ..., 1.7190e+03, 2.1840e+03,\n",
      "         2.4300e+03],\n",
      "        ...,\n",
      "        [2.2920e+03, 4.8240e+03, 8.6000e+03,  ..., 4.5664e+04, 4.8448e+04,\n",
      "         4.6144e+04],\n",
      "        [2.1580e+03, 5.4500e+01, 1.1350e+03,  ..., 1.9376e+04, 1.6928e+04,\n",
      "         2.1200e+04],\n",
      "        [8.9760e+03, 2.0470e+03, 5.8880e+03,  ..., 4.9824e+04, 5.3344e+04,\n",
      "         5.4400e+04]], device='cuda:0', dtype=torch.float16), tensor([ 2140.,  1474.,  1921.,  ..., 41664., 19008., 52416.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[19392.,  1738., 29344.,  ..., 12720., 11848.,  9824.],\n",
      "        [ 3708.,   754., 17328.,  ...,   632.,  4940.,  2760.],\n",
      "        [59040.,  3336.,  7832.,  ..., 48736., 12008.,  8312.],\n",
      "        ...,\n",
      "        [60448.,  3960.,  5592.,  ..., 51968., 13176., 10208.],\n",
      "        [   inf,  1264., 64832.,  ...,    inf, 36576., 22512.],\n",
      "        [   inf, 10416.,   448.,  ...,    inf, 19392., 18336.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([29232.0000,  7268.0000, 54880.0000, 63424.0000,        inf, 54208.0000,\n",
      "        33536.0000, 48224.0000,        inf, 13712.0000, 31584.0000,        inf,\n",
      "               inf,        inf, 60064.0000, 35392.0000,  7340.0000, 10344.0000,\n",
      "               inf, 53248.0000,  7460.0000,        inf,        inf, 63648.0000,\n",
      "        48480.0000, 34944.0000, 25152.0000,  7180.0000, 32688.0000, 44160.0000,\n",
      "        34592.0000, 37536.0000, 30240.0000, 13992.0000, 30912.0000,        inf,\n",
      "        56192.0000,  4760.0000,  1886.0000, 62880.0000,        inf, 20864.0000,\n",
      "               inf,  1035.0000,  7796.0000, 58464.0000,        inf,        inf,\n",
      "        49664.0000,  7416.0000,        inf,        inf,  2350.0000, 26336.0000,\n",
      "               inf,        inf, 34784.0000,        inf,   339.7500,        inf,\n",
      "               inf, 44800.0000,  2966.0000, 25200.0000,  4908.0000,        inf,\n",
      "               inf, 36192.0000,   889.5000,        inf,        inf,        inf,\n",
      "        10592.0000,  4042.0000, 29440.0000,   165.5000,        inf,        inf,\n",
      "         3310.0000, 59744.0000,        inf,        inf, 32416.0000, 14808.0000,\n",
      "        52448.0000,        inf,        inf,  3352.0000, 13808.0000, 34496.0000,\n",
      "               inf,        inf,        inf, 40288.0000, 34816.0000,        inf,\n",
      "        10112.0000, 51424.0000, 22256.0000, 14248.0000, 31904.0000,        inf,\n",
      "        38624.0000, 30688.0000,        inf,        inf,        inf,        inf,\n",
      "        62784.0000, 40032.0000, 44096.0000,        inf, 45216.0000,        inf,\n",
      "        19808.0000, 53056.0000, 16480.0000, 51456.0000, 51424.0000, 41792.0000,\n",
      "               inf, 37856.0000, 53152.0000,        inf,  9416.0000, 32896.0000,\n",
      "        63040.0000,  1515.0000,        inf, 52960.0000, 32192.0000,        inf,\n",
      "         6860.0000,        inf, 41440.0000,  9800.0000, 52608.0000,  1455.0000,\n",
      "        22336.0000, 25056.0000, 41920.0000,        inf,   343.0000,  9368.0000,\n",
      "               inf,        inf, 50880.0000, 14472.0000,        inf, 26976.0000,\n",
      "               inf,  2522.0000,        inf,        inf, 22448.0000,        inf,\n",
      "        20032.0000,        inf, 64640.0000,  1767.0000, 48288.0000, 20336.0000,\n",
      "        39360.0000,        inf, 23728.0000,        inf,        inf, 40192.0000,\n",
      "        29680.0000,        inf, 60768.0000,        inf,        inf,        inf,\n",
      "         9288.0000, 24288.0000, 32688.0000,        inf, 38016.0000,        inf,\n",
      "               inf, 35872.0000,        inf, 34784.0000,        inf,        inf,\n",
      "               inf, 20416.0000,        inf,  5080.0000, 16848.0000,        inf,\n",
      "        55232.0000,  5852.0000, 40384.0000,        inf,        inf, 39648.0000,\n",
      "        10472.0000,  5692.0000, 29280.0000,        inf,        inf, 64096.0000,\n",
      "        64128.0000, 63776.0000,        inf,   372.7500, 24336.0000, 52192.0000,\n",
      "               inf, 55712.0000, 29072.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 48128.0000, 62880.0000,        inf,\n",
      "        48768.0000, 25936.0000, 16368.0000, 28784.0000,        inf,        inf,\n",
      "        49568.0000, 50560.0000, 62976.0000, 13536.0000, 38048.0000,        inf,\n",
      "         3718.0000,        inf,  4028.0000, 28352.0000,        inf, 42944.0000,\n",
      "        35648.0000, 33792.0000, 10264.0000, 49632.0000,        inf, 64800.0000,\n",
      "        14688.0000,        inf, 64352.0000, 46176.0000,        inf, 18656.0000,\n",
      "         7500.0000,  7792.0000,   467.2500, 33248.0000,        inf, 41152.0000,\n",
      "        60544.0000,        inf, 11112.0000, 38784.0000,        inf,        inf,\n",
      "         5692.0000,   435.0000, 29920.0000, 20480.0000,        inf, 16448.0000,\n",
      "        45600.0000,        inf,        inf, 39488.0000, 22000.0000, 32416.0000,\n",
      "         6960.0000, 10536.0000,        inf, 29008.0000,        inf, 55392.0000,\n",
      "         1348.0000, 28976.0000, 18336.0000, 22784.0000,        inf,  3498.0000,\n",
      "        14008.0000,  9344.0000, 16848.0000, 18912.0000, 53696.0000,        inf,\n",
      "        63296.0000, 18304.0000,  9008.0000,        inf,        inf,  9624.0000,\n",
      "               inf,        inf,        inf, 15240.0000,        inf, 23056.0000,\n",
      "         5852.0000, 14792.0000, 62272.0000,        inf, 38816.0000, 12568.0000,\n",
      "               inf,        inf,        inf,  7260.0000,        inf,  4724.0000,\n",
      "               inf, 12808.0000,        inf, 53632.0000,        inf,        inf,\n",
      "        55840.0000,  5044.0000, 53952.0000, 41696.0000, 52832.0000, 39936.0000,\n",
      "        11288.0000, 53184.0000, 36352.0000, 63296.0000, 41760.0000,        inf,\n",
      "        34016.0000, 64512.0000, 27168.0000,        inf,        inf,        inf,\n",
      "               inf, 41856.0000, 20736.0000,        inf,  6736.0000,  5472.0000,\n",
      "        57792.0000,        inf,        inf,  4616.0000,        inf, 42464.0000,\n",
      "        30016.0000, 56800.0000,        inf,  4896.0000,  5772.0000,        inf,\n",
      "               inf, 15944.0000, 38368.0000, 18544.0000,        inf, 25600.0000,\n",
      "        43136.0000,        inf,        inf, 56928.0000, 35552.0000, 30240.0000,\n",
      "         1335.0000,   142.3750,        inf,        inf, 36672.0000,        inf,\n",
      "               inf,        inf,        inf,  6816.0000,  5588.0000,  6616.0000,\n",
      "               inf, 26448.0000, 52736.0000,        inf, 27552.0000,        inf,\n",
      "               inf, 32016.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 27872.0000, 26320.0000,        inf,        inf,        inf,\n",
      "               inf, 58880.0000, 34080.0000,  4300.0000, 39904.0000, 19488.0000,\n",
      "        36800.0000,        inf, 17824.0000, 10456.0000,        inf, 40352.0000,\n",
      "               inf,        inf, 62368.0000,  6456.0000,  3834.0000,        inf,\n",
      "               inf, 24800.0000,        inf, 42272.0000,        inf,        inf,\n",
      "               inf, 17600.0000,        inf, 30048.0000, 18864.0000, 42880.0000,\n",
      "               inf, 11968.0000, 31008.0000, 54464.0000, 39968.0000, 10056.0000,\n",
      "               inf, 16000.0000,        inf, 55712.0000, 37280.0000,        inf,\n",
      "        48768.0000,        inf, 63744.0000, 31792.0000,        inf,        inf,\n",
      "        20304.0000,        inf,  7820.0000, 33472.0000,  8424.0000, 25648.0000,\n",
      "               inf, 23440.0000, 51936.0000, 31840.0000, 44896.0000,        inf,\n",
      "        45248.0000, 13560.0000, 13792.0000,        inf, 27456.0000,  2330.0000,\n",
      "               inf, 35616.0000, 63776.0000, 10656.0000, 19680.0000,  9504.0000,\n",
      "        57248.0000, 56512.0000, 25504.0000, 63424.0000,        inf, 44928.0000,\n",
      "        20704.0000,        inf, 32000.0000, 59584.0000,  9040.0000,        inf,\n",
      "        25728.0000,        inf, 64416.0000,        inf, 12576.0000, 38752.0000,\n",
      "        25824.0000, 12752.0000,  8720.0000,        inf,        inf, 49792.0000,\n",
      "        21216.0000,        inf,        inf, 61472.0000,  5404.0000,        inf,\n",
      "        63712.0000,  5796.0000, 31968.0000,        inf, 37568.0000, 64192.0000,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([[ 2800.0000,  2888.0000,  1020.0000,  ...,  4252.0000,  8544.0000,\n",
      "          6716.0000],\n",
      "        [ 2044.0000, 29136.0000, 12088.0000,  ..., 17120.0000, 39840.0000,\n",
      "         27856.0000],\n",
      "        [ 2862.0000,  1409.0000,  2004.0000,  ...,  3366.0000,  7372.0000,\n",
      "          4788.0000],\n",
      "        ...,\n",
      "        [10096.0000,  9120.0000,  1748.0000,  ..., 18976.0000, 40704.0000,\n",
      "         33440.0000],\n",
      "        [  936.0000,   493.2500,   943.0000,  ...,   645.0000,   509.5000,\n",
      "          1244.0000],\n",
      "        [ 4536.0000, 11160.0000,   345.5000,  ...,  6720.0000,  5284.0000,\n",
      "          6428.0000]], device='cuda:0', dtype=torch.float16), tensor([ 6752.0000, 34848.0000,  5420.0000,  ..., 31056.0000,   553.5000,\n",
      "         9456.0000], device='cuda:0', dtype=torch.float16), tensor([[ 4396.0000,  7016.0000,    77.5000,  ...,  2972.0000,    41.1250,\n",
      "          2080.0000],\n",
      "        [ 2166.0000,  4128.0000,    48.5000,  ...,  1320.0000,   119.8125,\n",
      "          8440.0000],\n",
      "        [ 3354.0000, 11512.0000,   421.7500,  ...,  6172.0000,   501.5000,\n",
      "         19440.0000],\n",
      "        ...,\n",
      "        [ 2926.0000,  9760.0000,   542.5000,  ...,  7104.0000,   617.0000,\n",
      "         15104.0000],\n",
      "        [ 1607.0000, 18912.0000,    94.0000,  ..., 18608.0000,   913.0000,\n",
      "         11904.0000],\n",
      "        [ 6020.0000, 16464.0000,  2186.0000,  ...,  7216.0000,  1147.0000,\n",
      "         22848.0000]], device='cuda:0', dtype=torch.float16), tensor([1.7888e+04, 2.2640e+04, 5.2832e+04, 5.7440e+04,        inf, 2.6432e+04,\n",
      "        3.6544e+04, 3.5360e+04, 5.7600e+04, 2.7104e+04, 8.1600e+03,        inf,\n",
      "        3.9872e+04,        inf, 6.5408e+04, 1.2928e+04, 5.5280e+03, 3.8272e+04,\n",
      "               inf, 4.2720e+04, 1.3064e+04,        inf,        inf, 6.5440e+04,\n",
      "        3.6640e+04, 1.3176e+04, 1.5830e+03, 4.9240e+03, 3.0576e+04, 5.8944e+04,\n",
      "        9.9120e+03, 4.4960e+03,        inf, 3.0816e+04, 3.4656e+04, 5.3984e+04,\n",
      "        2.9712e+04, 1.4376e+04, 1.8992e+04,        inf,        inf, 2.2576e+04,\n",
      "               inf, 8.7280e+03, 1.6328e+04, 4.1056e+04,        inf,        inf,\n",
      "        2.6480e+04, 2.1720e+03,        inf,        inf, 2.0660e+03, 2.1904e+04,\n",
      "        5.1296e+04,        inf, 3.2080e+04, 4.8544e+04, 8.0080e+03, 4.7328e+04,\n",
      "               inf, 4.3232e+04, 6.3150e+02, 3.1168e+04, 2.4480e+04, 5.7344e+04,\n",
      "               inf, 4.2528e+04, 2.4096e+04,        inf, 3.4336e+04, 6.0384e+04,\n",
      "        2.1072e+04, 1.1336e+04, 5.5104e+04, 4.9240e+03,        inf,        inf,\n",
      "        3.5648e+04, 2.8592e+04,        inf,        inf, 3.5584e+04, 4.2400e+04,\n",
      "               inf,        inf,        inf, 2.1888e+04, 1.1140e+03, 3.4112e+04,\n",
      "        3.3952e+04, 6.4000e+04,        inf, 6.9880e+03, 4.8832e+04,        inf,\n",
      "        1.2344e+04, 2.7744e+04, 2.0592e+04, 3.2440e+03, 1.3024e+04, 5.1104e+04,\n",
      "        2.3248e+04, 3.2832e+04,        inf, 2.2048e+04, 6.0960e+04, 5.0464e+04,\n",
      "        4.4928e+04, 3.4340e+03, 2.3136e+04,        inf, 2.7776e+04,        inf,\n",
      "        7.1520e+03, 8.8640e+03, 7.1280e+03, 5.7984e+04, 4.1152e+04, 4.1920e+04,\n",
      "               inf, 2.5936e+04, 6.3424e+04,        inf, 1.2416e+04,        inf,\n",
      "        5.4560e+04, 6.2840e+03, 5.2512e+04, 5.3568e+04, 4.3968e+04,        inf,\n",
      "        1.4584e+04,        inf, 5.5808e+04, 1.8690e+03, 3.9328e+04, 2.5232e+04,\n",
      "        1.5738e+02, 1.4400e+04, 3.4048e+04, 5.1136e+04, 7.3800e+03, 9.2600e+02,\n",
      "               inf,        inf, 3.9904e+04, 2.8380e+03,        inf,        inf,\n",
      "               inf, 2.4704e+04, 5.3408e+04,        inf, 1.1680e+04,        inf,\n",
      "        2.3136e+04,        inf,        inf, 8.4320e+03, 3.7280e+04, 1.1760e+04,\n",
      "        8.7920e+03,        inf, 1.5992e+04, 3.5040e+04,        inf, 7.1880e+03,\n",
      "        2.5792e+04, 3.7184e+04, 4.0832e+04,        inf, 5.1904e+04, 5.6608e+04,\n",
      "        4.3392e+04, 4.5952e+04, 1.1600e+03,        inf, 3.3728e+04,        inf,\n",
      "               inf, 3.2480e+04,        inf, 3.1424e+04, 6.2560e+04,        inf,\n",
      "        4.7616e+04, 3.5392e+04,        inf, 1.5632e+04, 1.7808e+04,        inf,\n",
      "        2.9824e+04, 3.7125e+02, 3.1792e+04, 5.6224e+04,        inf, 1.9516e+01,\n",
      "        1.6064e+04, 2.0032e+04, 6.3160e+03, 5.6416e+04,        inf,        inf,\n",
      "        4.7680e+04, 5.0400e+04,        inf, 1.3864e+04, 1.5736e+04, 4.9248e+04,\n",
      "               inf,        inf, 2.7536e+04,        inf,        inf,        inf,\n",
      "               inf, 4.4544e+04, 6.3648e+04, 3.6992e+04, 5.0112e+04,        inf,\n",
      "        4.6464e+04, 1.8576e+04, 2.1488e+04, 3.6000e+04,        inf,        inf,\n",
      "        4.3840e+04, 3.6640e+04, 4.4160e+04, 1.4360e+04, 1.7580e+03,        inf,\n",
      "        8.8160e+03,        inf, 1.2936e+04, 2.7776e+04,        inf, 1.5128e+04,\n",
      "        1.6832e+04, 3.2144e+04, 2.9936e+04,        inf, 6.0512e+04, 3.8784e+04,\n",
      "        3.1760e+03,        inf, 6.3840e+04,        inf,        inf, 4.7360e+04,\n",
      "        1.4936e+04, 8.9840e+03, 5.5750e+02, 2.1008e+04, 5.4368e+04, 4.4512e+04,\n",
      "        6.1184e+04, 5.8144e+04, 2.2912e+04, 2.6464e+04,        inf, 3.5360e+04,\n",
      "        7.2950e+02, 8.3760e+03, 2.1248e+04, 1.7070e+03,        inf, 1.6864e+04,\n",
      "        6.2368e+04,        inf,        inf, 1.7030e+03, 1.5128e+04, 2.0992e+04,\n",
      "        2.4256e+04, 1.0824e+04, 5.8464e+04, 1.9344e+04,        inf,        inf,\n",
      "        1.5104e+04, 1.7248e+04, 3.2256e+04, 3.3440e+04,        inf, 1.5370e+03,\n",
      "        1.8032e+04, 5.8720e+03, 9.4320e+03, 3.2240e+04, 3.6736e+04, 5.3024e+04,\n",
      "               inf, 2.5296e+04, 1.8608e+04, 5.8272e+04,        inf, 1.0080e+04,\n",
      "        5.6384e+04, 3.9392e+04,        inf, 1.9008e+04,        inf, 2.1360e+04,\n",
      "        1.3888e+04, 1.1832e+04, 5.6480e+04,        inf, 2.6944e+04, 5.6640e+04,\n",
      "        6.1632e+04,        inf, 5.9104e+04, 8.7120e+03,        inf, 7.2280e+03,\n",
      "               inf, 4.2200e+03,        inf, 3.3888e+04,        inf, 5.3536e+04,\n",
      "        3.6960e+04, 4.7200e+03, 4.6944e+04, 3.6640e+03, 5.0176e+04, 3.7344e+04,\n",
      "        8.9680e+03, 7.8400e+03, 1.4568e+04, 4.1568e+04,        inf, 6.4896e+04,\n",
      "        1.5660e+03, 4.2432e+04, 1.4768e+04,        inf, 3.1312e+04,        inf,\n",
      "               inf, 1.1952e+04, 2.8784e+04,        inf, 3.6880e+03, 9.1840e+03,\n",
      "        3.2336e+04,        inf,        inf, 9.1440e+03, 5.2480e+04, 1.1128e+04,\n",
      "        3.1952e+04, 3.2064e+04, 2.7904e+04, 1.7056e+04, 1.3320e+04,        inf,\n",
      "               inf, 2.0992e+04, 3.6000e+04, 8.5120e+03,        inf, 3.7312e+04,\n",
      "        3.7184e+04,        inf, 6.0512e+04, 3.3792e+04, 2.3328e+04, 1.6960e+04,\n",
      "        7.4040e+03, 2.4320e+04, 5.8816e+04,        inf, 3.4816e+04,        inf,\n",
      "               inf, 4.5920e+04,        inf, 3.0950e+02, 1.3896e+04, 1.4704e+04,\n",
      "               inf, 1.7616e+04, 4.9056e+04,        inf, 1.7008e+04,        inf,\n",
      "               inf, 4.4064e+04,        inf,        inf, 6.4064e+04,        inf,\n",
      "        4.7520e+04, 7.8000e+03, 2.0144e+04,        inf,        inf, 2.2672e+04,\n",
      "        5.3696e+04, 5.1040e+04, 1.4712e+04, 1.2536e+04, 2.4416e+04, 9.2240e+03,\n",
      "        5.0144e+04,        inf, 1.3424e+04, 1.7616e+04,        inf, 6.1440e+04,\n",
      "               inf, 5.2096e+04, 2.7056e+04, 2.1088e+04, 2.2704e+04,        inf,\n",
      "        2.3200e+04, 1.5870e+03,        inf, 9.9040e+03, 4.3328e+04, 4.9664e+04,\n",
      "               inf, 2.0768e+04,        inf, 5.5264e+04, 2.0256e+04, 2.9632e+04,\n",
      "               inf, 1.5400e+04, 3.6300e+03, 1.9312e+04, 5.3920e+04, 1.3744e+04,\n",
      "        5.5296e+04, 1.7888e+04,        inf, 3.7632e+04, 3.3632e+04,        inf,\n",
      "        3.1376e+04,        inf, 3.1952e+04, 2.6688e+04,        inf, 4.4192e+04,\n",
      "        1.7264e+04,        inf, 1.7024e+04, 4.5000e+03, 9.0880e+03, 1.3928e+04,\n",
      "               inf, 2.8288e+04,        inf, 3.7952e+04, 1.7904e+04,        inf,\n",
      "        3.0544e+04, 2.4704e+04, 8.6960e+03, 6.4448e+04, 1.1888e+04, 1.8288e+04,\n",
      "               inf, 1.8592e+04, 3.6576e+04, 1.3312e+04, 4.8840e+03, 2.7504e+04,\n",
      "        2.2160e+04, 5.1968e+04, 1.0704e+04, 5.2544e+04, 5.9264e+04, 5.0752e+04,\n",
      "        1.1744e+04, 5.4816e+04, 2.9968e+04, 6.3360e+04, 6.1200e+03,        inf,\n",
      "        2.3408e+04, 2.8560e+04, 5.3120e+04,        inf, 3.8060e+03, 2.7408e+04,\n",
      "        3.0112e+04, 1.5296e+04, 1.0712e+04,        inf, 6.5088e+04, 3.3952e+04,\n",
      "        1.7770e+03, 5.4656e+04,        inf,        inf, 1.3352e+04,        inf,\n",
      "        1.8048e+04, 4.3800e+03, 1.3592e+04,        inf, 3.7184e+04, 5.6032e+04,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([7.5680e+03, 4.3360e+03, 1.1648e+04, 2.3632e+04, 1.9312e+04, 1.5128e+04,\n",
      "        6.1400e+02, 1.8192e+04, 2.6208e+04, 4.2650e+02, 6.2320e+03, 3.3600e+04,\n",
      "        5.2768e+04, 4.3424e+04, 1.7456e+04, 1.1152e+04, 2.7220e+03, 6.6720e+03,\n",
      "        5.0624e+04, 2.2656e+04, 2.9440e+03,        inf, 2.6032e+04, 3.7568e+04,\n",
      "        2.9456e+04, 2.0144e+04, 7.2240e+03, 4.8880e+03, 9.6080e+03,        inf,\n",
      "        1.5400e+04, 4.5856e+04, 1.9568e+04, 1.9104e+04, 1.2608e+04, 7.3640e+03,\n",
      "        1.5656e+04, 1.6560e+04, 1.6232e+04, 2.6160e+04, 3.3248e+04, 3.7340e+03,\n",
      "        2.6740e+03, 5.5120e+03, 1.7040e+04, 2.7152e+04, 5.1904e+04, 3.7824e+04,\n",
      "        2.1920e+04, 4.7680e+03,        inf, 5.0976e+04, 7.0480e+03, 3.4840e+03,\n",
      "        1.8944e+04, 3.8560e+04, 1.7470e+03, 1.6600e+03, 1.3440e+03, 4.2528e+04,\n",
      "        8.6080e+03, 2.9856e+04, 2.4180e+03, 4.8520e+03, 3.2940e+03, 1.9008e+04,\n",
      "               inf, 1.9792e+04, 8.4720e+03, 4.7744e+04, 9.8000e+03, 1.6864e+04,\n",
      "        1.7010e+03, 2.1808e+04, 6.4400e+03, 1.2680e+04, 2.2880e+04, 3.7760e+04,\n",
      "        1.2416e+04, 8.8000e+03, 1.3200e+04, 4.2080e+04, 2.7248e+04, 3.9180e+03,\n",
      "        1.4288e+04, 4.0096e+04, 3.1072e+04, 7.8600e+03, 3.4680e+03, 5.3200e+03,\n",
      "        1.6464e+04, 3.9072e+04, 1.1696e+04, 1.9600e+04, 1.1472e+04,        inf,\n",
      "        9.6240e+03, 1.5296e+04, 1.1848e+04, 2.7104e+04, 1.0424e+04, 3.5904e+04,\n",
      "        2.3980e+03, 1.0576e+04, 5.8800e+03, 2.8752e+04, 4.3616e+04, 1.1072e+04,\n",
      "               inf, 2.4660e+03, 2.7760e+04, 3.2720e+04, 2.6500e+01, 2.7825e+02,\n",
      "        6.6800e+03, 9.0400e+03, 4.5400e+03, 9.6720e+03, 2.1160e+03, 6.2000e+03,\n",
      "        3.7120e+04, 1.2584e+04, 7.1920e+03, 6.4704e+04, 5.1520e+03, 1.7664e+04,\n",
      "        2.1392e+04, 2.0660e+03, 6.2080e+03, 1.7888e+04, 1.3168e+04, 4.0160e+04,\n",
      "        1.0616e+04, 1.3632e+04, 2.5400e+03, 2.2860e+03, 6.4400e+03, 1.6590e+03,\n",
      "        8.7760e+03, 7.7880e+03, 1.5672e+04, 3.0144e+04, 1.3544e+04, 1.9290e+03,\n",
      "        3.8275e+02, 5.4000e+03, 9.7120e+03, 3.8580e+03, 1.3150e+03, 8.2560e+03,\n",
      "        3.1100e+03, 1.1312e+04, 1.6768e+04, 3.5552e+04, 2.4680e+03, 7.3960e+03,\n",
      "        9.4000e+03,        inf, 1.0768e+04, 4.4520e+03, 3.5328e+04, 9.9450e+02,\n",
      "        4.3072e+04,        inf, 1.6600e+02, 1.1496e+04, 5.3728e+04, 1.1384e+04,\n",
      "        3.4160e+03, 1.8820e+03, 7.0880e+03,        inf, 6.1680e+03, 3.1216e+04,\n",
      "        1.6860e+03, 1.8336e+04, 1.4784e+04, 3.9744e+04, 2.8000e+04, 1.2224e+04,\n",
      "               inf, 7.0640e+03, 2.9080e+03, 1.4544e+04, 2.4704e+04, 4.4032e+04,\n",
      "        2.8880e+04, 6.7520e+03,        inf, 4.0680e+03, 1.0960e+03, 1.2928e+04,\n",
      "        3.6448e+04, 9.2000e+03, 2.2048e+04, 1.4848e+04, 1.2672e+04, 9.0400e+03,\n",
      "        3.2800e+03, 2.1820e+03, 3.7792e+04, 2.5020e+03, 2.0016e+04, 2.0592e+04,\n",
      "        6.8040e+03, 1.2248e+04, 3.4816e+04, 1.3030e+03, 1.5776e+04, 1.8624e+04,\n",
      "        3.8496e+04, 3.8688e+04, 3.6560e+03, 8.1480e+03, 3.2960e+04,        inf,\n",
      "               inf, 4.9880e+03, 5.4368e+04, 7.3600e+03, 9.8320e+03, 4.7360e+04,\n",
      "        3.0752e+04, 2.2576e+04, 1.9136e+04, 1.0296e+04, 5.1680e+04, 5.3792e+04,\n",
      "        2.1344e+04, 2.9040e+04, 4.2304e+04, 7.6000e+03, 9.7520e+03, 1.7664e+04,\n",
      "        9.6500e+02,        inf, 2.1480e+03, 1.3624e+04, 3.2176e+04, 1.2224e+04,\n",
      "        1.0736e+04, 5.9000e+03, 2.6280e+03, 6.8300e+02, 4.1600e+04, 2.6032e+04,\n",
      "        6.5840e+03,        inf, 2.0416e+04, 4.1344e+04, 5.6608e+04, 4.8640e+03,\n",
      "        1.8020e+03, 3.4900e+03, 8.5100e+02, 3.6940e+03, 4.0608e+04, 8.3040e+03,\n",
      "        2.5888e+04, 3.2448e+04, 4.1560e+03, 3.5296e+04, 2.8120e+03, 3.4080e+04,\n",
      "        4.1400e+02, 9.2240e+03, 2.2160e+04, 5.1720e+03, 5.7760e+04, 7.2320e+03,\n",
      "        1.5152e+04, 5.0624e+04,        inf, 2.9648e+04, 1.5368e+04, 4.8000e+04,\n",
      "        1.2390e+03, 1.7264e+04, 2.5520e+04, 7.3880e+03, 4.5280e+04, 2.2080e+04,\n",
      "        4.7440e+03, 1.7168e+04, 4.6800e+03, 1.5096e+04,        inf, 5.9720e+03,\n",
      "        1.8256e+04, 8.6900e+02, 4.7440e+03, 1.0624e+04, 1.5000e+04, 1.9904e+04,\n",
      "        3.0896e+04, 3.5600e+03, 1.2296e+04,        inf, 6.2592e+04, 1.4784e+04,\n",
      "        4.1344e+04, 8.7600e+03, 3.7888e+04, 6.7840e+03, 1.9824e+04, 9.0320e+03,\n",
      "        2.3225e+02, 4.5600e+03, 2.3648e+04, 5.1840e+04, 6.3200e+03, 1.4400e+04,\n",
      "        2.2112e+04,        inf, 9.3840e+03, 4.9360e+03, 2.3808e+04, 2.0560e+04,\n",
      "        1.5672e+04, 3.8660e+03, 4.6048e+04, 6.2600e+03, 1.5936e+04, 1.1300e+03,\n",
      "        6.4520e+03, 4.3104e+04, 5.7120e+04, 7.8080e+03, 1.7440e+04, 6.0960e+03,\n",
      "        4.3120e+03, 3.1664e+04, 1.6272e+04, 4.1664e+04, 3.1984e+04,        inf,\n",
      "        5.4720e+04, 3.4816e+04, 3.8688e+04, 2.9200e+04, 5.0816e+04, 3.7568e+04,\n",
      "        4.4000e+04, 2.6320e+04, 1.2020e+03, 1.3296e+04, 7.5280e+03, 2.9250e+01,\n",
      "        1.1060e+03, 9.2560e+03, 2.3312e+04, 7.7160e+03, 4.3072e+04, 1.7808e+04,\n",
      "        6.3800e+03, 2.3392e+04, 1.7920e+03, 5.3000e+03, 1.1440e+04, 2.7088e+04,\n",
      "        6.0256e+04, 3.2480e+03, 3.2140e+03, 1.9810e+03, 1.8112e+04, 1.5376e+04,\n",
      "        7.4240e+03, 3.4688e+04, 1.4168e+04, 2.5200e+04, 1.6368e+04, 1.1488e+04,\n",
      "        5.3320e+03, 8.2640e+03, 5.8112e+04,        inf, 5.7040e+03, 3.8368e+04,\n",
      "        2.5968e+04, 2.5104e+04,        inf, 9.4720e+03, 2.2840e+03, 7.1520e+03,\n",
      "               inf, 1.8160e+04, 1.6656e+04, 2.0848e+04, 6.4440e+03, 4.5152e+04,\n",
      "        5.2672e+04, 1.1736e+04, 6.3584e+04,        inf, 3.2272e+04,        inf,\n",
      "        5.9808e+04, 7.4800e+03, 2.0608e+04, 6.4928e+04,        inf, 1.6256e+04,\n",
      "               inf, 6.8640e+03, 1.3760e+04, 2.0032e+04, 1.9680e+04, 1.3304e+04,\n",
      "        1.3224e+04, 5.0144e+04, 6.8280e+03, 4.2080e+03, 4.7552e+04, 1.5000e+04,\n",
      "               inf, 3.4720e+04, 4.7040e+04, 5.6760e+03, 1.5550e+03, 2.1968e+04,\n",
      "               inf, 1.0296e+04,        inf, 3.0464e+04, 5.6000e+04, 6.1952e+04,\n",
      "        5.3344e+04, 1.0304e+04, 2.9664e+04, 1.3832e+04, 7.0840e+03, 4.3616e+04,\n",
      "               inf, 5.3280e+03, 2.5184e+04, 3.2704e+04, 3.2608e+04, 1.6360e+04,\n",
      "        1.3912e+04, 2.0032e+04, 4.6048e+04, 3.8976e+04, 2.2192e+04,        inf,\n",
      "        3.2896e+04,        inf, 4.3584e+04, 2.5120e+04,        inf, 2.4576e+04,\n",
      "        2.8864e+04, 6.1184e+04, 1.0032e+04, 3.3024e+04, 5.3680e+03, 1.5368e+04,\n",
      "               inf, 2.1056e+04, 1.5760e+04, 2.3632e+04, 3.5008e+04, 2.1264e+04,\n",
      "        6.0400e+03, 8.5280e+03, 5.5560e+03, 2.4960e+04, 2.1664e+04, 3.1740e+03,\n",
      "               inf, 2.1120e+04, 2.4208e+04, 1.3912e+04, 5.5920e+03, 5.2280e+03,\n",
      "        6.1216e+04, 1.0888e+04, 9.7280e+03, 3.0624e+04, 5.3056e+04, 2.9184e+04,\n",
      "        9.0080e+03,        inf, 1.5472e+04, 2.1728e+04, 2.2480e+04,        inf,\n",
      "        1.4584e+04, 4.9248e+04, 4.1088e+04, 3.6640e+04, 1.5176e+04, 3.6608e+04,\n",
      "        1.7440e+04, 5.5080e+03, 1.7210e+03,        inf, 4.0448e+04, 3.1280e+04,\n",
      "        1.3552e+04,        inf, 4.0192e+04, 6.2496e+04, 3.0800e+03, 5.9168e+04,\n",
      "        2.4384e+04, 1.2600e+04, 2.0272e+04, 2.9296e+04, 1.9296e+04, 3.4208e+04,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([2.5264e+04, 7.3400e+03, 4.5408e+04, 5.3408e+04, 6.0832e+04, 4.6976e+04,\n",
      "        2.9024e+04, 4.1696e+04, 6.3040e+04, 9.6560e+03, 2.5648e+04,        inf,\n",
      "        6.3840e+04,        inf, 5.1200e+04, 2.8848e+04, 4.3840e+03, 9.7600e+03,\n",
      "               inf, 4.3808e+04, 2.4460e+03,        inf,        inf, 5.3760e+04,\n",
      "        3.9136e+04, 3.0032e+04, 1.9856e+04, 4.3040e+03, 2.7184e+04, 3.6736e+04,\n",
      "        2.8080e+04, 3.3312e+04, 2.5632e+04, 1.1488e+04, 2.7488e+04, 6.1632e+04,\n",
      "        4.7040e+04, 4.4760e+03, 9.2750e+02, 5.1520e+04,        inf, 1.6784e+04,\n",
      "               inf, 1.2494e+02, 9.2880e+03, 4.5856e+04,        inf,        inf,\n",
      "        4.1056e+04, 7.6520e+03,        inf,        inf, 4.0660e+03, 2.2256e+04,\n",
      "        6.3424e+04,        inf, 2.8544e+04,        inf, 2.3875e+01, 5.3984e+04,\n",
      "               inf, 3.8656e+04, 3.4325e+02, 2.1392e+04, 2.3900e+03,        inf,\n",
      "               inf, 3.0304e+04, 1.1690e+03,        inf, 5.7376e+04,        inf,\n",
      "        7.4960e+03, 6.0480e+03, 2.4416e+04, 1.4420e+03,        inf,        inf,\n",
      "        1.9180e+03, 4.7776e+04,        inf,        inf, 2.8768e+04, 1.2952e+04,\n",
      "        4.6528e+04,        inf,        inf, 3.8840e+03, 1.1200e+04, 2.8256e+04,\n",
      "        5.9104e+04, 6.4064e+04,        inf, 3.5168e+04, 2.5872e+04,        inf,\n",
      "        8.4240e+03, 4.2528e+04, 1.8496e+04, 1.3752e+04, 2.8016e+04,        inf,\n",
      "        3.0832e+04, 2.5360e+04,        inf, 5.6160e+04,        inf,        inf,\n",
      "        5.5424e+04, 3.3664e+04, 3.6096e+04,        inf, 3.8048e+04, 6.4384e+04,\n",
      "        1.4416e+04, 4.5120e+04, 1.1040e+04, 4.5472e+04, 4.3680e+04, 3.6000e+04,\n",
      "               inf, 2.8000e+04, 4.4576e+04,        inf, 6.3160e+03, 2.8224e+04,\n",
      "        5.3344e+04, 1.4630e+03,        inf, 4.6016e+04, 2.5712e+04,        inf,\n",
      "        4.8400e+03,        inf, 3.4528e+04, 7.5960e+03, 4.5504e+04, 2.1820e+03,\n",
      "        1.7184e+04, 2.0720e+04, 3.5296e+04,        inf, 1.0130e+03, 6.8600e+03,\n",
      "               inf,        inf, 4.4672e+04, 1.1616e+04,        inf, 2.2880e+04,\n",
      "               inf, 4.4575e+02, 6.4768e+04, 6.1952e+04, 1.8832e+04,        inf,\n",
      "        1.7392e+04,        inf, 5.1712e+04, 1.7530e+03, 3.7792e+04, 1.7504e+04,\n",
      "        3.2224e+04,        inf, 1.6024e+04, 6.2624e+04,        inf, 3.6288e+04,\n",
      "        2.5248e+04, 6.0448e+04, 4.8608e+04,        inf,        inf,        inf,\n",
      "        6.4080e+03, 1.8448e+04, 2.8464e+04,        inf, 3.5552e+04,        inf,\n",
      "               inf, 2.9872e+04,        inf, 2.8752e+04,        inf,        inf,\n",
      "        5.6128e+04, 1.8128e+04,        inf, 1.1500e+03, 1.7168e+04,        inf,\n",
      "        4.7808e+04, 5.4360e+03, 3.2704e+04, 6.3008e+04, 6.5152e+04, 3.4144e+04,\n",
      "        8.7280e+03, 4.0920e+03, 2.5008e+04, 5.5456e+04,        inf, 5.3824e+04,\n",
      "        5.5040e+04, 5.4560e+04, 5.4464e+04, 2.5800e+03, 1.9040e+04, 4.5216e+04,\n",
      "               inf, 4.8256e+04, 2.4448e+04,        inf,        inf,        inf,\n",
      "               inf, 6.2816e+04,        inf, 3.9328e+04, 5.4240e+04,        inf,\n",
      "        4.0256e+04, 1.8496e+04, 1.5624e+04, 2.5440e+04,        inf,        inf,\n",
      "        4.2752e+04, 4.1280e+04, 5.2352e+04, 8.6960e+03, 2.8928e+04,        inf,\n",
      "        2.1740e+03,        inf, 5.6240e+03, 2.4800e+04,        inf, 3.7920e+04,\n",
      "        2.9232e+04, 3.0032e+04, 9.6080e+03, 4.2080e+04,        inf, 5.2032e+04,\n",
      "        1.1912e+04,        inf, 5.2352e+04, 3.7952e+04,        inf, 1.7152e+04,\n",
      "        5.8480e+03, 8.7760e+03, 1.8080e+03, 2.9184e+04,        inf, 3.5136e+04,\n",
      "        5.0656e+04,        inf, 7.6440e+03, 3.1728e+04, 6.4416e+04,        inf,\n",
      "        7.3600e+03, 5.7800e+02, 2.4992e+04, 1.7312e+04,        inf, 1.3888e+04,\n",
      "        4.0672e+04,        inf,        inf, 3.3664e+04, 2.2240e+04, 3.0144e+04,\n",
      "        6.1040e+03, 9.2560e+03, 6.0128e+04, 2.3952e+04,        inf, 4.5632e+04,\n",
      "        1.1120e+03, 2.4320e+04, 1.4896e+04, 1.9552e+04,        inf, 5.1360e+03,\n",
      "        1.4344e+04, 6.7840e+03, 1.6464e+04, 1.3832e+04, 4.5952e+04,        inf,\n",
      "        5.4176e+04, 1.4120e+04, 5.3800e+03,        inf,        inf, 8.1000e+03,\n",
      "               inf, 5.3952e+04,        inf, 1.6672e+04,        inf, 2.0512e+04,\n",
      "        4.8680e+03, 1.0344e+04, 5.4080e+04, 6.1024e+04, 3.2112e+04, 1.1536e+04,\n",
      "        6.3424e+04,        inf, 6.0800e+04, 5.6960e+03,        inf, 7.0300e+02,\n",
      "               inf, 1.0288e+04,        inf, 4.4896e+04,        inf,        inf,\n",
      "        4.5344e+04, 6.9920e+03, 4.5984e+04, 3.3984e+04, 4.5248e+04, 3.2352e+04,\n",
      "        7.2680e+03, 4.4160e+04, 2.9312e+04, 5.4848e+04, 3.4880e+04,        inf,\n",
      "        3.0720e+04, 5.4752e+04, 2.4848e+04,        inf, 6.1152e+04,        inf,\n",
      "               inf, 3.5392e+04, 1.7984e+04, 5.8144e+04, 6.0320e+03, 3.9980e+03,\n",
      "        4.9760e+04, 6.0672e+04,        inf, 6.7080e+03,        inf, 3.5424e+04,\n",
      "        2.5392e+04, 4.7008e+04, 6.0992e+04, 2.9500e+03, 2.8480e+03,        inf,\n",
      "               inf, 1.4448e+04, 3.4528e+04, 1.6432e+04,        inf, 2.3824e+04,\n",
      "        3.5680e+04,        inf,        inf, 4.6944e+04, 3.0128e+04, 2.3808e+04,\n",
      "        5.3100e+02, 2.9100e+03,        inf,        inf, 3.2928e+04,        inf,\n",
      "        6.4672e+04, 5.9008e+04,        inf, 6.2880e+03, 5.5600e+03, 5.3800e+03,\n",
      "               inf, 2.4096e+04, 4.3296e+04,        inf, 1.8896e+04,        inf,\n",
      "        5.5424e+04, 2.7488e+04,        inf,        inf, 6.1248e+04,        inf,\n",
      "        6.1984e+04, 2.3376e+04, 2.2832e+04,        inf,        inf,        inf,\n",
      "               inf, 5.0624e+04, 2.8064e+04, 1.4950e+03, 3.2528e+04, 1.5392e+04,\n",
      "        2.9008e+04,        inf, 1.3040e+04, 1.1984e+04,        inf, 3.4560e+04,\n",
      "               inf, 5.7088e+04, 5.2096e+04, 7.3720e+03, 2.8540e+03,        inf,\n",
      "        5.7632e+04, 2.1840e+04,        inf, 3.7184e+04, 5.5040e+04, 6.2144e+04,\n",
      "               inf, 1.4296e+04,        inf, 2.8448e+04, 1.6304e+04, 3.4624e+04,\n",
      "               inf, 9.6640e+03, 2.5184e+04, 4.5504e+04, 3.1792e+04, 1.0816e+04,\n",
      "        5.4944e+04, 1.3440e+04,        inf, 4.8096e+04, 3.2896e+04,        inf,\n",
      "        3.8528e+04,        inf, 5.3824e+04, 2.5728e+04,        inf, 5.3056e+04,\n",
      "        1.8640e+04,        inf, 7.4920e+03, 2.7808e+04, 7.0480e+03, 2.2400e+04,\n",
      "               inf, 1.9632e+04, 4.1696e+04, 2.6624e+04, 3.6672e+04,        inf,\n",
      "        3.6960e+04, 1.0280e+04, 1.0344e+04,        inf, 2.5264e+04, 1.0490e+03,\n",
      "               inf, 2.7200e+04, 5.1264e+04, 9.4720e+03, 1.3152e+04, 5.9960e+03,\n",
      "        4.9568e+04, 4.8544e+04, 2.1632e+04, 5.6576e+04,        inf, 3.7600e+04,\n",
      "        1.6656e+04,        inf, 2.7504e+04, 4.9472e+04, 5.4480e+03,        inf,\n",
      "        2.1200e+04,        inf, 5.4176e+04,        inf, 8.5280e+03, 3.3504e+04,\n",
      "        2.3008e+04, 9.0640e+03, 6.1680e+03,        inf,        inf, 4.3072e+04,\n",
      "        1.8384e+04,        inf,        inf, 5.4752e+04, 6.0480e+03, 6.5120e+04,\n",
      "        5.1040e+04, 7.0560e+03, 2.5360e+04,        inf, 2.8640e+04, 5.2288e+04,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([ 4252.0000, 12672.0000,  8272.0000, 12256.0000,  7652.0000,  1928.0000,\n",
      "        10928.0000, 32400.0000, 30832.0000,  2896.0000,   546.0000, 18560.0000,\n",
      "        41792.0000, 29280.0000, 29008.0000,  9456.0000,  4320.0000, 42784.0000,\n",
      "        59744.0000, 36512.0000,  4336.0000, 64128.0000, 29136.0000, 24608.0000,\n",
      "        24288.0000, 11056.0000,  6464.0000,  5084.0000,  8728.0000,        inf,\n",
      "        18640.0000, 32144.0000,        inf, 39264.0000, 23616.0000,  2176.0000,\n",
      "         3768.0000, 24208.0000,  6340.0000, 40256.0000,   229.1250, 13160.0000,\n",
      "        25024.0000,  5588.0000,  5324.0000, 25888.0000, 53152.0000, 45088.0000,\n",
      "        12944.0000,  1705.0000, 61728.0000, 30224.0000,  8344.0000,  9080.0000,\n",
      "        12536.0000, 34240.0000,   151.6250,  2444.0000,  3074.0000, 46784.0000,\n",
      "        13472.0000, 22448.0000,  3294.0000, 16832.0000,  2176.0000,  1689.0000,\n",
      "               inf, 12480.0000, 12736.0000, 32320.0000,  7560.0000, 19968.0000,\n",
      "         5440.0000, 13552.0000, 33632.0000, 11792.0000, 25424.0000,        inf,\n",
      "         2736.0000,  3420.0000, 13408.0000, 16416.0000, 34016.0000, 10400.0000,\n",
      "         2786.0000, 35872.0000,        inf, 17344.0000, 14048.0000,  1826.0000,\n",
      "         1450.0000, 51328.0000,  4632.0000, 10064.0000, 10400.0000,        inf,\n",
      "         6140.0000,  3912.0000,  6048.0000, 20304.0000,  6048.0000, 14336.0000,\n",
      "         8496.0000,  7744.0000,  5772.0000,  4488.0000, 25872.0000,  3326.0000,\n",
      "        45632.0000,  1934.0000, 23328.0000, 35072.0000,  5216.0000,  5568.0000,\n",
      "         1442.0000,  1666.0000,  1188.0000,   294.0000, 11264.0000,  6976.0000,\n",
      "        41312.0000, 16136.0000,  8736.0000,        inf,  8976.0000, 11992.0000,\n",
      "        21936.0000,  6772.0000,  2618.0000, 20016.0000, 20064.0000, 50880.0000,\n",
      "        12312.0000, 25616.0000,  4476.0000,   620.0000,  8672.0000, 16736.0000,\n",
      "         2856.0000, 14600.0000,  2128.0000, 14360.0000, 19504.0000,  1258.0000,\n",
      "        10856.0000, 38496.0000, 13664.0000,   665.5000,  2436.0000, 32224.0000,\n",
      "        19216.0000, 10576.0000, 34336.0000, 34784.0000, 10688.0000, 18464.0000,\n",
      "        19808.0000,        inf,   181.2500,  3820.0000, 33024.0000,  1992.0000,\n",
      "        11136.0000,        inf,  1899.0000, 13616.0000,        inf,  1517.0000,\n",
      "          699.0000,  5028.0000,  2330.0000,        inf,  6816.0000,   353.5000,\n",
      "        16640.0000, 52256.0000,   387.7500, 39680.0000,  9640.0000,  8992.0000,\n",
      "               inf, 10040.0000, 32928.0000,  4180.0000, 18736.0000, 49312.0000,\n",
      "        36128.0000, 18080.0000, 50272.0000,  3598.0000,  3592.0000,  2240.0000,\n",
      "        18752.0000,  1969.0000, 19232.0000, 20528.0000, 24896.0000,  5976.0000,\n",
      "        10192.0000,  2068.0000, 14920.0000,  4700.0000,  7872.0000, 22256.0000,\n",
      "         1093.0000,  5432.0000, 43488.0000, 10144.0000, 13104.0000, 17984.0000,\n",
      "        38752.0000, 57696.0000,  9272.0000, 39776.0000,  4720.0000,        inf,\n",
      "               inf,  8744.0000, 39200.0000,  1399.0000, 11160.0000, 41632.0000,\n",
      "        12296.0000, 22672.0000, 18624.0000, 11000.0000, 35008.0000, 39424.0000,\n",
      "        33344.0000, 23024.0000, 35776.0000, 13552.0000, 11904.0000, 18960.0000,\n",
      "         6400.0000, 59232.0000,  1386.0000,  9408.0000, 11880.0000,  6424.0000,\n",
      "         7200.0000, 10848.0000, 18160.0000, 23184.0000, 34816.0000, 25408.0000,\n",
      "         1111.0000,        inf,  4932.0000,        inf, 35616.0000,  4760.0000,\n",
      "         1354.0000,  2982.0000,  2218.0000,  9160.0000, 11288.0000,  2892.0000,\n",
      "        19568.0000, 25040.0000,  8576.0000, 23408.0000, 11288.0000, 19072.0000,\n",
      "         2670.0000,  3464.0000, 16864.0000,  3890.0000, 38176.0000,  4636.0000,\n",
      "        37632.0000, 42144.0000,        inf,  2430.0000, 16832.0000, 49216.0000,\n",
      "        19632.0000,  4016.0000, 37696.0000, 10792.0000,        inf, 36640.0000,\n",
      "         2840.0000,  9224.0000,   298.5000, 21760.0000, 45376.0000,  3354.0000,\n",
      "        10392.0000,  6360.0000, 14296.0000, 33024.0000, 25552.0000, 20176.0000,\n",
      "        50784.0000,  5704.0000,  7036.0000, 62080.0000, 60608.0000,  4944.0000,\n",
      "        56096.0000,  2546.0000, 42944.0000, 10048.0000, 47360.0000,  8608.0000,\n",
      "         9336.0000,  4576.0000, 13248.0000, 61504.0000, 10248.0000,  8028.0000,\n",
      "        30928.0000, 54816.0000,  2814.0000,  3782.0000, 40096.0000, 32016.0000,\n",
      "          761.0000,  7312.0000, 45952.0000, 16240.0000, 11960.0000,  3132.0000,\n",
      "         1232.0000, 42016.0000,        inf, 36864.0000, 31776.0000, 13616.0000,\n",
      "        16496.0000,  8432.0000,  1312.0000, 34848.0000, 49728.0000,        inf,\n",
      "        18784.0000, 40384.0000, 25120.0000, 28016.0000, 32480.0000,        inf,\n",
      "               inf, 12848.0000,  4124.0000, 18096.0000,  9872.0000,  4296.0000,\n",
      "         4916.0000,  4732.0000, 32608.0000,  5752.0000, 31696.0000, 20128.0000,\n",
      "        12688.0000, 22656.0000,  1525.0000, 16736.0000, 11584.0000, 29376.0000,\n",
      "        64704.0000, 10656.0000, 14272.0000,  2862.0000, 18736.0000, 10240.0000,\n",
      "        14872.0000, 18848.0000, 18448.0000, 23280.0000,  9776.0000,  4852.0000,\n",
      "         9712.0000, 34592.0000, 54144.0000,        inf, 16144.0000, 44640.0000,\n",
      "        56768.0000, 32384.0000,        inf,  1981.0000,  6096.0000,   976.0000,\n",
      "               inf, 18656.0000, 23024.0000,   867.0000,  6704.0000, 50880.0000,\n",
      "               inf, 10744.0000,        inf,        inf, 34528.0000,        inf,\n",
      "        35872.0000,  4436.0000, 17472.0000,        inf, 37984.0000,  6700.0000,\n",
      "               inf,   543.0000,  9864.0000,  6404.0000, 15976.0000,  6880.0000,\n",
      "        39008.0000,        inf, 27600.0000, 11904.0000, 41024.0000, 33632.0000,\n",
      "        56576.0000, 48032.0000, 27280.0000, 29264.0000, 18976.0000, 46688.0000,\n",
      "        20240.0000,  4448.0000,        inf, 18240.0000, 39264.0000, 41280.0000,\n",
      "        64224.0000, 15696.0000, 46304.0000, 57344.0000, 10120.0000, 36320.0000,\n",
      "               inf,  5200.0000,  7312.0000, 21152.0000, 62336.0000,  3268.0000,\n",
      "        13952.0000,  6860.0000,        inf, 27968.0000, 24352.0000,        inf,\n",
      "        23264.0000,        inf, 33504.0000, 11272.0000,        inf, 12464.0000,\n",
      "        17616.0000, 63904.0000, 11872.0000, 10248.0000,  8552.0000, 12352.0000,\n",
      "               inf, 31456.0000, 31552.0000, 46432.0000,  9368.0000, 17584.0000,\n",
      "         2052.0000, 22384.0000,  3714.0000, 54240.0000, 14992.0000, 12056.0000,\n",
      "               inf, 20016.0000, 39424.0000, 14880.0000,  9680.0000, 24608.0000,\n",
      "        26560.0000, 18400.0000, 10504.0000, 30736.0000, 32048.0000, 43168.0000,\n",
      "         4584.0000, 49600.0000, 20576.0000, 17136.0000, 12896.0000,        inf,\n",
      "        14792.0000, 33280.0000, 33280.0000, 39168.0000, 12672.0000, 29888.0000,\n",
      "        33824.0000,  9664.0000, 11544.0000,        inf, 39552.0000, 18576.0000,\n",
      "         3408.0000,        inf,        inf,        inf,  5180.0000, 62848.0000,\n",
      "         7300.0000,  3600.0000, 18064.0000, 13592.0000, 33248.0000, 24896.0000,\n",
      "               inf, 60736.0000], device='cuda:0', dtype=torch.float16), tensor([2.0272e+04, 2.4848e+04, 5.9776e+04, 6.1824e+04,        inf, 3.0128e+04,\n",
      "        3.5744e+04, 3.6928e+04, 6.0416e+04, 2.8688e+04, 6.0440e+03,        inf,\n",
      "        3.9232e+04,        inf,        inf, 1.5808e+04, 6.2440e+03, 4.2592e+04,\n",
      "               inf, 4.4672e+04, 1.6656e+04,        inf,        inf,        inf,\n",
      "        3.7216e+04, 1.5936e+04, 2.3840e+03, 4.8520e+03, 3.2400e+04, 6.1280e+04,\n",
      "        1.1824e+04, 4.0800e+03,        inf, 3.1760e+04, 3.4464e+04, 5.8336e+04,\n",
      "        3.0000e+04, 1.5496e+04, 1.9456e+04,        inf,        inf, 2.1312e+04,\n",
      "               inf, 6.2200e+03, 1.5408e+04, 4.3904e+04,        inf,        inf,\n",
      "        2.8528e+04, 1.3370e+03,        inf,        inf, 1.1410e+03, 2.1696e+04,\n",
      "        5.4560e+04,        inf, 3.5616e+04, 5.2288e+04, 9.4160e+03, 5.1328e+04,\n",
      "               inf, 4.5664e+04, 4.2200e+03, 3.2832e+04, 2.5616e+04, 5.9296e+04,\n",
      "               inf, 4.5184e+04, 2.8000e+04,        inf, 3.6480e+04, 6.0960e+04,\n",
      "        2.4544e+04, 9.9600e+03, 6.0032e+04, 3.3620e+03,        inf,        inf,\n",
      "        3.5456e+04, 2.8384e+04,        inf,        inf, 3.3824e+04, 4.5056e+04,\n",
      "               inf,        inf,        inf, 2.5104e+04, 3.9220e+03, 3.2576e+04,\n",
      "        3.8432e+04,        inf,        inf, 1.1256e+04, 5.5392e+04,        inf,\n",
      "        1.2032e+04, 2.8032e+04, 2.2336e+04, 4.4880e+03, 1.1920e+04, 5.4304e+04,\n",
      "        2.2160e+04, 3.2928e+04,        inf, 2.1664e+04, 6.1792e+04, 5.4784e+04,\n",
      "        4.4320e+04, 6.7760e+03, 2.4848e+04,        inf, 2.7792e+04,        inf,\n",
      "        8.4320e+03, 1.0976e+04, 9.7600e+03, 5.8208e+04, 4.0800e+04, 4.1920e+04,\n",
      "               inf, 3.1392e+04, 6.3424e+04,        inf, 1.5096e+04,        inf,\n",
      "        5.7728e+04, 7.9880e+03, 5.5456e+04, 5.5072e+04, 4.8000e+04,        inf,\n",
      "        1.5240e+04,        inf, 5.7728e+04, 2.4660e+03, 4.0960e+04, 2.7120e+04,\n",
      "        3.1580e+03, 1.6368e+04, 3.1360e+04, 5.5904e+04, 8.0720e+03, 2.3340e+03,\n",
      "               inf,        inf, 4.3104e+04, 3.7900e+03,        inf,        inf,\n",
      "               inf, 2.6848e+04, 5.5968e+04,        inf, 1.2144e+04,        inf,\n",
      "        2.2800e+04,        inf,        inf, 9.7920e+03, 4.1312e+04, 1.0720e+04,\n",
      "        8.3840e+03,        inf, 2.1536e+04, 3.8048e+04,        inf, 8.1640e+03,\n",
      "        2.5472e+04, 3.8240e+04, 3.9040e+04,        inf, 5.3120e+04, 6.1536e+04,\n",
      "        4.3968e+04, 4.6944e+04, 1.9550e+02,        inf, 3.2256e+04,        inf,\n",
      "               inf, 3.3088e+04,        inf, 3.4240e+04,        inf,        inf,\n",
      "        5.1200e+04, 3.9008e+04,        inf, 1.5152e+04, 1.8912e+04,        inf,\n",
      "        2.9920e+04, 7.5664e+00, 3.4080e+04, 5.9616e+04,        inf, 1.0650e+03,\n",
      "        1.5120e+04, 2.1024e+04, 6.7120e+03, 5.9520e+04,        inf,        inf,\n",
      "        4.8608e+04, 5.1616e+04,        inf, 1.0616e+04, 1.5872e+04, 5.2160e+04,\n",
      "               inf,        inf, 2.9952e+04,        inf,        inf,        inf,\n",
      "               inf, 4.8256e+04, 6.5504e+04, 3.8528e+04, 5.1776e+04,        inf,\n",
      "        4.6976e+04, 2.0192e+04, 2.2384e+04, 3.6960e+04,        inf,        inf,\n",
      "        4.6720e+04, 3.7408e+04, 4.4224e+04, 1.4288e+04, 4.2080e+03,        inf,\n",
      "        9.5760e+03,        inf, 1.2528e+04, 3.0384e+04,        inf, 1.8144e+04,\n",
      "        1.6432e+04, 3.2960e+04, 3.0912e+04,        inf, 6.3968e+04, 4.1280e+04,\n",
      "        4.4080e+03,        inf, 6.3296e+04,        inf,        inf, 4.8352e+04,\n",
      "        1.8784e+04, 8.5440e+03, 3.0000e+03, 1.9968e+04, 5.3824e+04, 4.5440e+04,\n",
      "        6.4736e+04, 6.2592e+04, 2.4512e+04, 2.4752e+04,        inf, 3.7248e+04,\n",
      "        2.1360e+03, 9.3760e+03, 2.2560e+04, 4.8000e+03,        inf, 1.5456e+04,\n",
      "               inf,        inf,        inf, 3.1925e+02, 1.4128e+04, 2.3984e+04,\n",
      "        2.8016e+04, 1.3120e+04, 6.2496e+04, 1.9632e+04,        inf,        inf,\n",
      "        1.2968e+04, 1.8896e+04, 3.4336e+04, 3.1056e+04,        inf, 3.5700e+03,\n",
      "        1.7040e+04, 7.2560e+03, 8.8720e+03, 3.3024e+04, 3.5072e+04, 5.5968e+04,\n",
      "               inf, 2.5696e+04, 1.9136e+04, 6.2816e+04,        inf, 1.3376e+04,\n",
      "        5.9904e+04, 4.4256e+04,        inf, 2.1072e+04,        inf, 1.9984e+04,\n",
      "        1.6336e+04, 1.0304e+04, 5.4144e+04,        inf, 2.6672e+04, 5.6224e+04,\n",
      "        6.2944e+04,        inf, 6.0192e+04, 7.5280e+03,        inf, 5.2040e+03,\n",
      "               inf, 3.4780e+03,        inf, 3.4560e+04,        inf, 5.7760e+04,\n",
      "        4.2912e+04, 4.6400e+03, 4.8928e+04, 1.3260e+03, 5.3120e+04, 3.9040e+04,\n",
      "        9.7920e+03, 5.8040e+03, 1.5072e+04, 4.3456e+04,        inf,        inf,\n",
      "        2.4280e+03, 4.3968e+04, 1.4664e+04,        inf, 3.4976e+04,        inf,\n",
      "               inf, 1.0920e+04, 3.0000e+04,        inf, 2.1940e+03, 1.1032e+04,\n",
      "        3.2048e+04,        inf,        inf, 8.3760e+03, 5.5008e+04, 1.0824e+04,\n",
      "        3.5264e+04, 3.0832e+04, 3.3664e+04, 1.7776e+04, 1.1984e+04,        inf,\n",
      "               inf, 2.1616e+04, 3.5264e+04, 8.8160e+03,        inf, 3.4656e+04,\n",
      "        3.8592e+04,        inf,        inf, 3.4336e+04, 2.5248e+04, 1.5112e+04,\n",
      "        5.8920e+03, 2.3760e+04, 6.2880e+04,        inf, 3.7376e+04,        inf,\n",
      "               inf, 4.8000e+04,        inf, 1.5900e+03, 1.1952e+04, 1.2832e+04,\n",
      "               inf, 1.9536e+04, 5.2800e+04,        inf, 2.1136e+04,        inf,\n",
      "               inf, 4.7328e+04,        inf,        inf,        inf,        inf,\n",
      "        4.9504e+04, 9.3200e+03, 2.0560e+04,        inf,        inf, 2.2912e+04,\n",
      "        6.0288e+04, 5.5328e+04, 1.4496e+04, 1.6544e+04, 2.6832e+04, 9.6560e+03,\n",
      "        5.2608e+04,        inf, 1.7056e+04, 1.6928e+04,        inf, 6.1408e+04,\n",
      "               inf, 5.5776e+04, 3.2048e+04, 2.2912e+04, 2.4048e+04,        inf,\n",
      "        2.2912e+04, 1.6350e+03,        inf, 8.8720e+03, 4.5856e+04, 5.0208e+04,\n",
      "               inf, 2.2592e+04,        inf, 5.6288e+04, 2.0272e+04, 3.0560e+04,\n",
      "               inf, 1.8400e+04, 6.2440e+03, 2.1136e+04, 5.6288e+04, 1.0472e+04,\n",
      "        5.7952e+04, 2.0304e+04,        inf, 3.6224e+04, 3.7152e+04,        inf,\n",
      "        3.3568e+04,        inf, 3.1680e+04, 2.5632e+04,        inf, 4.2592e+04,\n",
      "        1.5112e+04,        inf, 2.0704e+04, 5.8560e+03, 6.5480e+03, 1.5928e+04,\n",
      "               inf, 3.0336e+04,        inf, 4.3264e+04, 1.7408e+04,        inf,\n",
      "        3.1520e+04, 2.6704e+04, 1.0984e+04,        inf, 9.6640e+03, 1.7984e+04,\n",
      "               inf, 1.9856e+04, 3.8848e+04, 1.4512e+04, 6.5160e+03, 3.0960e+04,\n",
      "        1.9392e+04, 5.7120e+04, 1.2128e+04, 5.7856e+04, 5.9328e+04, 5.1456e+04,\n",
      "        1.1680e+04, 5.9040e+04, 3.2096e+04,        inf, 5.5360e+03,        inf,\n",
      "        2.1232e+04, 3.0112e+04, 5.5072e+04,        inf, 5.7760e+03, 2.7104e+04,\n",
      "        3.4144e+04, 1.6624e+04, 7.4280e+03,        inf,        inf, 3.6640e+04,\n",
      "        3.6500e+02, 5.8528e+04,        inf,        inf, 1.3264e+04,        inf,\n",
      "        1.7600e+04, 3.3960e+03, 1.5776e+04,        inf, 3.9872e+04, 5.7440e+04,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([[1.0085e+03, 1.8120e+03, 1.6320e+03,  ..., 1.0900e+03, 1.4400e+03,\n",
      "         7.8750e+00],\n",
      "        [3.6175e+02, 8.3100e+02, 2.9025e+02,  ..., 3.9925e+02, 8.8000e+02,\n",
      "         1.8500e+02],\n",
      "        [9.5900e+02, 6.2100e+02, 4.6600e+02,  ..., 8.4200e+02, 1.8460e+03,\n",
      "         1.6600e+03],\n",
      "        ...,\n",
      "        [6.6920e+03, 2.3504e+04, 1.3584e+04,  ..., 1.1312e+04, 3.0688e+04,\n",
      "         1.2544e+04],\n",
      "        [5.6640e+03, 4.9200e+03, 1.0400e+03,  ..., 4.9680e+03, 8.8880e+03,\n",
      "         5.5680e+03],\n",
      "        [1.1392e+04, 1.8704e+04, 1.2568e+04,  ..., 3.8760e+03, 7.2000e+02,\n",
      "         2.8520e+03]], device='cuda:0', dtype=torch.float16), tensor([ 1643.,   823.,  2084.,  ..., 28416.,  9904.,  4172.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[ 9976.0000, 11176.0000, 23952.0000,  ..., 16608.0000, 26272.0000,\n",
      "         21040.0000],\n",
      "        [ 1168.0000,  4680.0000,  7496.0000,  ...,   652.5000,  3878.0000,\n",
      "           363.0000],\n",
      "        [ 9440.0000, 25008.0000, 13016.0000,  ...,  2248.0000,  2912.0000,\n",
      "         15640.0000],\n",
      "        ...,\n",
      "        [10152.0000, 32592.0000,  7208.0000,  ...,  6772.0000, 10704.0000,\n",
      "         16704.0000],\n",
      "        [ 8088.0000, 33056.0000,  5552.0000,  ...,  5344.0000, 30624.0000,\n",
      "          5260.0000],\n",
      "        [14128.0000, 52480.0000,  6376.0000,  ...,  8728.0000, 25664.0000,\n",
      "         20192.0000]], device='cuda:0', dtype=torch.float16), tensor([1.3800e+04, 3.0020e+03, 4.1856e+04, 5.4208e+04,        inf, 2.1200e+04,\n",
      "        3.7664e+04, 5.3216e+04,        inf, 3.5552e+04, 1.3936e+04,        inf,\n",
      "        2.4848e+04, 4.1856e+04,        inf, 3.1168e+04, 1.2200e+04, 4.5024e+04,\n",
      "               inf, 4.9664e+04, 3.8960e+03,        inf,        inf, 1.7968e+04,\n",
      "        2.6320e+04, 2.2224e+04, 1.1688e+04, 1.4970e+03, 4.0384e+04, 4.9920e+04,\n",
      "        6.2800e+03, 3.5360e+04, 5.6704e+04, 1.8688e+04, 4.8128e+04, 3.1120e+04,\n",
      "        4.4832e+04, 1.5624e+04, 2.2088e+02, 5.7024e+04,        inf, 2.6940e+03,\n",
      "               inf, 3.5904e+04, 5.8176e+04, 5.8496e+04, 4.8608e+04,        inf,\n",
      "        2.3232e+04, 1.8720e+04,        inf, 5.3472e+04, 2.4400e+04, 2.3040e+04,\n",
      "        2.6432e+04, 3.2560e+04, 6.5200e+03, 3.9200e+04, 9.5920e+03,        inf,\n",
      "        5.3056e+04, 2.3296e+04, 3.0528e+04,        inf, 3.6980e+03,        inf,\n",
      "               inf, 5.8720e+04, 4.0032e+04, 5.6960e+04, 3.1344e+04,        inf,\n",
      "        2.4944e+04, 2.6608e+04,        inf, 2.1744e+04,        inf,        inf,\n",
      "        2.7648e+04, 8.1640e+03, 2.1360e+04, 6.4096e+04, 5.8944e+04, 2.7808e+04,\n",
      "               inf,        inf,        inf, 1.2368e+04, 2.9000e+03, 1.5360e+04,\n",
      "        1.1304e+04,        inf,        inf, 2.0000e+04, 2.4976e+04,        inf,\n",
      "        6.9080e+03, 7.5150e+02, 5.8304e+04, 2.6336e+04, 3.8592e+04, 2.8160e+04,\n",
      "        2.4560e+04, 4.7776e+04,        inf, 2.8560e+04, 4.6080e+04, 4.8672e+04,\n",
      "        6.4032e+04, 3.3220e+03, 1.2232e+04,        inf, 3.7920e+04, 3.4496e+04,\n",
      "        2.2816e+04, 2.8740e+03, 4.9480e+03, 5.7664e+04, 2.6080e+04, 3.4560e+04,\n",
      "               inf, 4.6720e+03, 5.0432e+04,        inf, 2.3160e+03,        inf,\n",
      "               inf, 9.3120e+03, 2.7232e+04, 5.7952e+04, 3.4944e+04,        inf,\n",
      "        1.3768e+04, 5.8272e+04, 6.4800e+04, 1.0496e+04, 6.1728e+04, 2.5264e+04,\n",
      "        2.2640e+03, 4.4128e+04, 3.2272e+04, 5.0208e+04, 1.3720e+04, 2.3520e+04,\n",
      "        5.4784e+04,        inf,        inf, 4.7000e+03,        inf, 5.7760e+04,\n",
      "        4.7264e+04, 3.0256e+04,        inf,        inf, 3.0864e+04,        inf,\n",
      "        3.8048e+04,        inf,        inf, 3.5936e+04, 3.2250e+01, 2.3776e+04,\n",
      "        2.0352e+04,        inf, 1.7712e+04, 4.8448e+04, 5.9808e+04, 2.2000e+04,\n",
      "        1.9104e+04, 3.8720e+04, 6.0600e+03,        inf, 6.0352e+04, 4.9920e+04,\n",
      "        5.0944e+04, 4.6336e+04, 1.6192e+04,        inf, 4.4224e+04,        inf,\n",
      "        5.4432e+04, 3.3408e+04,        inf, 1.5720e+04, 5.6672e+04, 3.1904e+04,\n",
      "        2.6560e+04, 1.8416e+04,        inf, 5.0048e+04, 3.6480e+04,        inf,\n",
      "        1.4528e+04, 1.8864e+04, 7.0960e+03,        inf, 2.1568e+04, 1.2024e+04,\n",
      "        4.7640e+03, 2.4220e+03, 7.1360e+03, 4.5280e+04, 6.0672e+04, 3.7632e+04,\n",
      "        4.2144e+04, 5.1776e+04, 4.8384e+04, 8.9920e+03, 1.6576e+04, 5.3312e+04,\n",
      "               inf,        inf, 2.1328e+04,        inf,        inf, 5.2864e+04,\n",
      "               inf,        inf,        inf, 6.1280e+04,        inf, 4.8096e+04,\n",
      "        4.2592e+04, 9.9750e+02, 3.5648e+04, 1.8512e+04, 4.9312e+04,        inf,\n",
      "        6.5248e+04, 3.8592e+04, 3.2464e+04, 2.5024e+04, 3.6416e+04, 6.4704e+04,\n",
      "        2.8580e+03,        inf, 1.1456e+04, 3.0320e+04,        inf, 2.6416e+04,\n",
      "        1.1896e+04, 6.5152e+04, 4.3648e+04,        inf, 5.4752e+04, 3.6704e+04,\n",
      "        1.6768e+04,        inf, 5.3824e+04, 5.9488e+04,        inf, 6.3488e+04,\n",
      "        1.1992e+04, 2.5312e+04, 1.3424e+04, 3.7248e+04,        inf, 4.8576e+04,\n",
      "        4.4416e+04,        inf, 9.4000e+03, 1.6848e+04, 5.0016e+04,        inf,\n",
      "        3.8368e+04, 4.9000e+03, 2.5152e+04, 4.7000e+03,        inf, 1.1744e+04,\n",
      "        4.2304e+04, 6.0608e+04,        inf, 5.2800e+03, 4.7520e+04, 3.3856e+04,\n",
      "        7.8400e+02, 2.5392e+04, 2.5088e+02, 2.1408e+04,        inf, 4.7968e+04,\n",
      "        3.1648e+04, 6.8900e+02, 3.0720e+04, 2.8288e+04,        inf, 2.1040e+04,\n",
      "        3.5360e+04, 2.0640e+03, 6.2784e+04, 3.6544e+04, 9.0720e+03, 4.5536e+04,\n",
      "               inf, 1.6720e+04, 1.0264e+04,        inf,        inf, 1.6570e+03,\n",
      "        6.0576e+04, 5.1400e+03, 5.6960e+04, 2.4048e+04,        inf, 7.4200e+03,\n",
      "        7.0480e+03, 4.2080e+03, 4.0992e+04, 5.9712e+04, 3.7360e+03, 4.7968e+04,\n",
      "               inf, 5.5424e+04, 5.6928e+04, 9.8960e+03,        inf, 1.1512e+04,\n",
      "        4.8032e+04, 8.9600e+03,        inf, 6.5184e+04, 5.8304e+04, 2.0880e+04,\n",
      "        2.7600e+03, 3.6768e+04, 4.5376e+04, 3.4336e+04, 3.5104e+04, 4.2176e+04,\n",
      "        2.1792e+04, 2.0928e+04, 4.9320e+03,        inf,        inf,        inf,\n",
      "        3.8464e+04, 4.9728e+04, 1.4832e+04,        inf, 3.2160e+04,        inf,\n",
      "               inf, 2.6400e+03, 8.5120e+03, 2.9168e+04, 2.9136e+04, 1.9320e+03,\n",
      "        2.2784e+04,        inf,        inf, 2.7936e+04, 3.4208e+04, 1.6288e+04,\n",
      "        3.4784e+04, 2.1408e+04, 5.7360e+03, 1.8192e+04, 2.7088e+04,        inf,\n",
      "        5.0368e+04, 1.4648e+04, 4.6816e+04, 2.5536e+04,        inf, 2.2080e+04,\n",
      "        6.2784e+04,        inf, 6.3968e+04, 3.8176e+04, 4.8576e+04, 5.5760e+03,\n",
      "        1.9200e+04, 2.9088e+04, 3.0768e+04,        inf,        inf, 3.8144e+04,\n",
      "        6.1472e+04, 4.4960e+04,        inf, 1.5560e+04, 9.8080e+03, 5.0560e+04,\n",
      "               inf, 2.8288e+04, 4.2464e+04, 5.8048e+04, 3.1360e+03,        inf,\n",
      "        4.5792e+04,        inf,        inf,        inf,        inf, 4.9120e+04,\n",
      "               inf, 2.8048e+04, 1.6152e+04,        inf,        inf, 4.2368e+04,\n",
      "        6.3040e+04,        inf, 1.3072e+04, 6.2600e+03, 1.4808e+04, 1.2080e+04,\n",
      "        2.5168e+04, 5.1488e+04, 1.1520e+04, 5.9264e+04, 4.2016e+04, 6.1728e+04,\n",
      "               inf, 6.1568e+04, 9.7120e+03, 2.7000e+03, 8.8560e+03,        inf,\n",
      "        2.9584e+04, 9.2560e+03, 5.6096e+04, 2.7280e+04, 3.5552e+04, 5.3312e+04,\n",
      "               inf, 2.1328e+04,        inf,        inf, 5.3376e+04, 5.1616e+04,\n",
      "               inf, 5.2800e+03, 1.6208e+04, 3.0960e+04, 5.2384e+04, 3.9020e+03,\n",
      "        8.3360e+03, 3.2000e+04,        inf, 4.8864e+04, 4.2144e+04, 4.4576e+04,\n",
      "        2.2448e+04, 4.4960e+04, 3.2896e+04, 3.0720e+04, 5.1072e+04, 1.4160e+03,\n",
      "        3.2144e+04, 5.9904e+04, 1.4104e+04, 1.2400e+04, 1.9312e+04, 2.0848e+04,\n",
      "               inf, 2.5248e+04, 5.7888e+04, 5.9840e+04, 1.0540e+03,        inf,\n",
      "        1.4368e+04, 1.6248e+04, 1.7984e+04, 5.6000e+04, 2.0624e+04, 3.1648e+04,\n",
      "               inf, 2.1152e+04, 3.9712e+04, 1.8432e+04, 1.7456e+04, 4.3808e+04,\n",
      "        3.9296e+04,        inf, 1.6310e+03, 6.1984e+04, 3.1392e+04, 3.9072e+04,\n",
      "        3.1872e+04, 3.2736e+04, 2.8000e+04,        inf, 5.3240e+03,        inf,\n",
      "        2.0624e+04, 1.0776e+04, 6.2432e+04, 6.4512e+04, 2.0352e+04, 1.5608e+04,\n",
      "        5.1936e+04, 1.8544e+04, 2.4544e+04,        inf,        inf, 1.1456e+04,\n",
      "        2.3200e+03, 6.0832e+04, 4.0384e+04,        inf, 3.6675e+02, 2.6400e+04,\n",
      "        1.7780e+03, 1.5030e+03, 2.0864e+04,        inf, 1.4576e+04,        inf,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([[24336.0000, 10112.0000,   888.0000,  ...,  9344.0000, 27296.0000,\n",
      "         10728.0000],\n",
      "        [ 5860.0000,  3284.0000,  1835.0000,  ...,  2606.0000,  8600.0000,\n",
      "          2654.0000],\n",
      "        [ 4252.0000,  4648.0000,  2704.0000,  ...,  3908.0000, 11256.0000,\n",
      "          7856.0000],\n",
      "        ...,\n",
      "        [ 2484.0000, 16672.0000,   532.0000,  ...,  2696.0000, 24256.0000,\n",
      "         14224.0000],\n",
      "        [ 4208.0000,  4832.0000,   876.5000,  ...,   405.2500,  9504.0000,\n",
      "          3754.0000],\n",
      "        [ 1012.0000,  3720.0000,  1854.0000,  ...,  3814.0000,  3920.0000,\n",
      "          2412.0000]], device='cuda:0', dtype=torch.float16), tensor([23744.,  7440.,  9944.,  ..., 20160.,  6960.,  3976.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[ 7084.0000,  3772.0000,  5784.0000,  ..., 29152.0000,  1975.0000,\n",
      "          8872.0000],\n",
      "        [ 8624.0000,  1127.0000,  3812.0000,  ..., 21312.0000,  1780.0000,\n",
      "          4748.0000],\n",
      "        [14080.0000,   937.0000,  1992.0000,  ...,  8912.0000,  2456.0000,\n",
      "          1544.0000],\n",
      "        ...,\n",
      "        [19840.0000,   868.5000,  1697.0000,  ...,  7596.0000,  3308.0000,\n",
      "          3814.0000],\n",
      "        [22832.0000,  2860.0000,  5208.0000,  ..., 12208.0000,  2452.0000,\n",
      "          7024.0000],\n",
      "        [46816.0000,  1922.0000,  6656.0000,  ..., 12728.0000,  6136.0000,\n",
      "          8288.0000]], device='cuda:0', dtype=torch.float16), tensor([21808.0000, 26976.0000, 30656.0000, 61888.0000,        inf, 49664.0000,\n",
      "        29392.0000, 55296.0000,        inf, 30144.0000, 31536.0000,        inf,\n",
      "         9776.0000,        inf, 65088.0000, 15712.0000,  2948.0000, 49056.0000,\n",
      "               inf, 57632.0000,  7724.0000,        inf, 42080.0000, 10344.0000,\n",
      "        30608.0000,  2464.0000,  7276.0000, 15440.0000, 44480.0000, 45728.0000,\n",
      "        35008.0000, 31936.0000, 54656.0000,  5180.0000, 21792.0000, 57888.0000,\n",
      "        42432.0000, 31568.0000,  9880.0000, 28384.0000,        inf, 13384.0000,\n",
      "               inf,  7380.0000, 42912.0000, 59104.0000, 40800.0000, 37408.0000,\n",
      "        51680.0000, 36000.0000,        inf, 19376.0000, 25200.0000,  2534.0000,\n",
      "        33600.0000, 60128.0000,  4944.0000,        inf, 28464.0000, 64928.0000,\n",
      "        48352.0000, 25280.0000, 15280.0000,        inf,  2092.0000, 35744.0000,\n",
      "               inf, 56288.0000, 27792.0000, 28992.0000, 34464.0000, 58720.0000,\n",
      "         9600.0000,  6072.0000,        inf,  9424.0000,        inf,        inf,\n",
      "        10656.0000, 20400.0000, 17040.0000,        inf,        inf, 24000.0000,\n",
      "               inf,        inf,        inf,  7416.0000, 18816.0000, 26784.0000,\n",
      "        31376.0000,        inf,        inf, 15648.0000, 17888.0000,        inf,\n",
      "        11144.0000, 13152.0000,        inf,  2228.0000, 29056.0000,   799.0000,\n",
      "        21296.0000,        inf,        inf,  1704.0000, 51936.0000, 25408.0000,\n",
      "               inf,  4058.0000,  9936.0000,        inf, 51584.0000, 41312.0000,\n",
      "        32192.0000, 20048.0000,  5496.0000, 64224.0000, 42304.0000, 36640.0000,\n",
      "               inf,  6192.0000, 21744.0000,        inf,  9744.0000,        inf,\n",
      "               inf,  6644.0000, 31360.0000,        inf, 64320.0000,        inf,\n",
      "         9384.0000, 56320.0000,        inf, 26432.0000, 40480.0000, 13608.0000,\n",
      "          312.0000,        inf, 16688.0000,        inf, 10264.0000, 27792.0000,\n",
      "        50720.0000,        inf, 47520.0000,  8160.0000,        inf, 64672.0000,\n",
      "        55904.0000,  6892.0000,        inf,        inf, 16848.0000,        inf,\n",
      "        24272.0000,        inf, 56992.0000, 48448.0000,  2572.0000,  9536.0000,\n",
      "        31840.0000, 47456.0000, 10192.0000, 56576.0000, 38848.0000, 22240.0000,\n",
      "        11072.0000, 40224.0000,  9680.0000,        inf, 60192.0000, 12816.0000,\n",
      "        38048.0000, 28256.0000, 14272.0000, 44736.0000,        inf, 64352.0000,\n",
      "        56704.0000, 27392.0000,        inf, 12136.0000, 54464.0000, 36672.0000,\n",
      "        39072.0000, 10824.0000,        inf,        inf, 35616.0000,        inf,\n",
      "          905.0000, 35296.0000,  6740.0000, 53280.0000,  2940.0000, 19328.0000,\n",
      "        23504.0000,  8320.0000, 16296.0000,        inf,        inf, 29280.0000,\n",
      "        11808.0000, 62688.0000, 38720.0000,  2406.0000, 28640.0000, 61536.0000,\n",
      "               inf,        inf, 39680.0000,        inf,        inf, 46592.0000,\n",
      "               inf,        inf,        inf,        inf,        inf, 51520.0000,\n",
      "        39840.0000,  3556.0000, 25296.0000, 40192.0000, 24576.0000, 62720.0000,\n",
      "        21600.0000, 14000.0000, 60000.0000, 25456.0000, 24320.0000, 53408.0000,\n",
      "        11192.0000,        inf, 16056.0000,  5628.0000,        inf, 32272.0000,\n",
      "        17488.0000,        inf, 52352.0000, 63296.0000,        inf, 11152.0000,\n",
      "          892.5000,        inf, 47136.0000, 46560.0000,        inf, 47776.0000,\n",
      "        14720.0000,  7608.0000, 13680.0000, 34656.0000, 60512.0000, 60288.0000,\n",
      "        47072.0000, 40352.0000,  3836.0000, 29728.0000, 52896.0000,        inf,\n",
      "        44064.0000,  3464.0000, 20560.0000, 21856.0000,        inf,  3610.0000,\n",
      "        48128.0000, 57120.0000,        inf, 29328.0000, 49376.0000, 37248.0000,\n",
      "         8448.0000, 10152.0000, 30352.0000, 26704.0000, 46688.0000, 20528.0000,\n",
      "         9656.0000,  2528.0000, 20688.0000, 39328.0000, 48000.0000, 17296.0000,\n",
      "        27040.0000,  6568.0000, 51296.0000, 42624.0000,  7420.0000, 36960.0000,\n",
      "               inf, 13336.0000,  3108.0000, 60320.0000,        inf,  8688.0000,\n",
      "               inf,  7908.0000, 33632.0000, 30720.0000,        inf, 13912.0000,\n",
      "        22592.0000, 15632.0000,  1035.0000, 38208.0000,  5952.0000, 40416.0000,\n",
      "               inf, 34656.0000,        inf, 27088.0000,        inf, 10424.0000,\n",
      "        64672.0000, 29360.0000,        inf, 64160.0000, 43424.0000,  1568.0000,\n",
      "         2116.0000, 33440.0000, 18704.0000, 37120.0000, 21600.0000, 49728.0000,\n",
      "        36832.0000,  3744.0000, 29632.0000, 59200.0000, 45920.0000,        inf,\n",
      "        36224.0000, 29984.0000, 20544.0000,        inf,        inf, 50560.0000,\n",
      "        64704.0000, 13960.0000, 19680.0000, 19968.0000,  8344.0000,  5560.0000,\n",
      "        36992.0000,        inf,        inf, 28064.0000,        inf, 11024.0000,\n",
      "        39680.0000,  3698.0000, 17360.0000,  5692.0000, 11632.0000,        inf,\n",
      "        54336.0000, 19056.0000, 46656.0000, 32720.0000,        inf,   271.2500,\n",
      "        50080.0000,        inf, 64352.0000, 46208.0000, 32832.0000, 26256.0000,\n",
      "        25024.0000, 11328.0000, 20640.0000,        inf,        inf,  3624.0000,\n",
      "        52096.0000, 27360.0000,        inf,  1949.0000, 10624.0000, 31856.0000,\n",
      "               inf, 19040.0000, 40320.0000,        inf, 22256.0000,        inf,\n",
      "        40000.0000,        inf,        inf,        inf,        inf, 13600.0000,\n",
      "               inf, 23088.0000, 26160.0000,        inf,        inf, 34496.0000,\n",
      "        55360.0000,        inf, 33152.0000, 17616.0000,  4912.0000,  2376.0000,\n",
      "         9008.0000, 48800.0000, 24048.0000, 58880.0000, 58016.0000, 46464.0000,\n",
      "               inf, 21888.0000,  8528.0000, 24752.0000,  6356.0000,        inf,\n",
      "         8264.0000,  4836.0000, 60288.0000, 16192.0000, 53056.0000,        inf,\n",
      "               inf, 29552.0000,        inf, 47168.0000, 21088.0000, 58144.0000,\n",
      "               inf, 19552.0000, 14248.0000, 22592.0000,        inf, 12552.0000,\n",
      "        35488.0000, 44928.0000,        inf, 29136.0000, 32480.0000, 36320.0000,\n",
      "        22464.0000, 16224.0000, 30256.0000,        inf, 22736.0000, 21104.0000,\n",
      "        33344.0000, 40864.0000, 16264.0000, 23040.0000,  5016.0000,  1295.0000,\n",
      "               inf, 40896.0000, 56512.0000, 43200.0000,  2962.0000, 61760.0000,\n",
      "         1139.0000, 33728.0000, 25360.0000, 26464.0000, 43488.0000, 52224.0000,\n",
      "               inf,  8456.0000, 21792.0000,  7712.0000, 25920.0000, 18496.0000,\n",
      "        42336.0000,        inf, 12568.0000, 53024.0000,  3078.0000,  2448.0000,\n",
      "        20688.0000, 40160.0000,   112.6250, 63744.0000, 19824.0000, 31984.0000,\n",
      "        14152.0000, 23552.0000,        inf, 40736.0000, 27968.0000, 29712.0000,\n",
      "        29840.0000, 42688.0000, 17712.0000,        inf,        inf, 14856.0000,\n",
      "        29152.0000, 43648.0000, 30464.0000, 50208.0000, 14136.0000, 20720.0000,\n",
      "         5748.0000, 23120.0000, 10768.0000, 64384.0000,  5828.0000, 44800.0000,\n",
      "        61376.0000,        inf], device='cuda:0', dtype=torch.float16), tensor([ 6396.0000,  4784.0000,  5064.0000,  6976.0000,  7908.0000,  3844.0000,\n",
      "        13384.0000, 55840.0000, 23328.0000, 13416.0000,  1180.0000, 40032.0000,\n",
      "        28992.0000, 11792.0000, 24784.0000,  7000.0000, 15264.0000, 45952.0000,\n",
      "        53344.0000, 41824.0000,  2390.0000, 59328.0000, 12664.0000,   443.2500,\n",
      "        11304.0000, 10136.0000, 10256.0000,  6568.0000, 24160.0000,        inf,\n",
      "         9152.0000, 30656.0000, 34144.0000, 20288.0000, 36096.0000,  6248.0000,\n",
      "         9712.0000, 19376.0000, 13400.0000, 22736.0000, 19536.0000, 10584.0000,\n",
      "        51392.0000, 21856.0000, 19200.0000, 58880.0000, 35008.0000, 41536.0000,\n",
      "        10888.0000,  6544.0000, 52768.0000, 16544.0000, 21520.0000, 14240.0000,\n",
      "         2966.0000, 11376.0000,  3540.0000,  1006.5000,  1035.0000, 52576.0000,\n",
      "        22688.0000, 14904.0000, 14048.0000, 24128.0000,  1386.0000, 13888.0000,\n",
      "               inf, 19056.0000, 12128.0000, 35520.0000, 11600.0000, 15792.0000,\n",
      "        11584.0000,  3786.0000, 38656.0000, 18176.0000, 24384.0000,        inf,\n",
      "         3280.0000,  8624.0000, 10632.0000,  2976.0000, 30688.0000,  2426.0000,\n",
      "        14392.0000, 52416.0000,        inf,  8848.0000,  4444.0000,   351.0000,\n",
      "         2208.0000,        inf,   300.5000,  1600.0000,  5424.0000,        inf,\n",
      "         7464.0000,  3170.0000,  5316.0000, 19152.0000,  8440.0000, 13048.0000,\n",
      "         8352.0000,  6804.0000, 20480.0000,  6820.0000,  8448.0000,  6860.0000,\n",
      "        45792.0000,  5740.0000, 14296.0000, 50080.0000,  1041.0000,  9000.0000,\n",
      "         5636.0000,   199.3750,   607.0000, 12672.0000,  5948.0000,  3556.0000,\n",
      "        39936.0000,  2220.0000,  9344.0000,        inf,  4664.0000,  7348.0000,\n",
      "        52928.0000,  6996.0000,  6660.0000, 14320.0000, 11616.0000, 42464.0000,\n",
      "        10208.0000, 11456.0000, 23424.0000,  5796.0000, 41696.0000,  5380.0000,\n",
      "         3096.0000, 18096.0000,  5804.0000,  2988.0000, 22832.0000,   261.7500,\n",
      "         9360.0000, 30848.0000, 15648.0000,   576.0000,  8124.0000, 30560.0000,\n",
      "         8544.0000, 10408.0000, 62720.0000, 50080.0000,  4956.0000,  4912.0000,\n",
      "        38496.0000,        inf, 13552.0000,  5980.0000,   507.5000,  5644.0000,\n",
      "        28064.0000, 47360.0000,  1722.0000, 24000.0000, 55808.0000,   750.0000,\n",
      "          803.0000,   826.0000,  6136.0000,        inf,  3490.0000,   812.5000,\n",
      "        16432.0000, 58688.0000,  9856.0000, 26064.0000, 26432.0000, 27664.0000,\n",
      "        40288.0000, 11120.0000, 59904.0000,   139.5000, 17296.0000, 15856.0000,\n",
      "        17664.0000,  6840.0000, 33024.0000, 36416.0000,  2560.0000, 34528.0000,\n",
      "         9000.0000,  8992.0000,  4696.0000, 10608.0000,  3522.0000,  3052.0000,\n",
      "          949.5000,   958.0000, 12624.0000,  2080.0000,  4556.0000, 15192.0000,\n",
      "         8600.0000,  4976.0000, 14536.0000,  4128.0000, 13984.0000, 13232.0000,\n",
      "        46176.0000,        inf, 12216.0000, 44640.0000, 29408.0000, 35520.0000,\n",
      "               inf, 32992.0000, 35264.0000, 19440.0000, 25152.0000, 27360.0000,\n",
      "         3212.0000,  5192.0000, 25328.0000,  8368.0000, 24880.0000, 25296.0000,\n",
      "        59008.0000, 35008.0000, 17648.0000, 14264.0000,  6012.0000,  5548.0000,\n",
      "         1827.0000,        inf,  1624.0000, 13888.0000,  3968.0000, 15736.0000,\n",
      "         3666.0000, 18688.0000, 26480.0000, 42240.0000, 35424.0000, 25232.0000,\n",
      "         9536.0000,        inf,  5988.0000,        inf, 41056.0000,  3240.0000,\n",
      "         1328.0000, 21504.0000,  8080.0000, 23568.0000,  3794.0000, 12680.0000,\n",
      "        14952.0000, 12176.0000,  6660.0000, 14160.0000,  3772.0000, 59776.0000,\n",
      "         6360.0000,  6644.0000, 17088.0000,  5156.0000, 31584.0000,  5252.0000,\n",
      "        32896.0000, 27616.0000,        inf,  4804.0000, 17840.0000, 53440.0000,\n",
      "        11208.0000,  2888.0000,  5728.0000, 13768.0000, 61888.0000, 22016.0000,\n",
      "        22640.0000,  3916.0000,  7704.0000, 20176.0000, 19760.0000, 23472.0000,\n",
      "        12624.0000,  6168.0000,  7192.0000, 54944.0000,  1726.0000, 10392.0000,\n",
      "        59520.0000,  2007.0000, 18368.0000,        inf,        inf, 11056.0000,\n",
      "        51552.0000, 10408.0000, 14520.0000, 15920.0000, 62112.0000,  3592.0000,\n",
      "         1093.0000,  1929.0000,  9736.0000, 52160.0000,  6264.0000,  5496.0000,\n",
      "        22992.0000, 27200.0000, 16432.0000,  6680.0000, 52896.0000, 33952.0000,\n",
      "         3236.0000,  7708.0000, 48576.0000, 24928.0000,  8792.0000,  6748.0000,\n",
      "         4256.0000, 35072.0000, 62272.0000, 57312.0000, 25120.0000, 13712.0000,\n",
      "         8528.0000,  2164.0000,  8480.0000, 34080.0000, 25664.0000,        inf,\n",
      "        48320.0000, 29104.0000, 11368.0000, 16416.0000, 42336.0000, 58464.0000,\n",
      "               inf, 10824.0000, 13824.0000, 11584.0000, 10456.0000,  2132.0000,\n",
      "         2996.0000,  4948.0000, 18160.0000,  5604.0000, 38496.0000, 10240.0000,\n",
      "        19568.0000, 18880.0000,  2608.0000,  5620.0000,  2484.0000,  2072.0000,\n",
      "        45024.0000,  8400.0000, 40192.0000, 11080.0000,        inf,  7172.0000,\n",
      "         1895.0000, 25200.0000, 13048.0000, 33312.0000,  8984.0000,  2970.0000,\n",
      "        19632.0000, 50560.0000, 18160.0000,        inf, 21808.0000, 19584.0000,\n",
      "        46560.0000, 37248.0000,        inf,  4112.0000,  6748.0000,  2206.0000,\n",
      "        64992.0000, 21216.0000, 26336.0000, 20000.0000, 10968.0000, 59552.0000,\n",
      "        46208.0000, 13208.0000,        inf,        inf,        inf, 38208.0000,\n",
      "               inf,  2846.0000, 16576.0000, 58944.0000, 42560.0000,  2346.0000,\n",
      "               inf, 15976.0000, 11952.0000, 14896.0000, 12016.0000,  7736.0000,\n",
      "        21984.0000, 40608.0000, 15088.0000, 39520.0000, 31040.0000, 15016.0000,\n",
      "               inf, 50176.0000, 10640.0000,  1520.0000,  5784.0000, 21360.0000,\n",
      "        23424.0000,  6260.0000,        inf, 25312.0000, 39232.0000, 53472.0000,\n",
      "               inf, 11304.0000, 48192.0000, 57632.0000, 39840.0000, 60416.0000,\n",
      "               inf,  2102.0000, 14832.0000, 29408.0000, 58400.0000,  2628.0000,\n",
      "          151.8750, 14520.0000,        inf, 38848.0000, 24192.0000, 58112.0000,\n",
      "        15688.0000,        inf, 32000.0000,  9848.0000, 58624.0000,  7312.0000,\n",
      "        37760.0000, 36192.0000,  6416.0000, 14016.0000, 16640.0000, 17760.0000,\n",
      "               inf, 28640.0000,  6160.0000, 57728.0000,  7776.0000, 16496.0000,\n",
      "          298.2500, 21808.0000, 10320.0000, 30624.0000, 19312.0000, 23008.0000,\n",
      "               inf,  4868.0000, 52704.0000, 17808.0000, 16184.0000, 30960.0000,\n",
      "        34880.0000, 62304.0000,  2194.0000, 58976.0000, 13480.0000, 28272.0000,\n",
      "         6616.0000, 19568.0000, 16360.0000, 10544.0000, 13408.0000,        inf,\n",
      "        20592.0000,  7540.0000, 49600.0000, 28896.0000,  4544.0000, 17744.0000,\n",
      "        51488.0000,  5540.0000, 20800.0000,        inf, 36224.0000,  1570.0000,\n",
      "         2492.0000,        inf, 28928.0000,        inf,   289.0000, 24784.0000,\n",
      "         1851.0000,  4652.0000, 15664.0000,  5104.0000,  9976.0000,  7096.0000,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([15400.0000,  3922.0000, 39232.0000, 54464.0000,        inf, 21424.0000,\n",
      "        39424.0000, 51712.0000,        inf, 33568.0000, 11856.0000,        inf,\n",
      "        25536.0000, 41696.0000,        inf, 30400.0000, 13640.0000, 46432.0000,\n",
      "               inf, 51648.0000,  3962.0000,        inf,        inf, 19424.0000,\n",
      "        26448.0000, 21888.0000, 11360.0000,  3750.0000, 41568.0000, 50976.0000,\n",
      "         5596.0000, 34848.0000, 55232.0000, 20256.0000, 47264.0000, 30832.0000,\n",
      "        46688.0000, 14584.0000,  1195.0000, 57056.0000,        inf,  3230.0000,\n",
      "               inf, 36960.0000, 59264.0000, 58176.0000, 49152.0000,        inf,\n",
      "        21616.0000, 19472.0000,        inf, 52256.0000, 23568.0000, 25024.0000,\n",
      "        24336.0000, 35520.0000,  3380.0000, 38208.0000,  8728.0000,        inf,\n",
      "        55296.0000, 23872.0000, 31184.0000,        inf,  2754.0000,        inf,\n",
      "               inf, 57760.0000, 38496.0000, 57952.0000, 31680.0000,        inf,\n",
      "        21936.0000, 29168.0000,        inf, 20784.0000,        inf,        inf,\n",
      "        27408.0000,  8760.0000, 23792.0000, 64544.0000, 59808.0000, 27536.0000,\n",
      "               inf,        inf,        inf, 11448.0000,  3258.0000, 15296.0000,\n",
      "        13248.0000,        inf,        inf, 20176.0000, 23728.0000,        inf,\n",
      "         6228.0000,   722.5000, 60704.0000, 25232.0000, 38944.0000, 29168.0000,\n",
      "        24128.0000, 48448.0000,        inf, 29840.0000, 46656.0000, 47872.0000,\n",
      "        65440.0000,  1618.0000, 10232.0000,        inf, 38432.0000, 33824.0000,\n",
      "        23808.0000,  1519.0000,  3494.0000, 58272.0000, 27008.0000, 35360.0000,\n",
      "               inf,  6668.0000, 50752.0000,        inf,  1004.5000,        inf,\n",
      "               inf,  8776.0000, 27584.0000, 61120.0000, 33920.0000,        inf,\n",
      "        13160.0000, 59680.0000,        inf, 11024.0000, 60288.0000, 27824.0000,\n",
      "         1562.0000, 43840.0000, 32928.0000, 50400.0000, 14608.0000, 24672.0000,\n",
      "        53664.0000,        inf,        inf,  4796.0000,        inf, 59744.0000,\n",
      "        48352.0000, 29648.0000,        inf,        inf, 28592.0000,        inf,\n",
      "        36928.0000,        inf,        inf, 35456.0000,  1105.0000, 24192.0000,\n",
      "        22752.0000,        inf, 17632.0000, 47584.0000, 59392.0000, 20336.0000,\n",
      "        18320.0000, 37376.0000,  5116.0000,        inf, 62240.0000, 48864.0000,\n",
      "        54048.0000, 47552.0000, 14296.0000,        inf, 46336.0000,        inf,\n",
      "        55680.0000, 33696.0000,        inf, 15016.0000, 57376.0000, 31552.0000,\n",
      "        25136.0000, 16704.0000,        inf, 51680.0000, 36768.0000,        inf,\n",
      "        16544.0000, 18576.0000,  6724.0000,        inf, 24576.0000, 11952.0000,\n",
      "         3962.0000,  3828.0000,  5612.0000, 45440.0000, 64640.0000, 39136.0000,\n",
      "        43328.0000, 53248.0000, 46880.0000,  7472.0000, 16832.0000, 54656.0000,\n",
      "               inf,        inf, 20752.0000,        inf,        inf, 54272.0000,\n",
      "               inf,        inf,        inf, 62016.0000,        inf, 48992.0000,\n",
      "        43136.0000,   771.0000, 36544.0000, 19584.0000, 51008.0000,        inf,\n",
      "        64576.0000, 40544.0000, 32064.0000, 24080.0000, 37632.0000,        inf,\n",
      "         4144.0000,        inf, 11432.0000, 30944.0000,        inf, 26288.0000,\n",
      "        12424.0000, 65408.0000, 45600.0000,        inf, 55360.0000, 38592.0000,\n",
      "        16360.0000,        inf, 54464.0000, 59936.0000,        inf, 65152.0000,\n",
      "        10784.0000, 26160.0000, 12360.0000, 39520.0000,        inf, 48672.0000,\n",
      "        46016.0000,        inf, 10656.0000, 18528.0000, 48064.0000,        inf,\n",
      "        39616.0000,  2890.0000, 25664.0000,  5016.0000,        inf, 12600.0000,\n",
      "        44928.0000, 59840.0000,        inf,  4888.0000, 49184.0000, 36992.0000,\n",
      "         2058.0000, 24736.0000,   205.7500, 22640.0000,        inf, 49312.0000,\n",
      "        31872.0000,  1111.0000, 30224.0000, 27840.0000,        inf, 21536.0000,\n",
      "        35552.0000,  4044.0000, 63648.0000, 38016.0000,  5748.0000, 46048.0000,\n",
      "               inf, 16720.0000,  9600.0000,        inf,        inf,  2866.0000,\n",
      "        60928.0000,  6048.0000, 56672.0000, 22656.0000,        inf,  7636.0000,\n",
      "         7056.0000,  4148.0000, 44096.0000, 58752.0000,  3348.0000, 49920.0000,\n",
      "               inf, 56832.0000, 57888.0000, 10864.0000,        inf, 12840.0000,\n",
      "        48704.0000, 10200.0000,        inf,        inf, 61536.0000, 19968.0000,\n",
      "         3876.0000, 37184.0000, 47904.0000, 36384.0000, 35456.0000, 43232.0000,\n",
      "        20000.0000, 22272.0000,  4572.0000,        inf,        inf,        inf,\n",
      "        39616.0000, 50816.0000, 18256.0000,        inf, 32080.0000,        inf,\n",
      "               inf,  5048.0000, 11160.0000, 29344.0000, 28512.0000,  1939.0000,\n",
      "        22832.0000,        inf,        inf, 29264.0000, 34752.0000, 17104.0000,\n",
      "        34368.0000, 23008.0000,  7240.0000, 17376.0000, 26928.0000,        inf,\n",
      "        52096.0000, 14792.0000, 45536.0000, 25040.0000,        inf, 22848.0000,\n",
      "        61312.0000,        inf, 64800.0000, 38592.0000, 50144.0000,  6172.0000,\n",
      "        19952.0000, 31216.0000, 30048.0000,        inf,        inf, 39904.0000,\n",
      "        63360.0000, 44960.0000,        inf, 16248.0000, 11072.0000, 50944.0000,\n",
      "               inf, 28352.0000, 42176.0000, 59744.0000,  4716.0000,        inf,\n",
      "        46208.0000,        inf,        inf,        inf,        inf, 50528.0000,\n",
      "               inf, 27776.0000, 16992.0000,        inf,        inf, 43840.0000,\n",
      "        63584.0000,        inf, 12856.0000,  7828.0000, 15512.0000, 12480.0000,\n",
      "        25600.0000, 53024.0000, 12000.0000, 62432.0000, 43040.0000, 63296.0000,\n",
      "               inf, 60992.0000, 10816.0000,  4452.0000,  8472.0000,        inf,\n",
      "        30064.0000, 10168.0000, 55808.0000, 27552.0000, 34976.0000, 54272.0000,\n",
      "               inf, 20896.0000,        inf,        inf, 55168.0000, 51936.0000,\n",
      "               inf,  6620.0000, 15392.0000, 31632.0000, 53760.0000,  5096.0000,\n",
      "        10120.0000, 32592.0000,        inf, 52416.0000, 40768.0000, 45440.0000,\n",
      "        22832.0000, 45408.0000, 31696.0000, 30720.0000, 52128.0000,   647.5000,\n",
      "        31808.0000, 61536.0000, 14368.0000, 12128.0000, 19632.0000, 20528.0000,\n",
      "               inf, 24816.0000, 59424.0000, 57664.0000,   150.6250,        inf,\n",
      "        13400.0000, 16832.0000, 18448.0000, 57056.0000, 21440.0000, 32400.0000,\n",
      "               inf, 19712.0000, 40096.0000, 17856.0000, 20112.0000, 44448.0000,\n",
      "        40416.0000,        inf,  1453.0000, 63872.0000, 34272.0000, 39392.0000,\n",
      "        32176.0000, 31152.0000, 27200.0000,        inf,  5904.0000,        inf,\n",
      "        22912.0000, 11200.0000, 63328.0000,        inf, 22912.0000, 17024.0000,\n",
      "        53536.0000, 19552.0000, 26576.0000,        inf,        inf, 12216.0000,\n",
      "         2526.0000, 62528.0000, 42976.0000,        inf,   109.9375, 26704.0000,\n",
      "         2134.0000,   628.5000, 20448.0000,        inf, 15280.0000,        inf,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([7.2000e+03, 1.8272e+04, 1.2650e+03, 3.0224e+04, 1.3640e+04, 9.5920e+03,\n",
      "        8.6480e+03,        inf,        inf, 2.6860e+03, 9.5360e+03, 1.1336e+04,\n",
      "        1.8736e+04, 9.4800e+03, 1.4528e+04, 6.8200e+03, 5.4880e+03, 5.9232e+04,\n",
      "               inf, 2.3776e+04, 7.1750e+02, 4.6624e+04, 1.2504e+04, 9.4850e+02,\n",
      "        1.6370e+03, 4.0260e+03, 8.0120e+03, 1.1072e+04, 4.7072e+04, 5.8656e+04,\n",
      "        2.5360e+04, 3.5712e+04, 3.0384e+04, 1.6896e+04, 2.1264e+04, 2.4128e+04,\n",
      "        7.5640e+03, 3.2832e+04, 4.5520e+03, 1.5096e+04, 3.4944e+04, 3.9860e+03,\n",
      "        3.2832e+04, 1.0336e+04, 2.6340e+03, 4.6112e+04, 2.4576e+04, 2.3744e+04,\n",
      "        2.6496e+04, 2.4608e+04, 6.5472e+04, 7.1280e+03, 3.1424e+04, 2.7480e+03,\n",
      "        1.4630e+03, 1.2032e+04, 4.3360e+03, 5.7960e+03, 1.3440e+04, 4.5696e+04,\n",
      "        2.5280e+04, 1.2680e+04, 4.7400e+03, 2.1856e+04, 7.1920e+03, 2.9320e+03,\n",
      "               inf, 6.3240e+03, 9.2960e+03, 2.3248e+04, 2.0020e+03, 3.8620e+03,\n",
      "        8.0640e+03, 6.4960e+03, 2.9040e+04, 2.9475e+02, 2.9232e+04, 4.2048e+04,\n",
      "        3.4780e+03, 8.6240e+03, 2.7820e+03, 1.5390e+03, 3.0752e+04, 5.8800e+03,\n",
      "        3.1056e+04, 1.7392e+04, 4.8256e+04, 9.6000e+03, 1.1240e+04, 8.9040e+03,\n",
      "        6.7200e+03,        inf, 2.1920e+03, 2.1820e+03, 3.1260e+03, 6.3904e+04,\n",
      "        8.7200e+03, 1.4050e+02, 4.0560e+03, 3.6720e+03, 1.6072e+04, 3.5380e+03,\n",
      "        1.3816e+04, 1.9792e+04, 5.7088e+04, 8.2240e+03, 1.5744e+04, 6.9080e+03,\n",
      "               inf, 1.0512e+04, 1.1320e+04, 4.3264e+04, 2.4160e+04, 1.5560e+04,\n",
      "        1.1152e+04, 1.4510e+03, 1.5970e+03, 1.3496e+04, 4.9960e+03, 2.1280e+03,\n",
      "        3.3472e+04, 5.8350e+02, 5.3080e+03,        inf, 1.2816e+04, 1.1376e+04,\n",
      "        3.4688e+04, 5.4920e+03, 9.5760e+03, 1.0680e+04, 2.1060e+03, 8.0160e+03,\n",
      "        7.6720e+03, 7.3320e+03, 7.1840e+03, 1.1360e+04, 2.2160e+04, 3.5060e+03,\n",
      "        1.1570e+03, 2.1240e+03, 4.7760e+03, 2.6992e+04, 4.0580e+03, 1.1032e+04,\n",
      "        2.1020e+03, 2.9360e+04, 6.5480e+03, 3.2800e+03, 2.5440e+03, 5.0176e+04,\n",
      "        1.0680e+04, 5.5720e+03, 3.4784e+04, 3.7536e+04, 1.2112e+02, 1.6464e+04,\n",
      "        1.0312e+04,        inf, 1.2168e+04, 7.3160e+03, 3.4450e+02, 4.1760e+03,\n",
      "        4.3456e+04, 4.2240e+04, 1.2950e+03, 1.2592e+04, 4.7136e+04, 2.1200e+02,\n",
      "        7.1750e+02, 5.6960e+03, 7.9320e+03,        inf, 1.7504e+04, 5.9920e+03,\n",
      "        7.5400e+03, 3.8144e+04, 8.4000e+03, 1.7824e+04,        inf, 3.3952e+04,\n",
      "        5.1552e+04, 1.5072e+04,        inf, 6.8960e+03, 2.9872e+04, 1.1712e+04,\n",
      "        1.9232e+04, 5.1600e+03, 2.3024e+04, 5.7952e+04, 9.5840e+03, 1.2720e+04,\n",
      "        2.3080e+03, 2.9232e+04, 9.4800e+03, 7.2640e+03, 7.0680e+03, 7.9000e+03,\n",
      "        2.5232e+04, 1.5820e+03, 2.3232e+04, 1.1984e+04, 5.5360e+04, 3.7500e+03,\n",
      "        1.9260e+03, 2.9620e+03, 1.6296e+04, 4.7760e+03, 4.2208e+04, 3.4144e+04,\n",
      "        4.3680e+03, 4.0768e+04, 3.7760e+04, 8.6500e+02, 2.0768e+04, 2.7392e+04,\n",
      "               inf, 5.5968e+04, 3.1184e+04, 1.8912e+04, 4.2848e+04, 3.2336e+04,\n",
      "        8.3520e+03, 8.4400e+03, 1.6432e+04, 8.9440e+03, 1.6944e+04, 2.7072e+04,\n",
      "        2.1248e+04, 1.1952e+04, 4.3904e+04, 2.2416e+04, 9.5000e+00, 1.5080e+04,\n",
      "        6.9240e+03,        inf, 2.5520e+03, 5.7560e+03, 4.6880e+04, 1.2200e+04,\n",
      "        4.8520e+03, 2.2160e+04, 1.0640e+04, 2.0768e+04, 4.6368e+04, 1.0112e+04,\n",
      "        1.7712e+02,        inf, 7.3750e+02, 5.7152e+04, 6.4000e+04, 1.9720e+03,\n",
      "        3.1380e+03, 5.8720e+03, 3.5620e+03, 1.3232e+04, 1.0350e+03, 1.2784e+04,\n",
      "        2.0768e+04, 2.0220e+03, 1.5840e+03, 2.7856e+04, 2.1632e+04,        inf,\n",
      "        1.4968e+04, 7.5880e+03, 8.7200e+03, 7.7640e+03,        inf, 1.3400e+03,\n",
      "        2.6032e+04, 3.0240e+04,        inf, 3.2448e+04, 9.4080e+03, 5.9104e+04,\n",
      "        1.1328e+04, 1.1640e+03, 3.2704e+04, 1.6336e+04, 5.1744e+04, 7.3880e+03,\n",
      "        6.8320e+03, 2.5100e+03, 2.5500e+03, 2.0512e+04, 3.2704e+04, 1.1360e+04,\n",
      "        1.0352e+04, 3.2760e+03, 8.9200e+03,        inf, 1.4220e+03, 9.5050e+02,\n",
      "        5.4496e+04, 2.1780e+03, 8.3040e+03,        inf, 2.5424e+04, 3.1400e+03,\n",
      "        2.4160e+04, 5.0360e+03, 5.5560e+03, 2.2144e+04, 2.7536e+04, 8.4720e+03,\n",
      "        1.2624e+04, 1.6912e+04, 6.4000e+03, 3.6832e+04, 1.2400e+04, 8.7600e+03,\n",
      "        4.8192e+04, 1.3592e+04, 3.5040e+04, 9.0480e+03, 5.1872e+04, 2.3952e+04,\n",
      "        6.2400e+03, 1.9600e+04, 3.4176e+04, 2.5952e+04, 1.6352e+04, 1.1310e+03,\n",
      "        7.7000e+03, 3.2992e+04, 3.4464e+04, 5.2864e+04, 3.6608e+04, 4.3040e+04,\n",
      "        8.1040e+03, 5.5440e+03, 1.5712e+04, 3.0608e+04, 7.0440e+03,        inf,\n",
      "        4.8288e+04, 1.9184e+04, 1.5496e+04, 1.2864e+04,        inf, 6.0000e+04,\n",
      "        3.5072e+04, 2.9936e+04, 2.4896e+04, 1.2064e+04, 1.3184e+04, 4.1040e+03,\n",
      "        6.5640e+03, 1.7712e+04, 1.8448e+04, 9.7760e+03,        inf, 6.6200e+03,\n",
      "        1.3088e+04, 3.7140e+03, 9.7200e+02, 6.2680e+03, 1.2900e+03, 5.4000e+03,\n",
      "        4.7904e+04, 7.6160e+03, 3.7216e+04, 4.2880e+03,        inf, 2.9480e+03,\n",
      "        5.0840e+03, 4.2400e+04, 2.4704e+04, 3.8816e+04, 2.1136e+04, 3.0720e+03,\n",
      "        2.4944e+04, 1.6008e+04, 1.3808e+04, 3.3280e+04, 2.6384e+04, 3.2060e+03,\n",
      "        4.0096e+04, 2.4160e+04,        inf, 3.1580e+03, 6.3640e+03, 5.1080e+03,\n",
      "        4.5344e+04, 2.2448e+04, 3.3888e+04, 3.9648e+04, 1.9248e+04, 3.9200e+04,\n",
      "        5.2928e+04, 1.8720e+04,        inf, 6.0320e+04, 4.4864e+04, 7.1800e+03,\n",
      "               inf, 2.5820e+03, 2.6896e+04, 3.6672e+04, 5.9040e+04, 6.5550e+02,\n",
      "        3.9040e+04, 3.3216e+04, 2.8160e+04, 9.2880e+03, 1.0952e+04, 1.2020e+03,\n",
      "        8.8400e+03, 3.6576e+04, 2.3296e+04, 2.6672e+04, 3.5904e+04, 2.6240e+04,\n",
      "        5.1488e+04, 2.1648e+04, 1.3504e+04, 2.6992e+04, 4.3320e+03, 1.8380e+03,\n",
      "        2.6020e+03, 3.0400e+03,        inf, 1.9120e+04,        inf,        inf,\n",
      "               inf, 3.2416e+04, 4.3552e+04, 4.3296e+04, 1.9088e+04, 6.1696e+04,\n",
      "               inf, 4.4920e+03, 2.2992e+04, 2.0528e+04,        inf, 1.2250e+01,\n",
      "        4.8320e+03, 2.3856e+04,        inf, 1.5496e+04, 1.6848e+04, 4.5024e+04,\n",
      "        2.1200e+04, 3.4176e+04, 2.9424e+04, 7.9160e+03, 1.4696e+04, 1.5368e+04,\n",
      "        3.2800e+04, 2.7152e+04, 9.2400e+02, 1.7232e+04, 2.9900e+03, 2.2438e+02,\n",
      "               inf, 3.8976e+04, 3.3080e+03, 4.3776e+04, 7.8680e+03, 1.2720e+04,\n",
      "        2.1640e+03, 3.9008e+04, 1.3312e+04, 1.0440e+04, 5.0976e+04, 4.4864e+04,\n",
      "               inf, 3.1500e+03, 2.9104e+04, 3.2580e+03, 9.7280e+03, 1.6832e+04,\n",
      "        2.9600e+04,        inf, 5.5280e+03, 4.5696e+04, 2.2260e+03, 4.9240e+03,\n",
      "        4.9280e+03, 9.3920e+03, 2.3220e+03, 2.3200e+04, 1.0488e+04, 4.9056e+04,\n",
      "        1.2328e+04, 2.2448e+04, 4.2208e+04, 1.6592e+04, 7.4200e+03, 3.6000e+04,\n",
      "        1.5240e+04, 1.3848e+04, 1.8176e+04, 6.1216e+04, 3.0640e+04, 7.5560e+03,\n",
      "        1.8240e+04, 5.6864e+04, 2.8608e+04, 5.5808e+04, 2.1760e+03, 2.2800e+04,\n",
      "        8.4125e+01, 3.0048e+04, 6.1160e+03, 4.4560e+03, 2.3420e+03, 8.4960e+03,\n",
      "               inf, 5.6320e+04], device='cuda:0', dtype=torch.float16), tensor([2.3312e+04, 2.7712e+04, 3.3376e+04,        inf,        inf, 5.1808e+04,\n",
      "        3.0528e+04, 5.8368e+04,        inf, 3.3632e+04, 3.3376e+04,        inf,\n",
      "        1.0736e+04,        inf,        inf, 1.6184e+04, 2.0740e+03, 5.2160e+04,\n",
      "               inf, 5.9520e+04, 8.2320e+03,        inf, 4.6016e+04, 1.0712e+04,\n",
      "        3.2416e+04, 1.8975e+02, 8.1880e+03, 1.3200e+04, 4.6176e+04, 4.8416e+04,\n",
      "        3.7248e+04, 3.5072e+04, 5.9104e+04, 3.3760e+03, 2.5024e+04, 5.9008e+04,\n",
      "        4.4832e+04, 3.2064e+04, 1.1624e+04, 3.0352e+04,        inf, 1.2280e+04,\n",
      "               inf, 7.6200e+03, 4.4480e+04, 6.2976e+04, 4.3040e+04, 4.0192e+04,\n",
      "        5.3600e+04, 3.7600e+04,        inf, 2.1056e+04, 2.6960e+04, 4.8160e+03,\n",
      "        3.3888e+04,        inf, 6.4280e+03,        inf, 2.9808e+04,        inf,\n",
      "        5.1360e+04, 2.6400e+04, 1.5912e+04,        inf, 1.4660e+03, 3.9168e+04,\n",
      "               inf, 5.9200e+04, 2.8832e+04, 3.0240e+04, 3.6704e+04, 6.1952e+04,\n",
      "        1.2176e+04, 4.8120e+03,        inf, 1.0024e+04,        inf,        inf,\n",
      "        1.3648e+04, 2.2288e+04, 1.9168e+04,        inf,        inf, 2.4864e+04,\n",
      "               inf,        inf,        inf, 8.7280e+03, 1.9920e+04, 2.9920e+04,\n",
      "        3.2176e+04,        inf,        inf, 1.7744e+04, 1.7936e+04,        inf,\n",
      "        1.2016e+04, 1.5128e+04,        inf, 1.3180e+03, 3.0592e+04, 6.5350e+02,\n",
      "        2.4928e+04,        inf,        inf, 1.8160e+03, 5.3536e+04, 2.6064e+04,\n",
      "               inf, 5.0120e+03, 1.0240e+04,        inf, 5.5936e+04, 4.4096e+04,\n",
      "        3.4976e+04, 2.1488e+04, 4.9240e+03,        inf, 4.5248e+04, 3.7760e+04,\n",
      "               inf, 3.3720e+03, 2.2896e+04,        inf, 1.0408e+04,        inf,\n",
      "               inf, 7.5040e+03, 3.1376e+04,        inf,        inf,        inf,\n",
      "        8.2320e+03, 6.0832e+04,        inf, 2.9344e+04, 4.3136e+04, 1.5184e+04,\n",
      "        8.6600e+02,        inf, 1.8000e+04,        inf, 1.0912e+04, 2.9344e+04,\n",
      "        5.3504e+04,        inf, 4.9792e+04, 9.1440e+03,        inf,        inf,\n",
      "        5.8464e+04, 6.8400e+03,        inf,        inf, 1.8416e+04,        inf,\n",
      "        2.3792e+04,        inf, 5.9424e+04, 5.0112e+04, 2.4140e+03, 1.1264e+04,\n",
      "        3.2624e+04, 5.0496e+04, 1.2072e+04, 6.2144e+04, 4.2112e+04, 2.2176e+04,\n",
      "        1.2216e+04, 4.2592e+04, 1.0064e+04,        inf, 6.0512e+04, 1.5608e+04,\n",
      "        3.9616e+04, 2.7808e+04, 1.3784e+04, 4.8000e+04,        inf,        inf,\n",
      "        6.0064e+04, 2.8880e+04,        inf, 1.3904e+04, 5.7504e+04, 3.8592e+04,\n",
      "        4.0448e+04, 9.4400e+03,        inf,        inf, 3.2144e+04,        inf,\n",
      "        3.2406e+01, 3.7632e+04, 6.5280e+03, 5.4784e+04, 5.4520e+03, 1.9792e+04,\n",
      "        2.5360e+04, 1.0408e+04, 1.8384e+04,        inf,        inf, 3.0816e+04,\n",
      "        1.3432e+04,        inf, 4.1856e+04, 1.5790e+03, 3.0592e+04, 6.4544e+04,\n",
      "               inf,        inf, 4.2592e+04,        inf,        inf, 4.8768e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 5.6672e+04,\n",
      "        4.1728e+04, 3.5020e+03, 2.6864e+04, 4.4416e+04, 2.5872e+04,        inf,\n",
      "        2.3040e+04, 1.4616e+04, 6.4288e+04, 2.7536e+04, 2.6752e+04, 5.5808e+04,\n",
      "        1.2704e+04,        inf, 1.7184e+04, 7.4680e+03,        inf, 3.3504e+04,\n",
      "        1.8336e+04,        inf, 5.3216e+04, 6.4672e+04,        inf, 1.2888e+04,\n",
      "        8.2600e+02,        inf, 5.0720e+04, 4.8352e+04,        inf, 4.9088e+04,\n",
      "        1.7200e+04, 8.3440e+03, 1.3064e+04, 3.5360e+04, 6.3808e+04, 6.3200e+04,\n",
      "        4.9792e+04, 4.2784e+04, 4.8040e+03, 3.1488e+04, 5.2064e+04,        inf,\n",
      "        4.5024e+04, 4.2480e+03, 2.2528e+04, 2.4032e+04,        inf, 3.2720e+03,\n",
      "        4.7520e+04, 5.9648e+04,        inf, 3.1472e+04, 5.2128e+04, 3.7504e+04,\n",
      "        8.5280e+03, 8.2080e+03, 3.2192e+04, 3.0000e+04, 4.9664e+04, 2.2096e+04,\n",
      "        9.5440e+03, 3.3760e+03, 2.2528e+04, 3.9584e+04, 4.9664e+04, 1.6960e+04,\n",
      "        2.6592e+04, 8.3600e+03, 5.5136e+04, 4.5248e+04, 1.1072e+04, 3.8336e+04,\n",
      "               inf, 1.2880e+04, 4.3920e+03, 6.4544e+04,        inf, 9.3120e+03,\n",
      "               inf, 9.2000e+03, 3.8528e+04, 3.2048e+04,        inf, 1.4904e+04,\n",
      "        2.2192e+04, 1.6736e+04, 2.3260e+03, 4.1760e+04, 5.8960e+03, 4.1888e+04,\n",
      "               inf, 3.5136e+04,        inf, 2.8832e+04,        inf, 1.2104e+04,\n",
      "               inf, 3.0560e+04,        inf,        inf, 4.4864e+04, 4.1200e+03,\n",
      "        2.2020e+03, 3.5456e+04, 2.0064e+04, 4.0416e+04, 2.2080e+04, 5.1168e+04,\n",
      "        3.7888e+04, 4.4320e+03, 3.0672e+04, 6.3136e+04, 5.0272e+04,        inf,\n",
      "        3.7568e+04, 3.2096e+04, 2.1984e+04,        inf,        inf, 5.5104e+04,\n",
      "               inf, 1.5632e+04, 1.9536e+04, 2.1376e+04, 9.7280e+03, 6.2880e+03,\n",
      "        3.8816e+04,        inf,        inf, 2.6944e+04,        inf, 1.4008e+04,\n",
      "        4.2080e+04, 1.2370e+03, 1.8384e+04, 8.6880e+03, 1.3304e+04,        inf,\n",
      "        5.7632e+04, 1.7936e+04, 4.9504e+04, 3.3344e+04,        inf, 1.8060e+03,\n",
      "        5.1936e+04,        inf,        inf, 4.9728e+04, 3.6224e+04, 2.8368e+04,\n",
      "        2.7568e+04, 1.2656e+04, 2.1408e+04,        inf,        inf, 2.7080e+03,\n",
      "        5.3600e+04, 2.8016e+04,        inf, 3.2480e+03, 1.0616e+04, 3.4208e+04,\n",
      "               inf, 1.9712e+04, 4.3200e+04,        inf, 2.2320e+04,        inf,\n",
      "        4.2464e+04,        inf,        inf,        inf,        inf, 1.3720e+04,\n",
      "               inf, 2.4704e+04, 2.7424e+04,        inf,        inf, 3.5680e+04,\n",
      "        5.8464e+04,        inf, 3.4752e+04, 2.1184e+04, 4.5680e+03, 2.4640e+03,\n",
      "        9.3920e+03, 5.0304e+04, 2.7472e+04, 6.1696e+04, 6.0896e+04, 4.9440e+04,\n",
      "               inf, 2.3328e+04, 1.1488e+04, 2.5504e+04, 5.6320e+03,        inf,\n",
      "        7.4520e+03, 4.7120e+03, 6.3872e+04, 1.6480e+04, 5.7184e+04,        inf,\n",
      "               inf, 3.0704e+04,        inf, 4.9984e+04, 2.2320e+04, 6.1312e+04,\n",
      "               inf, 1.9664e+04, 1.5272e+04, 2.3776e+04,        inf, 1.2712e+04,\n",
      "        3.5520e+04, 4.6144e+04,        inf, 3.0576e+04, 3.4784e+04, 3.6704e+04,\n",
      "        2.4816e+04, 1.6832e+04, 3.0816e+04,        inf, 2.3744e+04, 2.2912e+04,\n",
      "        3.6064e+04, 4.4992e+04, 1.6400e+04, 2.4688e+04, 5.6920e+03, 1.7790e+03,\n",
      "               inf, 4.2432e+04, 6.1024e+04, 4.6048e+04, 2.2440e+03, 6.5152e+04,\n",
      "        4.3225e+02, 3.4048e+04, 2.6352e+04, 2.8112e+04, 4.5440e+04, 5.2928e+04,\n",
      "               inf, 9.3520e+03, 2.3472e+04, 8.4320e+03, 2.5568e+04, 2.0624e+04,\n",
      "        4.4960e+04,        inf, 1.3048e+04, 5.1104e+04, 1.8770e+03, 1.6910e+03,\n",
      "        2.2672e+04, 4.4384e+04, 1.0488e+02,        inf, 2.1472e+04, 3.5200e+04,\n",
      "        1.6528e+04, 2.4656e+04,        inf, 4.2464e+04, 2.7152e+04, 3.2544e+04,\n",
      "        3.0096e+04, 4.4576e+04, 1.8496e+04,        inf,        inf, 1.7856e+04,\n",
      "        2.8272e+04, 4.7520e+04, 3.3088e+04, 5.2992e+04, 1.6480e+04, 2.1056e+04,\n",
      "        5.6080e+03, 2.4448e+04, 1.3056e+04,        inf, 4.7920e+03, 4.7616e+04,\n",
      "        6.4992e+04,        inf], device='cuda:0', dtype=torch.float16), tensor([6.9680e+03, 1.9824e+04, 1.9840e+03, 2.9136e+04, 1.3416e+04, 1.1224e+04,\n",
      "        8.7360e+03,        inf,        inf, 2.3260e+03, 8.1360e+03, 1.1200e+04,\n",
      "        1.9712e+04, 9.1520e+03, 1.4024e+04, 6.6720e+03, 6.4520e+03, 5.7376e+04,\n",
      "               inf, 2.5216e+04, 3.5550e+02, 4.7360e+04, 1.2808e+04, 4.7400e+02,\n",
      "        8.4950e+02, 4.5600e+03, 7.3600e+03, 1.4048e+04, 4.3232e+04, 6.0320e+04,\n",
      "        2.4048e+04, 3.5680e+04, 2.9472e+04, 1.9680e+04, 2.8224e+04, 2.2880e+04,\n",
      "        8.1440e+03, 3.0944e+04, 4.4280e+03, 1.3904e+04, 3.5872e+04, 3.8260e+03,\n",
      "        3.2720e+04, 1.1216e+04, 1.8660e+03, 4.2944e+04, 2.5824e+04, 2.2560e+04,\n",
      "        2.5072e+04, 2.3248e+04,        inf, 6.8640e+03, 3.2864e+04, 3.3600e+03,\n",
      "        1.5270e+03, 1.2056e+04, 4.7560e+03, 4.1760e+03, 1.1520e+04, 4.7808e+04,\n",
      "        2.5184e+04, 1.3560e+04, 3.1420e+03, 2.2240e+04, 8.8960e+03, 5.2560e+03,\n",
      "               inf, 5.2040e+03, 9.9680e+03, 2.1856e+04, 2.5040e+03, 1.7090e+03,\n",
      "        7.8360e+03, 6.6360e+03, 2.8176e+04, 2.7700e+03, 2.6144e+04, 4.3776e+04,\n",
      "        1.4500e+03, 1.0064e+04, 1.8250e+03, 8.3050e+02, 3.2992e+04, 5.6480e+03,\n",
      "        3.0368e+04, 1.6624e+04, 4.7936e+04, 5.5960e+03, 1.4752e+04, 1.2800e+04,\n",
      "        7.8080e+03,        inf, 1.9390e+03, 2.0900e+03, 2.4620e+03, 6.1856e+04,\n",
      "        7.1880e+03, 1.4112e+02, 3.5360e+03, 2.1740e+03, 1.7536e+04, 1.0296e+04,\n",
      "        1.7488e+04, 2.0576e+04, 5.8720e+04, 7.5240e+03, 1.6576e+04, 5.8480e+03,\n",
      "               inf, 1.2608e+04, 9.5520e+03, 4.2720e+04, 2.4608e+04, 1.3664e+04,\n",
      "        9.0320e+03, 1.5100e+03, 3.7200e+03, 1.5104e+04, 4.6640e+03, 2.3720e+03,\n",
      "        3.4080e+04, 1.0035e+03, 4.6920e+03,        inf, 1.2040e+04, 1.0848e+04,\n",
      "        3.3184e+04, 4.8680e+03, 6.2040e+03, 1.1368e+04, 1.9090e+03, 7.7640e+03,\n",
      "        7.5720e+03, 6.7880e+03, 7.6240e+03, 1.3328e+04, 1.8496e+04, 2.3080e+03,\n",
      "        6.5000e+02, 2.0190e+03, 6.4120e+03, 2.6592e+04, 1.4620e+03, 1.3568e+04,\n",
      "        1.6980e+03, 3.0560e+04, 6.2400e+03, 2.7540e+03, 2.6060e+03, 4.7008e+04,\n",
      "        1.1328e+04, 9.4800e+03, 3.5584e+04, 3.7568e+04, 1.0680e+03, 1.5480e+04,\n",
      "        1.1680e+04,        inf, 1.1376e+04, 6.9800e+03, 4.3950e+02, 3.1760e+03,\n",
      "        4.3488e+04, 4.4960e+04, 2.0480e+03, 1.2280e+04, 5.0400e+04, 8.9438e+01,\n",
      "        5.6700e+02, 6.0680e+03, 9.4080e+03,        inf, 1.8544e+04, 6.7800e+03,\n",
      "        8.5600e+03, 4.0768e+04, 6.5800e+03, 1.5560e+04,        inf, 3.5040e+04,\n",
      "        4.8704e+04, 1.6512e+04,        inf, 6.9400e+03, 3.1072e+04, 1.1472e+04,\n",
      "        1.8560e+04, 4.0780e+03, 2.2608e+04, 6.0480e+04, 1.0056e+04, 1.3000e+04,\n",
      "        2.7500e+03, 2.7488e+04, 7.8200e+03, 7.0720e+03, 8.8560e+03, 9.3200e+03,\n",
      "        2.6224e+04, 2.4600e+03, 2.0736e+04, 1.1744e+04, 5.6672e+04, 3.5700e+03,\n",
      "        2.2560e+03, 2.7020e+03, 1.4544e+04, 4.7560e+03, 3.9840e+04, 3.6864e+04,\n",
      "        3.2800e+03, 4.0288e+04, 3.6480e+04, 3.6850e+02, 1.9376e+04, 2.8768e+04,\n",
      "               inf, 5.3760e+04, 3.2592e+04, 1.8352e+04, 4.5792e+04, 3.3856e+04,\n",
      "        1.0048e+04, 6.7440e+03, 1.8032e+04, 1.0272e+04, 1.8512e+04, 2.6912e+04,\n",
      "        1.9760e+04, 1.3800e+04, 4.5152e+04, 1.8448e+04, 5.8750e+01, 1.3512e+04,\n",
      "        5.8120e+03,        inf, 2.7180e+03, 4.6760e+03, 4.5088e+04, 1.1680e+04,\n",
      "        5.8560e+03, 2.2224e+04, 9.8560e+03, 1.9248e+04, 4.8096e+04, 9.8320e+03,\n",
      "        3.6980e+03,        inf, 1.8120e+03, 5.4080e+04,        inf, 2.8920e+03,\n",
      "        2.2340e+03, 3.8980e+03, 2.6620e+03, 1.4504e+04, 1.5520e+03, 1.1568e+04,\n",
      "        2.0384e+04, 1.8780e+03, 2.6740e+03, 2.7712e+04, 1.9776e+04,        inf,\n",
      "        1.4816e+04, 7.5280e+03, 8.6000e+03, 8.7760e+03,        inf, 2.3800e+03,\n",
      "        2.4128e+04, 2.8560e+04,        inf, 2.8352e+04, 9.5520e+03, 5.8816e+04,\n",
      "        1.3088e+04, 2.9425e+02, 3.1536e+04, 1.7056e+04, 4.9856e+04, 6.6960e+03,\n",
      "        8.4960e+03, 2.9940e+03, 1.1430e+03, 2.0448e+04, 3.1632e+04, 1.0888e+04,\n",
      "        1.0608e+04, 1.1330e+03, 7.5240e+03,        inf, 3.0680e+03, 1.5060e+03,\n",
      "        5.5584e+04, 1.0530e+03, 1.0568e+04,        inf, 2.6624e+04, 3.1760e+03,\n",
      "        2.3344e+04, 5.5400e+03, 4.9560e+03, 2.3552e+04, 2.7008e+04, 9.3840e+03,\n",
      "        1.1600e+04, 1.5904e+04, 5.5520e+03, 3.5456e+04, 1.2968e+04, 8.3200e+03,\n",
      "        4.8768e+04, 1.3416e+04, 3.4976e+04, 8.5360e+03, 5.1296e+04, 2.4304e+04,\n",
      "        8.0160e+03, 1.9344e+04, 3.3696e+04, 2.4896e+04, 1.6560e+04, 2.0680e+03,\n",
      "        9.7760e+03, 3.3440e+04, 3.6768e+04, 5.0624e+04, 3.6576e+04, 4.1824e+04,\n",
      "        7.8880e+03, 5.2560e+03, 1.5704e+04, 3.1280e+04, 7.6040e+03,        inf,\n",
      "        5.7696e+04, 1.9776e+04, 1.1232e+04, 1.4248e+04,        inf, 5.6992e+04,\n",
      "        3.5936e+04, 3.0928e+04, 2.7072e+04, 1.2920e+04, 1.4760e+04, 3.5700e+03,\n",
      "        5.7840e+03, 1.5424e+04, 1.9664e+04, 1.0104e+04,        inf, 5.3480e+03,\n",
      "        1.1928e+04, 3.9880e+03, 1.0680e+03, 7.7120e+03, 5.9400e+03, 6.1520e+03,\n",
      "        5.0880e+04, 8.0280e+03, 3.8528e+04, 5.8800e+03,        inf, 1.2410e+03,\n",
      "        3.3260e+03, 4.4288e+04, 2.4000e+04, 4.2272e+04, 1.7712e+04, 2.6720e+03,\n",
      "        2.9696e+04, 2.1312e+04, 1.0264e+04, 3.7824e+04, 2.7104e+04, 1.7190e+03,\n",
      "        3.7728e+04, 2.8336e+04,        inf, 5.7480e+03, 9.1200e+03, 5.7520e+03,\n",
      "        4.7264e+04, 1.8896e+04, 3.1120e+04, 4.0544e+04, 1.4336e+04, 4.2080e+04,\n",
      "        4.9184e+04, 1.7744e+04,        inf, 6.5088e+04, 4.8960e+04, 8.9280e+03,\n",
      "               inf, 1.9770e+03, 2.3664e+04, 3.6512e+04, 5.5264e+04, 3.4975e+02,\n",
      "        3.5232e+04, 3.4240e+04, 3.2752e+04, 1.1208e+04, 9.5920e+03, 5.1240e+03,\n",
      "        1.2456e+04, 3.3760e+04, 2.3792e+04, 2.8800e+04, 3.9584e+04, 2.8928e+04,\n",
      "        5.2480e+04, 1.7424e+04, 1.0440e+04, 2.3456e+04, 7.4360e+03, 2.8440e+03,\n",
      "        9.4800e+03, 8.2800e+03,        inf, 2.2064e+04,        inf,        inf,\n",
      "               inf, 2.9008e+04, 4.5568e+04, 4.7424e+04, 2.2304e+04, 5.6928e+04,\n",
      "               inf, 6.4360e+03, 1.8400e+04, 1.8128e+04,        inf, 1.3730e+03,\n",
      "        3.5500e+03, 2.0080e+04,        inf, 1.9232e+04, 1.4672e+04, 5.1904e+04,\n",
      "        2.5008e+04, 1.7200e+04, 3.2576e+04, 8.9520e+03, 1.2944e+04, 1.3856e+04,\n",
      "        3.5328e+04, 3.0192e+04, 1.0419e+02, 1.5128e+04, 7.3640e+03, 4.7040e+03,\n",
      "               inf, 3.6096e+04, 4.2400e+03, 3.9392e+04, 5.0560e+03, 1.3408e+04,\n",
      "        2.5156e+00, 4.6496e+04, 1.1624e+04, 8.6560e+03, 5.4016e+04, 4.0256e+04,\n",
      "               inf, 2.4620e+03, 2.0096e+04, 6.8750e+02, 1.1792e+04, 1.3720e+04,\n",
      "        3.1344e+04,        inf, 2.7780e+03, 3.9072e+04, 4.9875e+02, 8.2800e+03,\n",
      "        8.4640e+03, 7.9120e+03, 1.4010e+03, 2.1568e+04, 9.4400e+03, 4.1920e+04,\n",
      "        1.5872e+04, 2.7872e+04, 4.3296e+04, 1.6384e+04, 8.0120e+03, 4.0096e+04,\n",
      "        1.2520e+04, 1.5312e+04, 2.1296e+04, 5.8240e+04, 3.2240e+04, 5.1000e+03,\n",
      "        1.9712e+04,        inf, 3.0080e+04, 5.8080e+04, 1.9370e+03, 2.7200e+04,\n",
      "        2.7280e+03, 3.5008e+04, 7.2880e+03, 4.3360e+03, 2.4980e+03, 8.3120e+03,\n",
      "        6.5408e+04, 5.8208e+04], device='cuda:0', dtype=torch.float16), tensor([20176.0000, 32576.0000, 29344.0000,        inf,        inf, 49280.0000,\n",
      "        32448.0000, 56448.0000,        inf, 31952.0000, 30704.0000,        inf,\n",
      "        12232.0000,        inf,        inf, 12912.0000,  1371.0000, 49632.0000,\n",
      "               inf, 58016.0000,  7680.0000,        inf, 49216.0000, 12872.0000,\n",
      "        34752.0000,  2580.0000,  5288.0000, 18992.0000, 42240.0000, 52448.0000,\n",
      "        33024.0000, 32928.0000, 56704.0000,  8856.0000, 31488.0000, 56544.0000,\n",
      "        47584.0000, 27616.0000,  7608.0000, 25936.0000,        inf, 12712.0000,\n",
      "               inf,  8872.0000, 48640.0000, 58080.0000, 45824.0000, 36480.0000,\n",
      "        49856.0000, 39488.0000,        inf, 17008.0000, 26112.0000,  6236.0000,\n",
      "        31888.0000,        inf,  4440.0000,        inf, 24112.0000,        inf,\n",
      "        54016.0000, 25312.0000, 10784.0000,        inf,  3940.0000, 43040.0000,\n",
      "               inf, 52736.0000, 26576.0000, 31984.0000, 34656.0000, 63936.0000,\n",
      "         4960.0000,  7696.0000,        inf, 14768.0000,        inf,        inf,\n",
      "        15832.0000, 24208.0000, 23312.0000,        inf,        inf, 20272.0000,\n",
      "               inf,        inf,        inf,  5100.0000, 23568.0000, 35008.0000,\n",
      "        35936.0000,        inf,        inf, 13304.0000, 16168.0000,        inf,\n",
      "         9328.0000, 15296.0000,        inf,  2188.0000, 35264.0000,  6360.0000,\n",
      "        31152.0000,        inf,        inf,  2094.0000, 56704.0000, 22752.0000,\n",
      "               inf,  6768.0000,  6292.0000,        inf, 57376.0000, 38752.0000,\n",
      "        37952.0000, 24976.0000, 10616.0000,        inf, 49984.0000, 41440.0000,\n",
      "               inf,  6524.0000, 25776.0000,        inf,  8864.0000,        inf,\n",
      "               inf,  5660.0000, 26624.0000,        inf, 62560.0000,        inf,\n",
      "         4276.0000, 59776.0000,        inf, 34464.0000, 37600.0000, 20320.0000,\n",
      "          713.5000,        inf, 20432.0000,        inf, 15328.0000, 33312.0000,\n",
      "        49408.0000,        inf, 45632.0000,  6456.0000,        inf, 61536.0000,\n",
      "        59872.0000,  1221.0000,        inf,        inf, 13056.0000,        inf,\n",
      "        26752.0000,        inf, 53312.0000, 53504.0000,   432.0000, 14680.0000,\n",
      "        32960.0000, 54432.0000, 16736.0000, 60736.0000, 46240.0000, 18272.0000,\n",
      "         8336.0000, 40800.0000, 13320.0000,        inf, 64224.0000,  9944.0000,\n",
      "        44160.0000, 30336.0000,  9160.0000, 42144.0000,        inf,        inf,\n",
      "        56576.0000, 33056.0000,        inf, 16312.0000, 60000.0000, 39840.0000,\n",
      "        37504.0000,  6828.0000,        inf,        inf, 37920.0000,        inf,\n",
      "          539.0000, 34176.0000,  3582.0000, 52512.0000,  7664.0000, 21264.0000,\n",
      "        25712.0000, 13096.0000, 15336.0000,        inf,        inf, 32896.0000,\n",
      "        15280.0000,        inf, 38496.0000,  1251.0000, 28592.0000,        inf,\n",
      "               inf,        inf, 41376.0000,        inf,        inf, 50656.0000,\n",
      "               inf,        inf,        inf,        inf,        inf, 59584.0000,\n",
      "        45824.0000,   536.0000, 30112.0000, 48608.0000, 29744.0000, 62400.0000,\n",
      "        19648.0000, 18304.0000,        inf, 22768.0000, 28720.0000, 49952.0000,\n",
      "        11424.0000,        inf, 22304.0000,  5444.0000,        inf, 30176.0000,\n",
      "        21648.0000,        inf, 54720.0000, 62304.0000,        inf, 12816.0000,\n",
      "         4772.0000,        inf, 55424.0000, 45824.0000,        inf, 50144.0000,\n",
      "        13000.0000,  4720.0000,  9488.0000, 39264.0000,        inf, 58272.0000,\n",
      "        48096.0000, 39328.0000,  9144.0000, 32592.0000, 47808.0000,        inf,\n",
      "        47136.0000,   865.0000, 20896.0000, 28928.0000,        inf,   184.1250,\n",
      "        42464.0000, 55136.0000,        inf, 27488.0000, 55552.0000, 39648.0000,\n",
      "         7140.0000,  5812.0000, 32768.0000, 32640.0000, 46976.0000, 26032.0000,\n",
      "        13544.0000,  1981.0000, 20176.0000, 40480.0000, 50112.0000, 17904.0000,\n",
      "        29456.0000,  4024.0000, 51648.0000, 45568.0000,  9920.0000, 36384.0000,\n",
      "               inf, 17408.0000,  9680.0000, 63744.0000,        inf,  6140.0000,\n",
      "               inf, 13656.0000, 44640.0000, 34304.0000,        inf, 16400.0000,\n",
      "        20144.0000, 14152.0000,   924.5000, 39456.0000,  9560.0000, 43136.0000,\n",
      "               inf, 31584.0000,        inf, 25744.0000,        inf,  8928.0000,\n",
      "               inf, 31728.0000,        inf,        inf, 46336.0000,   431.2500,\n",
      "         5924.0000, 33600.0000, 23920.0000, 36288.0000, 25232.0000, 49472.0000,\n",
      "        35904.0000,  1363.0000, 32864.0000, 64992.0000, 52128.0000,        inf,\n",
      "        43904.0000, 34816.0000, 27104.0000,        inf,        inf, 51872.0000,\n",
      "               inf, 15888.0000, 24448.0000, 25344.0000,  7256.0000, 10840.0000,\n",
      "        40640.0000,        inf,        inf, 31376.0000,        inf,  9328.0000,\n",
      "        37760.0000,  2222.0000, 15096.0000, 11944.0000, 19392.0000,        inf,\n",
      "        60864.0000, 20272.0000, 52288.0000, 36896.0000,        inf,  5632.0000,\n",
      "        49088.0000,        inf,        inf, 54304.0000, 31264.0000, 30592.0000,\n",
      "        32960.0000, 15472.0000, 17376.0000,        inf,        inf,   373.5000,\n",
      "        51520.0000, 31024.0000,        inf,  6716.0000, 14744.0000, 35904.0000,\n",
      "               inf, 16752.0000, 40512.0000,        inf, 18320.0000,        inf,\n",
      "        39360.0000,        inf,        inf,        inf,        inf, 16112.0000,\n",
      "               inf, 27072.0000, 24112.0000,        inf,        inf, 36384.0000,\n",
      "        53696.0000,        inf, 38912.0000, 25824.0000,  3868.0000,  7440.0000,\n",
      "        13184.0000, 46624.0000, 26560.0000,        inf,        inf, 54752.0000,\n",
      "               inf, 19744.0000,  8004.0000, 21856.0000,  9696.0000,        inf,\n",
      "        12480.0000,  9872.0000,        inf, 19840.0000, 55424.0000,        inf,\n",
      "               inf, 25456.0000,        inf, 54400.0000, 25760.0000, 56736.0000,\n",
      "               inf, 23872.0000, 11792.0000, 20304.0000,        inf, 15480.0000,\n",
      "        38272.0000, 40800.0000,        inf, 35488.0000, 30240.0000, 42592.0000,\n",
      "        29264.0000,  8168.0000, 34112.0000,        inf, 19296.0000, 18640.0000,\n",
      "        37952.0000, 49664.0000, 13344.0000, 20048.0000, 10152.0000,  1562.0000,\n",
      "               inf, 38912.0000,        inf, 42144.0000,  5932.0000,        inf,\n",
      "         4424.0000, 40896.0000, 23808.0000, 25344.0000, 48384.0000, 49120.0000,\n",
      "               inf,  5216.0000, 16016.0000,  3822.0000, 29488.0000, 16464.0000,\n",
      "        48512.0000,        inf,  9344.0000, 45568.0000,  1687.0000,  4872.0000,\n",
      "        28928.0000, 37856.0000,  2984.0000, 65088.0000, 22640.0000, 30480.0000,\n",
      "        19616.0000, 29696.0000,        inf, 43712.0000, 30224.0000, 37088.0000,\n",
      "        26752.0000, 48096.0000, 23984.0000,        inf,        inf, 11296.0000,\n",
      "        30976.0000, 52480.0000, 34528.0000, 56352.0000, 11064.0000, 24864.0000,\n",
      "         9848.0000, 27920.0000, 14528.0000,        inf,   801.0000, 49632.0000,\n",
      "        62528.0000,        inf], device='cuda:0', dtype=torch.float16), tensor([[1.7310e+03, 7.1700e+02, 2.9650e+02,  ..., 4.1120e+03, 4.9040e+03,\n",
      "         3.1300e+03],\n",
      "        [1.8760e+03, 1.4838e+02, 6.0391e+00,  ..., 3.1660e+03, 4.2560e+03,\n",
      "         1.9320e+03],\n",
      "        [8.2800e+02, 7.6500e+02, 3.6575e+02,  ..., 1.5840e+03, 1.0280e+03,\n",
      "         1.1790e+03],\n",
      "        ...,\n",
      "        [1.4872e+04, 3.3344e+04, 5.4440e+03,  ...,        inf,        inf,\n",
      "                inf],\n",
      "        [4.3800e+03, 3.5640e+03, 1.3160e+04,  ...,        inf,        inf,\n",
      "                inf],\n",
      "        [2.6896e+04, 2.4528e+04, 1.9696e+04,  ...,        inf,        inf,\n",
      "                inf]], device='cuda:0', dtype=torch.float16), tensor([4188., 3144., 1300.,  ...,   inf,   inf,   inf], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[ 3056., 16352.,  6468.,  ...,    inf,  1792., 37888.],\n",
      "        [46656., 23616., 19296.,  ..., 21248.,    inf,  1480.],\n",
      "        [47424., 18288.,  2200.,  ..., 47680.,    inf, 17632.],\n",
      "        ...,\n",
      "        [   inf, 57408.,  3732.,  ...,    inf,    inf,  9600.],\n",
      "        [52672., 32224., 15432.,  ..., 54528.,    inf, 62176.],\n",
      "        [   inf,    inf, 17264.,  ...,    inf,    inf, 30480.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([       inf, 59520.0000,  3914.0000,        inf, 24064.0000, 39072.0000,\n",
      "        62272.0000, 38016.0000, 38336.0000,        inf, 39744.0000,        inf,\n",
      "        49984.0000,        inf, 57600.0000, 59296.0000,        inf, 19840.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 14616.0000,\n",
      "               inf, 42144.0000,        inf,        inf, 45408.0000,        inf,\n",
      "               inf,        inf,        inf, 56608.0000,        inf,        inf,\n",
      "        41792.0000,        inf,        inf,  2156.0000,        inf,        inf,\n",
      "               inf,        inf, 25552.0000,        inf,        inf,        inf,\n",
      "        20736.0000,        inf,        inf, 38496.0000,        inf,        inf,\n",
      "               inf, 33344.0000,        inf,  1874.0000, 29616.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,  6388.0000,\n",
      "               inf,        inf,        inf,        inf, 22256.0000,        inf,\n",
      "               inf, 58688.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 65408.0000,        inf,        inf,        inf,   630.0000,\n",
      "               inf,        inf,        inf,        inf,  1623.0000,        inf,\n",
      "               inf,        inf, 44832.0000, 13440.0000,        inf,        inf,\n",
      "               inf,  8776.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 31296.0000,        inf, 33504.0000,        inf,        inf,\n",
      "        64320.0000, 58720.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 46112.0000, 64640.0000,        inf,        inf,\n",
      "         3006.0000, 30160.0000, 12560.0000,        inf, 54816.0000,        inf,\n",
      "               inf,        inf,  7612.0000,        inf,        inf,        inf,\n",
      "        13976.0000,        inf,        inf,  6012.0000,        inf, 53824.0000,\n",
      "               inf, 15768.0000,        inf,        inf, 40992.0000, 41376.0000,\n",
      "        26528.0000, 12448.0000,        inf,        inf,        inf,        inf,\n",
      "        48416.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,  7528.0000, 32336.0000,  9640.0000,        inf,\n",
      "               inf,        inf,  7448.0000,        inf,        inf,        inf,\n",
      "               inf,        inf, 19392.0000,        inf, 36544.0000,        inf,\n",
      "               inf, 10128.0000,  4932.0000,        inf,        inf,        inf,\n",
      "        15424.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 35872.0000, 15256.0000,        inf,\n",
      "               inf, 60640.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 45888.0000, 31744.0000,        inf,        inf, 50496.0000,\n",
      "        35200.0000,        inf, 50944.0000,        inf, 60768.0000,        inf,\n",
      "               inf,        inf,        inf, 42720.0000,        inf, 51744.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "        13912.0000, 36736.0000,        inf, 48704.0000, 23984.0000, 25904.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 13864.0000,  4676.0000, 13688.0000,\n",
      "               inf,        inf, 37984.0000,        inf,        inf, 16288.0000,\n",
      "               inf,        inf,  7780.0000, 10040.0000,        inf,        inf,\n",
      "         6184.0000,        inf,        inf, 32304.0000,        inf,        inf,\n",
      "               inf, 46656.0000,        inf, 28896.0000, 61184.0000, 53536.0000,\n",
      "        16104.0000, 45216.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,  8100.0000, 14352.0000, 40192.0000,\n",
      "               inf,        inf,        inf,        inf, 30736.0000,  5172.0000,\n",
      "        18112.0000,        inf,        inf, 24640.0000, 18496.0000,        inf,\n",
      "        62560.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        47584.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        48928.0000, 45280.0000,        inf, 41440.0000,        inf, 19008.0000,\n",
      "               inf,        inf, 18816.0000,        inf, 24096.0000, 20272.0000,\n",
      "               inf, 20704.0000, 18512.0000,        inf, 10584.0000,        inf,\n",
      "               inf,        inf, 30176.0000,  4040.0000,        inf,        inf,\n",
      "        16312.0000, 36416.0000, 10136.0000,        inf,  4136.0000,        inf,\n",
      "               inf,        inf, 18976.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "        44192.0000, 52608.0000, 30656.0000,        inf, 33504.0000,        inf,\n",
      "               inf, 58688.0000, 31552.0000,        inf,        inf, 26608.0000,\n",
      "               inf,        inf,        inf,        inf,        inf, 42464.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,  5856.0000,        inf,        inf,\n",
      "        45312.0000,        inf,        inf,        inf, 17616.0000, 48896.0000,\n",
      "        13016.0000,        inf,        inf, 54816.0000,   238.3750, 16576.0000,\n",
      "               inf, 31520.0000,  9144.0000, 60608.0000,        inf, 19008.0000,\n",
      "        44640.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        41696.0000, 46528.0000,        inf, 56288.0000,        inf, 56416.0000,\n",
      "               inf,        inf,        inf,        inf,        inf, 25760.0000,\n",
      "               inf, 31008.0000, 31664.0000, 25872.0000, 51744.0000, 18304.0000,\n",
      "        35520.0000,        inf,        inf, 18704.0000,  9528.0000,        inf,\n",
      "        16784.0000,        inf,        inf,        inf,        inf, 37120.0000,\n",
      "        37728.0000,  1395.0000,        inf,        inf,        inf, 29168.0000,\n",
      "               inf, 35328.0000,        inf,        inf, 47456.0000,        inf,\n",
      "               inf, 36448.0000,        inf,        inf, 37344.0000,        inf,\n",
      "               inf, 26704.0000,        inf,        inf, 58112.0000,        inf,\n",
      "               inf,        inf,        inf, 47488.0000,        inf, 22176.0000,\n",
      "        37312.0000,        inf,        inf,        inf,        inf, 55712.0000,\n",
      "        64832.0000,  8904.0000,        inf,        inf, 45600.0000,        inf,\n",
      "        27488.0000, 59040.0000,        inf,        inf,        inf,        inf,\n",
      "        10952.0000, 45792.0000, 11040.0000,        inf, 38816.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 49920.0000,\n",
      "               inf,        inf,        inf, 29392.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 32160.0000, 48512.0000,        inf,\n",
      "          158.3750,        inf,        inf,        inf, 27920.0000,        inf,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([[ 3000.,  1622.,  1986.,  ...,  2660.,  9504.,  1514.],\n",
      "        [ 6112.,  2702.,  2732.,  ...,  7100.,  5272.,  5252.],\n",
      "        [ 3172.,  2134.,  1500.,  ...,   639.,  6376.,   317.],\n",
      "        ...,\n",
      "        [40960.,  4288.,  6800.,  ..., 41472.,    inf, 23584.],\n",
      "        [ 5248., 16576.,  1286.,  ...,  2988., 19408., 10752.],\n",
      "        [ 8480., 13600.,  7432.,  ...,  9160.,  5284.,  1024.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([ 6200.,  3738.,  4860.,  ...,    inf, 11576.,  5976.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[54432., 48288.,  2534.,  ..., 35040.,  8784.,    inf],\n",
      "        [38464., 24544.,  7928.,  ...,  4924., 10256., 21056.],\n",
      "        [52928., 43520.,  2744.,  ..., 15648., 10880., 51616.],\n",
      "        ...,\n",
      "        [   inf,    inf, 12232.,  ...,  6624., 26752.,    inf],\n",
      "        [ 6864.,  1312.,  9776.,  ..., 50528., 16056.,    inf],\n",
      "        [   inf,    inf, 23104.,  ..., 38272., 31872.,    inf]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([3.4208e+04, 3.0048e+04, 6.8320e+03,        inf, 2.2528e+04, 2.8384e+04,\n",
      "        3.3280e+04, 2.7904e+04, 3.4208e+04,        inf, 2.5552e+04,        inf,\n",
      "        3.8208e+04,        inf, 4.0288e+04, 5.2416e+04,        inf, 1.5336e+04,\n",
      "               inf,        inf, 4.3680e+04,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 9.8960e+03,\n",
      "               inf, 2.3008e+04,        inf,        inf, 4.3968e+04,        inf,\n",
      "               inf,        inf,        inf, 5.3440e+04,        inf,        inf,\n",
      "        4.2656e+04,        inf,        inf, 9.0560e+03,        inf, 5.9072e+04,\n",
      "               inf, 5.2032e+04, 2.9248e+04,        inf,        inf,        inf,\n",
      "        1.5936e+04,        inf,        inf, 4.1504e+04,        inf, 6.1248e+04,\n",
      "               inf, 2.5728e+04,        inf, 5.7720e+03, 1.7232e+04,        inf,\n",
      "               inf,        inf, 3.3824e+04,        inf,        inf, 4.1525e+02,\n",
      "        5.6640e+04,        inf,        inf,        inf, 8.7550e+02,        inf,\n",
      "               inf, 4.8672e+04,        inf,        inf, 5.3728e+04,        inf,\n",
      "               inf, 4.7552e+04,        inf,        inf,        inf, 1.4496e+04,\n",
      "               inf,        inf,        inf, 5.8272e+04, 1.2464e+04,        inf,\n",
      "               inf,        inf, 4.0640e+04, 1.3696e+04,        inf,        inf,\n",
      "        6.5312e+04, 1.8281e+01,        inf,        inf, 4.2784e+04, 6.1024e+04,\n",
      "               inf, 3.5744e+04,        inf, 2.4496e+04,        inf,        inf,\n",
      "        4.1312e+04, 3.7600e+04,        inf,        inf,        inf,        inf,\n",
      "        5.5904e+04, 4.0800e+04, 2.7968e+04, 5.0880e+04,        inf,        inf,\n",
      "        1.6970e+03, 2.2640e+04, 2.2100e+03,        inf, 1.7264e+04,        inf,\n",
      "               inf,        inf, 5.1400e+03,        inf,        inf, 4.9376e+04,\n",
      "        7.2640e+03,        inf,        inf, 1.7568e+04,        inf, 2.6784e+04,\n",
      "               inf, 8.2080e+03, 4.7712e+04, 5.3472e+04, 3.2416e+04, 3.3824e+04,\n",
      "        2.4940e+03, 1.3776e+04,        inf,        inf, 5.0432e+04,        inf,\n",
      "        2.7008e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 1.0280e+04, 1.4776e+04, 2.2688e+04,        inf,\n",
      "               inf,        inf, 3.3480e+03,        inf,        inf,        inf,\n",
      "        3.9168e+04,        inf, 1.3944e+04,        inf, 1.6152e+04,        inf,\n",
      "               inf, 1.5952e+04, 1.1270e+03,        inf,        inf,        inf,\n",
      "        2.8280e+03,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 6.1056e+04, 2.5456e+04, 2.1600e+04,        inf,\n",
      "               inf, 3.2496e+04,        inf,        inf,        inf,        inf,\n",
      "               inf, 2.1408e+04, 1.5072e+04,        inf,        inf, 2.6752e+04,\n",
      "        1.9440e+04,        inf, 3.3728e+04,        inf, 4.6560e+04,        inf,\n",
      "               inf,        inf,        inf, 3.8720e+04,        inf, 4.6912e+04,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "        2.4816e+04, 4.2624e+04,        inf, 4.2400e+04, 1.6160e+04, 9.6160e+03,\n",
      "               inf, 5.0784e+04, 6.4768e+04, 6.2240e+04,        inf,        inf,\n",
      "               inf,        inf,        inf, 4.5560e+03, 4.3960e+03, 1.9440e+04,\n",
      "               inf,        inf, 1.9584e+04,        inf, 5.7984e+04, 4.5000e+03,\n",
      "               inf,        inf, 7.8240e+03, 2.5540e+03, 4.3360e+04,        inf,\n",
      "        2.2060e+03,        inf,        inf, 4.4192e+04,        inf,        inf,\n",
      "               inf, 3.2192e+04, 4.9248e+04, 2.0608e+04, 6.0288e+04, 3.4560e+04,\n",
      "        1.2136e+04, 4.3264e+04,        inf, 5.8464e+04,        inf,        inf,\n",
      "        5.5360e+04,        inf,        inf, 8.7920e+03, 2.2064e+04, 2.7408e+04,\n",
      "               inf, 5.6128e+04,        inf,        inf, 2.3472e+04, 8.6240e+03,\n",
      "        1.1480e+04,        inf, 5.2032e+04, 3.5040e+04, 2.3184e+04,        inf,\n",
      "        5.5744e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "        5.1136e+04,        inf, 6.4032e+04,        inf,        inf,        inf,\n",
      "        3.3056e+04, 3.4528e+04, 6.1408e+04, 2.6144e+04,        inf, 1.8432e+04,\n",
      "               inf,        inf, 1.3296e+04, 6.4672e+04, 1.6384e+04, 1.3912e+04,\n",
      "               inf, 1.6768e+04, 6.9300e+02,        inf, 4.7640e+03,        inf,\n",
      "               inf,        inf, 2.0256e+04, 1.1270e+03,        inf,        inf,\n",
      "        9.0720e+03, 3.5392e+04, 2.4540e+03,        inf, 1.7216e+04,        inf,\n",
      "        4.8544e+04,        inf, 1.5712e+04,        inf,        inf,        inf,\n",
      "               inf, 5.1168e+04,        inf,        inf,        inf,        inf,\n",
      "        4.6240e+04, 3.4208e+04, 5.5480e+03,        inf, 1.6048e+04,        inf,\n",
      "               inf, 4.0416e+04, 1.4872e+04,        inf, 6.1888e+04, 2.2500e+03,\n",
      "        5.0624e+04,        inf,        inf, 5.8944e+04, 4.7872e+04, 2.8208e+04,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 1.7632e+04,        inf,        inf,\n",
      "        4.3072e+04,        inf,        inf,        inf, 6.4440e+03, 4.4416e+04,\n",
      "        9.0240e+03,        inf,        inf, 2.9568e+04, 7.9640e+03, 2.9008e+04,\n",
      "               inf, 4.4064e+04, 1.9200e+03, 3.5680e+04,        inf, 1.6912e+04,\n",
      "        2.6896e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "        3.9872e+04, 3.0768e+04,        inf, 3.6320e+04,        inf, 3.5072e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 2.6624e+04,\n",
      "               inf, 3.3540e+03, 1.3272e+04, 3.1616e+04, 4.5056e+04, 4.7200e+03,\n",
      "        4.0000e+04,        inf,        inf, 2.9440e+04, 2.4660e+03,        inf,\n",
      "        1.3024e+04,        inf,        inf, 5.5040e+04, 6.0288e+04, 3.7216e+04,\n",
      "        2.4528e+04, 2.6340e+03,        inf, 6.1888e+04,        inf, 3.9520e+04,\n",
      "               inf, 2.6880e+04,        inf,        inf, 4.0064e+04,        inf,\n",
      "               inf, 2.9200e+04,        inf,        inf, 3.7280e+03, 4.1920e+04,\n",
      "               inf, 1.1224e+04,        inf,        inf, 4.0672e+04, 4.4096e+04,\n",
      "        5.9680e+04,        inf,        inf, 1.5192e+04,        inf, 1.4440e+04,\n",
      "        2.0784e+04,        inf,        inf,        inf, 5.0464e+04, 4.6496e+04,\n",
      "        6.3904e+04, 1.5136e+04,        inf,        inf, 6.3424e+04,        inf,\n",
      "        3.2336e+04, 2.6080e+04,        inf, 5.6544e+04,        inf, 6.0000e+04,\n",
      "        2.1808e+04, 4.5728e+04, 1.6040e+03,        inf, 3.2304e+04,        inf,\n",
      "        5.1008e+04, 4.1760e+04,        inf,        inf,        inf, 4.2560e+04,\n",
      "               inf,        inf,        inf, 8.4240e+03,        inf,        inf,\n",
      "               inf,        inf,        inf, 2.7376e+04, 4.2400e+04,        inf,\n",
      "        2.3488e+04,        inf,        inf,        inf, 2.2576e+04,        inf,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([[13504.,  2588.,   562.,  ...,  5116.,  8728., 10032.],\n",
      "        [26320.,  6992., 21968.,  ..., 18528., 10336.,  6532.],\n",
      "        [53952.,  6600.,  8496.,  ..., 34496., 32992., 14232.],\n",
      "        ...,\n",
      "        [ 5284.,  4252., 18592.,  ..., 29120., 34176.,  7816.],\n",
      "        [23616., 19200., 39552.,  ..., 22048.,  8872., 10320.],\n",
      "        [40128., 24944., 32416.,  ..., 37120., 37248., 11072.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([ 4034., 16376., 26416.,  ..., 20800.,  4332., 36096.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[  888.,  2616., 17680.,  ..., 10416., 35904.,  7832.],\n",
      "        [ 2120.,  5552.,  1504.,  ...,  4396.,  1824.,  2778.],\n",
      "        [ 7324., 10464.,  3612.,  ...,   518.,  3944.,  2038.],\n",
      "        ...,\n",
      "        [15504.,  2406., 21184.,  ..., 12344.,  3000., 14048.],\n",
      "        [ 2464.,  8272., 22112.,  ..., 16016., 32416., 16224.],\n",
      "        [ 3556.,  4004., 18128.,  ..., 25088., 13656.,  9848.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([38240.0000, 29984.0000, 25824.0000,        inf, 22560.0000, 12880.0000,\n",
      "        18256.0000, 17728.0000, 48000.0000,    88.5625, 23072.0000,        inf,\n",
      "        33184.0000,        inf,        inf,        inf,        inf, 30432.0000,\n",
      "               inf,        inf, 46880.0000, 29136.0000,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 19264.0000,\n",
      "               inf,        inf, 23760.0000, 23456.0000,  7128.0000,        inf,\n",
      "               inf,        inf, 37280.0000, 28064.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 16104.0000, 64928.0000, 26480.0000,\n",
      "               inf,        inf, 17264.0000,        inf,        inf,        inf,\n",
      "        53120.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 16232.0000, 10072.0000,  3106.0000, 23680.0000,        inf,\n",
      "               inf, 63616.0000, 64512.0000,        inf,        inf,  6660.0000,\n",
      "               inf,        inf,        inf,        inf,  5584.0000, 33376.0000,\n",
      "               inf, 55040.0000,        inf,        inf, 42624.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,  8864.0000,\n",
      "               inf,        inf,        inf,        inf, 62816.0000, 16720.0000,\n",
      "               inf, 52864.0000,  9584.0000, 12144.0000, 28752.0000,        inf,\n",
      "               inf, 24880.0000,        inf, 61344.0000, 28416.0000,        inf,\n",
      "               inf,        inf,        inf,  4136.0000,        inf,        inf,\n",
      "               inf, 36352.0000,        inf,        inf,        inf, 51680.0000,\n",
      "        62016.0000, 25248.0000, 28256.0000,        inf, 30416.0000,        inf,\n",
      "        42208.0000, 42784.0000, 32208.0000,        inf, 24560.0000, 45600.0000,\n",
      "               inf,        inf, 24368.0000,        inf, 45440.0000, 47008.0000,\n",
      "         5196.0000,        inf,        inf,  7580.0000,        inf, 36704.0000,\n",
      "               inf, 19776.0000, 60992.0000,        inf, 45856.0000,        inf,\n",
      "        16544.0000, 41376.0000,        inf,        inf,        inf, 50752.0000,\n",
      "         8720.0000, 64608.0000,        inf,        inf,        inf,        inf,\n",
      "        40256.0000,        inf, 51584.0000, 11192.0000,  2590.0000,        inf,\n",
      "               inf,        inf,  6384.0000,        inf,        inf,        inf,\n",
      "        62720.0000,        inf, 19376.0000,        inf, 28144.0000,        inf,\n",
      "               inf, 26368.0000, 44416.0000,        inf, 52416.0000,        inf,\n",
      "        59456.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        24288.0000, 54304.0000,        inf, 12464.0000, 38240.0000,        inf,\n",
      "               inf,  7276.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 32352.0000,        inf,        inf, 15720.0000,\n",
      "        22496.0000,        inf, 30656.0000,        inf,  2146.0000,        inf,\n",
      "               inf,        inf,        inf,        inf, 60768.0000,  8592.0000,\n",
      "               inf,        inf, 25040.0000,        inf,        inf,        inf,\n",
      "        34560.0000,  1710.0000,        inf, 47488.0000,  8856.0000, 58656.0000,\n",
      "               inf, 52096.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 25056.0000, 12624.0000, 45632.0000,\n",
      "               inf,        inf, 13232.0000,        inf,  4040.0000,  5036.0000,\n",
      "               inf,        inf, 39552.0000, 44832.0000, 52512.0000,        inf,\n",
      "         3736.0000,        inf, 30128.0000, 63840.0000, 64576.0000, 58432.0000,\n",
      "               inf,        inf, 56512.0000, 25856.0000,        inf, 48352.0000,\n",
      "               inf, 60256.0000,        inf,        inf,        inf,        inf,\n",
      "        21232.0000,        inf,        inf, 10864.0000, 34208.0000,  1487.0000,\n",
      "        45152.0000,        inf,        inf,        inf, 65024.0000, 52288.0000,\n",
      "        13512.0000,        inf, 46688.0000,  7464.0000, 26512.0000,        inf,\n",
      "          892.5000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 46400.0000,\n",
      "         1907.0000, 36224.0000, 58272.0000, 20512.0000,        inf,        inf,\n",
      "               inf,        inf, 33856.0000, 48576.0000, 11208.0000, 55136.0000,\n",
      "               inf, 10256.0000, 11312.0000, 48800.0000, 35968.0000,        inf,\n",
      "               inf,        inf, 26416.0000, 28736.0000,        inf,        inf,\n",
      "        24464.0000, 22576.0000, 29088.0000,        inf, 39872.0000,        inf,\n",
      "        62528.0000,        inf,        inf,        inf, 65408.0000,        inf,\n",
      "               inf, 37184.0000,        inf,        inf,        inf,        inf,\n",
      "        41984.0000,        inf, 44064.0000,        inf,        inf,        inf,\n",
      "        56192.0000, 10880.0000, 11128.0000,        inf, 24112.0000,   299.5000,\n",
      "               inf,        inf,        inf,  7432.0000, 48800.0000,  3354.0000,\n",
      "               inf, 43456.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 34496.0000,        inf,        inf,\n",
      "        36384.0000,        inf, 37184.0000,        inf, 64640.0000, 29264.0000,\n",
      "         3114.0000,        inf,        inf, 53344.0000,  2304.0000, 11496.0000,\n",
      "               inf, 30528.0000,   718.0000, 63936.0000,        inf, 33216.0000,\n",
      "        32496.0000, 38432.0000,        inf, 50304.0000,        inf,        inf,\n",
      "        27392.0000, 21952.0000,        inf, 19952.0000, 23632.0000,  2986.0000,\n",
      "               inf,        inf, 29952.0000,        inf,        inf,        inf,\n",
      "               inf, 20784.0000,  3436.0000, 41792.0000, 13448.0000, 10080.0000,\n",
      "           70.1875,        inf,        inf, 17056.0000, 13360.0000,        inf,\n",
      "        17216.0000,        inf, 16312.0000, 63264.0000, 41312.0000,  4364.0000,\n",
      "        31184.0000, 33152.0000,        inf,        inf,        inf, 41056.0000,\n",
      "               inf,        inf,        inf, 44992.0000,        inf,        inf,\n",
      "        26240.0000, 44736.0000,        inf,        inf, 39328.0000, 42272.0000,\n",
      "               inf,        inf,        inf,        inf, 30016.0000, 51360.0000,\n",
      "               inf,        inf,        inf, 22240.0000, 35904.0000, 34624.0000,\n",
      "         8272.0000,        inf,        inf, 36768.0000, 54752.0000,        inf,\n",
      "        44032.0000, 13000.0000,        inf,        inf, 51712.0000,        inf,\n",
      "        52416.0000,   174.1250,        inf,        inf,        inf, 53600.0000,\n",
      "        40416.0000, 45888.0000, 48288.0000,        inf, 19904.0000,        inf,\n",
      "        51296.0000,        inf,        inf, 54720.0000,        inf, 33760.0000,\n",
      "        32688.0000,        inf, 55616.0000, 29264.0000,        inf,        inf,\n",
      "               inf,        inf, 55936.0000, 10328.0000, 24960.0000,        inf,\n",
      "        56128.0000,        inf,        inf,        inf, 64480.0000, 61120.0000,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([3.3440e+04, 9.6960e+03, 5.7000e+01, 3.3568e+04, 2.6480e+04, 6.0080e+03,\n",
      "        2.1104e+04, 2.5056e+04, 2.6680e+03, 1.1608e+04, 1.3856e+04,        inf,\n",
      "        1.2464e+04,        inf, 9.1360e+03, 3.0496e+04,        inf, 4.2368e+04,\n",
      "        3.1000e+02,        inf, 1.0544e+04, 6.2784e+04,        inf, 4.9440e+04,\n",
      "        1.9984e+04, 2.2304e+04,        inf,        inf, 1.9552e+04, 1.8480e+04,\n",
      "               inf, 4.3936e+04, 5.6608e+04, 3.2752e+04, 2.9920e+04, 3.7664e+04,\n",
      "        4.9760e+04, 2.7480e+03, 2.8700e+03, 2.2912e+04, 6.1920e+04, 4.9088e+04,\n",
      "        6.3424e+04, 4.7744e+04, 1.8192e+04, 2.5040e+04, 3.3600e+04, 5.5200e+03,\n",
      "        4.9888e+04, 7.8360e+03, 1.3120e+03, 2.1664e+04, 4.0704e+04, 3.7500e+03,\n",
      "        3.3504e+04,        inf,        inf,        inf,        inf, 1.6448e+04,\n",
      "        2.4976e+04, 2.1056e+04,        inf, 1.6512e+04, 8.8500e+02,        inf,\n",
      "        5.7760e+04, 1.8608e+04, 2.7536e+04, 2.0496e+04, 3.0944e+04, 2.1536e+04,\n",
      "        2.9312e+04, 1.9696e+04, 1.6288e+04,        inf, 1.8592e+04, 2.0592e+04,\n",
      "        1.7120e+04,        inf, 4.7680e+04,        inf,        inf, 3.9400e+03,\n",
      "        1.8320e+04, 3.6640e+04, 3.3152e+04,        inf, 1.4016e+04, 5.1520e+03,\n",
      "        5.6080e+03, 3.6224e+04,        inf, 4.6400e+04, 9.0800e+03,        inf,\n",
      "        1.6448e+04, 2.2928e+04, 6.1024e+04, 1.2184e+04, 9.3760e+03, 2.6576e+04,\n",
      "               inf, 8.6000e+03, 5.1904e+04, 8.3040e+03, 3.2624e+04, 4.9408e+04,\n",
      "        7.3680e+03, 3.3160e+03, 6.1664e+04, 2.2864e+04,        inf, 4.6240e+04,\n",
      "        1.3120e+04, 1.4330e+03, 3.0704e+04, 4.3456e+04, 1.7088e+04, 6.3360e+04,\n",
      "        1.1424e+04, 3.0720e+04, 1.3000e+04, 3.2000e+04, 1.1656e+04, 5.0368e+04,\n",
      "        1.8992e+04, 2.5520e+04, 7.8400e+03, 5.1072e+04, 3.6768e+04,        inf,\n",
      "               inf,        inf, 1.2232e+04, 3.3800e+02, 7.7400e+03, 2.4048e+04,\n",
      "        2.2080e+04, 1.0400e+04,        inf, 1.5160e+04,        inf, 2.5264e+04,\n",
      "        4.1440e+04, 5.5072e+04, 4.4896e+04, 2.3520e+04, 5.4800e+03, 3.3536e+04,\n",
      "        1.4576e+04, 2.6160e+04,        inf, 2.5856e+04, 3.1408e+04, 7.4520e+03,\n",
      "        1.2560e+04, 4.4928e+04, 3.2000e+04, 2.8848e+04, 4.5120e+03,        inf,\n",
      "        2.0960e+04,        inf, 2.5800e+02, 1.6104e+04, 4.7040e+04,        inf,\n",
      "        2.9000e+03,        inf, 2.0608e+04,        inf,        inf, 5.0464e+04,\n",
      "        5.2864e+04, 5.6800e+04, 1.0744e+04, 1.0776e+04, 1.2304e+04,        inf,\n",
      "        2.0320e+04, 1.4808e+04, 2.0080e+04,        inf, 3.9488e+04, 1.2680e+04,\n",
      "        8.3550e+02, 4.0680e+03, 2.9984e+04,        inf, 5.5680e+04,        inf,\n",
      "        3.7056e+04,        inf, 7.0720e+03, 3.9552e+04, 7.1440e+03,        inf,\n",
      "               inf, 2.7520e+04, 5.0816e+04, 4.3104e+04, 3.3320e+03, 1.6304e+04,\n",
      "        5.6384e+04, 1.9150e+02, 8.8240e+03, 1.5472e+04,        inf, 1.3872e+04,\n",
      "        1.1200e+04,        inf, 1.3728e+04, 1.0216e+04, 3.4656e+04, 5.3216e+04,\n",
      "        6.4800e+04,        inf, 5.5744e+04, 1.2776e+04, 6.0672e+04, 2.8256e+04,\n",
      "        3.4464e+04, 3.5392e+04, 7.9560e+03, 6.3008e+04,        inf,        inf,\n",
      "        1.8570e+03, 2.0016e+04, 4.5344e+04, 2.1968e+04, 2.3664e+04, 6.3360e+03,\n",
      "               inf, 7.1200e+02, 5.1456e+04, 3.0096e+04,        inf, 2.9152e+04,\n",
      "        6.2112e+04,        inf,        inf, 1.9776e+04, 1.6608e+04, 2.2048e+04,\n",
      "               inf, 2.5120e+04, 2.5552e+04, 5.3888e+04, 1.4600e+04, 3.5260e+03,\n",
      "        5.5712e+04, 4.7712e+04, 1.9984e+04, 3.9760e+03, 1.8480e+04, 3.7696e+04,\n",
      "               inf, 2.9168e+04, 3.3664e+04, 4.5080e+03,        inf, 6.1408e+04,\n",
      "        1.6960e+04,        inf,        inf, 8.7360e+03,        inf, 1.2432e+04,\n",
      "        5.6864e+04, 5.4120e+03, 4.4928e+04, 3.6720e+03,        inf,        inf,\n",
      "        1.4520e+04,        inf,        inf, 4.0800e+04,        inf, 1.5200e+03,\n",
      "               inf, 4.4192e+04, 6.3936e+04,        inf, 3.1700e+03, 1.7440e+04,\n",
      "        3.3088e+04,        inf, 2.9328e+04, 2.0448e+04, 2.4064e+04,        inf,\n",
      "        4.4896e+04, 4.8880e+03, 6.2304e+04,        inf,        inf,        inf,\n",
      "               inf, 1.2940e+03, 4.2560e+04, 3.8400e+04, 4.3776e+04, 3.4720e+04,\n",
      "        1.8688e+04, 5.8176e+04, 8.0800e+03,        inf, 2.7136e+04,        inf,\n",
      "               inf, 1.3800e+04, 4.9056e+04,        inf, 3.5136e+04, 3.5648e+04,\n",
      "               inf, 3.9616e+04, 5.8048e+04, 4.0480e+04, 1.4320e+04, 5.2128e+04,\n",
      "        5.0368e+04, 5.9232e+04,        inf, 8.7520e+03, 9.9200e+03, 3.2000e+04,\n",
      "        1.4120e+04, 1.6832e+04, 4.3648e+04, 2.0336e+04, 1.6032e+04, 7.6320e+03,\n",
      "        1.2496e+04, 4.3072e+04, 5.0208e+04, 4.0736e+04,        inf,        inf,\n",
      "        4.3456e+04, 7.7840e+03,        inf,        inf, 2.0832e+04, 3.7536e+04,\n",
      "        2.2688e+04, 4.9408e+04, 5.0816e+04, 2.3104e+04, 1.2840e+03,        inf,\n",
      "        5.4784e+04, 3.5620e+03, 2.0448e+04, 6.8320e+03, 3.9392e+04, 1.1184e+04,\n",
      "               inf,        inf, 6.0160e+04, 1.1216e+04, 2.6096e+04, 5.5520e+03,\n",
      "               inf, 3.9936e+04,        inf,        inf, 3.5808e+04, 4.9280e+04,\n",
      "        2.4920e+03,        inf,        inf, 2.0032e+04, 6.4096e+04,        inf,\n",
      "        4.2240e+04, 2.5648e+04,        inf,        inf, 7.5640e+03, 5.1456e+04,\n",
      "        1.1928e+04,        inf,        inf, 2.5800e+03, 5.9800e+03, 2.7152e+04,\n",
      "               inf, 2.8016e+04, 4.9600e+03, 4.8832e+04,        inf, 2.0032e+04,\n",
      "        2.4800e+04,        inf, 4.7328e+04, 1.8752e+04,        inf,        inf,\n",
      "        2.6064e+04, 3.4016e+04,        inf, 3.2128e+04, 4.6720e+04, 3.7376e+04,\n",
      "               inf,        inf, 5.9104e+04,        inf,        inf, 3.6480e+04,\n",
      "               inf, 1.3752e+04, 9.4640e+03, 2.0768e+04, 4.9536e+04, 4.8480e+03,\n",
      "        9.4160e+03, 3.3888e+04,        inf, 2.0608e+04, 2.6000e+04,        inf,\n",
      "        4.8128e+04, 6.1216e+04, 2.4032e+04, 2.1776e+04, 1.1552e+04, 2.7104e+04,\n",
      "        2.7350e+02, 2.7552e+04,        inf, 6.3968e+04, 3.3984e+04, 3.9904e+04,\n",
      "               inf, 2.9312e+04, 6.1472e+04,        inf, 1.9376e+04,        inf,\n",
      "               inf, 7.5800e+02, 5.6288e+04,        inf, 1.4208e+04, 3.7824e+04,\n",
      "               inf, 1.0040e+04,        inf,        inf, 7.6640e+03, 2.1984e+04,\n",
      "               inf, 3.2112e+04,        inf, 4.7360e+04,        inf, 1.5728e+04,\n",
      "        1.7888e+04, 2.0496e+04,        inf,        inf, 2.2384e+04, 5.5488e+04,\n",
      "        3.9488e+04, 1.8080e+04,        inf,        inf, 4.3456e+04, 4.8096e+04,\n",
      "        4.3280e+03, 2.3296e+04,        inf, 4.3600e+02,        inf, 4.5504e+04,\n",
      "        2.6624e+04, 6.1600e+04, 2.0736e+04,        inf, 1.5904e+04, 3.6416e+04,\n",
      "               inf, 4.7456e+04,        inf, 1.4016e+04,        inf, 3.2304e+04,\n",
      "        5.9904e+04,        inf, 3.8272e+04, 3.5648e+04,        inf, 2.4448e+04,\n",
      "               inf,        inf, 7.2080e+03, 1.8320e+04, 5.3120e+04,        inf,\n",
      "        9.4080e+03,        inf,        inf,        inf, 4.6144e+04,        inf,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([33088.0000, 40352.0000, 14416.0000,        inf, 25664.0000, 30496.0000,\n",
      "        41024.0000, 16112.0000, 33664.0000,        inf, 24848.0000,        inf,\n",
      "        38112.0000,        inf, 38784.0000, 47168.0000,        inf,  9192.0000,\n",
      "               inf,        inf, 50912.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,  4316.0000,\n",
      "               inf, 27760.0000,        inf,        inf, 46048.0000,        inf,\n",
      "               inf,        inf, 60320.0000, 48672.0000,        inf,        inf,\n",
      "        45728.0000,        inf,        inf,  1197.0000,        inf, 54784.0000,\n",
      "               inf, 50144.0000, 15752.0000,        inf,        inf,        inf,\n",
      "        18048.0000,        inf,        inf, 37888.0000,        inf, 63872.0000,\n",
      "               inf, 20864.0000,        inf,  5820.0000, 21984.0000,        inf,\n",
      "               inf,        inf, 44928.0000,        inf,        inf,  5128.0000,\n",
      "        57984.0000,        inf,        inf,        inf,  5376.0000,        inf,\n",
      "               inf, 43072.0000,        inf,        inf, 60064.0000,        inf,\n",
      "               inf, 44544.0000,        inf,        inf,        inf,  2876.0000,\n",
      "               inf,        inf,        inf,        inf, 10088.0000,        inf,\n",
      "               inf,        inf, 35904.0000, 25968.0000,        inf,        inf,\n",
      "               inf,  6808.0000,        inf,        inf, 45728.0000, 50912.0000,\n",
      "               inf, 29184.0000,        inf, 25296.0000,        inf,        inf,\n",
      "        41312.0000, 39744.0000,        inf, 62624.0000, 64320.0000,        inf,\n",
      "        61824.0000, 43584.0000, 29376.0000, 55616.0000,        inf,        inf,\n",
      "         6460.0000, 27024.0000,   479.7500,        inf, 34496.0000,        inf,\n",
      "               inf,        inf,  7124.0000,        inf,        inf, 46848.0000,\n",
      "         1389.0000,        inf,        inf, 12384.0000,        inf, 36512.0000,\n",
      "               inf, 11000.0000, 62976.0000, 45216.0000, 24560.0000, 29536.0000,\n",
      "         8664.0000,  8472.0000,        inf,        inf, 52608.0000,        inf,\n",
      "        37376.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,  3764.0000, 22384.0000, 10824.0000,        inf,\n",
      "               inf,        inf,  5160.0000,        inf,        inf,        inf,\n",
      "        53760.0000,        inf, 13512.0000,        inf, 22752.0000,        inf,\n",
      "               inf, 11792.0000,  1221.0000,        inf,        inf,        inf,\n",
      "         8232.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 57600.0000, 24432.0000, 22208.0000,        inf,\n",
      "               inf, 39968.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 27200.0000, 17440.0000,        inf,        inf, 28944.0000,\n",
      "        31264.0000,        inf, 30192.0000,        inf, 41920.0000,        inf,\n",
      "               inf,        inf,        inf, 25632.0000,        inf, 36800.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "        20640.0000, 30768.0000, 62816.0000, 37824.0000, 21792.0000, 12288.0000,\n",
      "               inf, 47104.0000, 63616.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,  5332.0000,  8092.0000, 22080.0000,\n",
      "               inf,        inf, 23264.0000,        inf, 51936.0000, 17104.0000,\n",
      "               inf,        inf,  5992.0000,  4988.0000, 50400.0000,        inf,\n",
      "         2968.0000,        inf,        inf, 36320.0000,        inf,        inf,\n",
      "               inf, 43328.0000, 63200.0000, 19632.0000, 45600.0000, 41568.0000,\n",
      "        16464.0000, 34496.0000,        inf, 57216.0000,        inf,        inf,\n",
      "        59264.0000,        inf,        inf, 11216.0000, 17408.0000, 22032.0000,\n",
      "               inf, 56608.0000,        inf,        inf, 24672.0000,  8168.0000,\n",
      "        13368.0000,        inf, 58048.0000, 28768.0000, 13336.0000,        inf,\n",
      "        54240.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        53792.0000, 62720.0000, 53408.0000,        inf,        inf,        inf,\n",
      "        38624.0000, 28896.0000, 56352.0000, 25264.0000,        inf, 15688.0000,\n",
      "               inf,        inf, 11000.0000,        inf,  8560.0000, 18496.0000,\n",
      "               inf, 13176.0000,  8000.0000,        inf,  1347.0000,        inf,\n",
      "               inf,        inf, 26528.0000,  9792.0000,        inf,        inf,\n",
      "        16448.0000, 27424.0000,  3412.0000,        inf, 11304.0000,        inf,\n",
      "        49344.0000,        inf, 10128.0000,        inf,        inf, 63712.0000,\n",
      "               inf, 53600.0000,        inf,        inf,        inf,        inf,\n",
      "        45760.0000, 35360.0000, 13880.0000,        inf, 26352.0000,        inf,\n",
      "               inf, 41920.0000, 10168.0000,        inf, 56928.0000, 17088.0000,\n",
      "        58176.0000,        inf,        inf, 59232.0000, 53120.0000, 27824.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,  1760.0000,        inf,        inf,\n",
      "        27200.0000,        inf,        inf,        inf, 10016.0000, 33792.0000,\n",
      "        15592.0000,        inf,        inf, 39968.0000,  1941.0000, 19504.0000,\n",
      "               inf, 31392.0000,  3494.0000, 32384.0000,        inf, 23504.0000,\n",
      "        36608.0000,        inf,        inf, 58528.0000,        inf,        inf,\n",
      "        39360.0000, 35584.0000,        inf, 36000.0000, 51744.0000, 44096.0000,\n",
      "               inf,        inf,        inf,        inf,        inf, 20912.0000,\n",
      "               inf, 12512.0000, 20112.0000, 25088.0000, 31872.0000,  3900.0000,\n",
      "        33152.0000,        inf,        inf, 24544.0000,  4684.0000,        inf,\n",
      "        12320.0000,        inf, 59424.0000, 53248.0000, 51232.0000, 32496.0000,\n",
      "        18272.0000,  1181.0000,        inf,        inf,        inf, 33120.0000,\n",
      "               inf, 27392.0000,        inf,        inf, 42112.0000,        inf,\n",
      "               inf, 32160.0000,        inf,        inf, 21120.0000, 53760.0000,\n",
      "               inf,  3862.0000,        inf,        inf, 47808.0000, 49696.0000,\n",
      "               inf,        inf,        inf, 27008.0000,        inf, 22736.0000,\n",
      "        29056.0000,        inf,        inf,        inf, 51168.0000, 47712.0000,\n",
      "        60448.0000,  8424.0000,        inf,        inf, 46464.0000,        inf,\n",
      "        16376.0000, 35200.0000,        inf, 42144.0000,        inf,        inf,\n",
      "        16104.0000, 43648.0000,  6576.0000,        inf, 31536.0000,        inf,\n",
      "        48224.0000, 54560.0000,        inf,        inf,        inf, 23664.0000,\n",
      "        64160.0000,        inf,        inf,  7024.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 23040.0000, 37280.0000,        inf,\n",
      "        13544.0000,        inf,        inf,        inf, 17456.0000,        inf,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([47360.0000, 17168.0000,  1083.0000, 41440.0000, 20192.0000,  2009.0000,\n",
      "        21776.0000, 20928.0000,  4612.0000,  3312.0000, 18384.0000,        inf,\n",
      "        10240.0000,        inf,  3096.0000, 26416.0000, 54784.0000, 47264.0000,\n",
      "        17744.0000,        inf, 11744.0000,        inf, 21504.0000,        inf,\n",
      "        34560.0000, 24576.0000,        inf,        inf, 32720.0000, 21184.0000,\n",
      "               inf, 35232.0000, 62688.0000, 17072.0000, 21328.0000, 30240.0000,\n",
      "               inf,  8400.0000, 14760.0000,  9312.0000, 34496.0000, 40000.0000,\n",
      "               inf, 34624.0000, 15272.0000, 17872.0000,  1524.0000,  3286.0000,\n",
      "        41376.0000,  1706.0000,   575.5000, 32112.0000, 43776.0000,  4804.0000,\n",
      "        40224.0000,        inf, 41312.0000, 42560.0000,        inf, 29696.0000,\n",
      "        39296.0000, 17808.0000,        inf, 17648.0000,  6748.0000,        inf,\n",
      "        43328.0000, 43520.0000, 26768.0000, 37248.0000, 41632.0000, 17184.0000,\n",
      "        29296.0000, 19648.0000, 20656.0000,        inf, 16656.0000, 34208.0000,\n",
      "        29984.0000,        inf, 61632.0000,        inf,        inf, 20608.0000,\n",
      "        51616.0000, 31120.0000,  4752.0000,        inf, 16624.0000,  1489.0000,\n",
      "        32048.0000, 20624.0000,        inf, 42880.0000, 11168.0000, 60448.0000,\n",
      "        14800.0000, 18880.0000, 44928.0000,  7388.0000, 20032.0000,  2200.0000,\n",
      "               inf,  8968.0000,        inf, 26960.0000, 39392.0000, 34432.0000,\n",
      "         2804.0000,  8744.0000,        inf, 25904.0000,        inf, 47776.0000,\n",
      "         1025.0000,  3048.0000, 52800.0000, 44192.0000, 28416.0000, 65088.0000,\n",
      "        14832.0000, 29744.0000, 16992.0000, 36320.0000,  6420.0000, 58624.0000,\n",
      "        19104.0000, 21280.0000,  2080.0000, 57632.0000, 33376.0000,        inf,\n",
      "               inf, 63072.0000, 10768.0000, 12456.0000, 18512.0000,  8584.0000,\n",
      "        22352.0000, 18336.0000, 46240.0000, 16448.0000,        inf, 15744.0000,\n",
      "               inf, 55328.0000, 30960.0000, 35968.0000, 10432.0000, 18896.0000,\n",
      "        10624.0000, 20752.0000,        inf, 36864.0000, 24736.0000,  2174.0000,\n",
      "        15720.0000, 22688.0000, 21344.0000, 10424.0000, 39712.0000,        inf,\n",
      "        50176.0000,        inf,  1164.0000, 15944.0000, 52128.0000,        inf,\n",
      "        19600.0000,        inf, 21056.0000,        inf,        inf, 49088.0000,\n",
      "        49920.0000,        inf, 10912.0000, 27248.0000,  3828.0000,        inf,\n",
      "        10944.0000, 15832.0000,  9616.0000,        inf, 46336.0000, 23136.0000,\n",
      "         3392.0000,  6516.0000,  9072.0000,        inf, 58624.0000,        inf,\n",
      "        58976.0000, 60864.0000,  8164.0000, 28160.0000,  4640.0000,        inf,\n",
      "               inf, 21840.0000,        inf, 53280.0000,  2408.0000, 62400.0000,\n",
      "               inf,  5208.0000,  7040.0000, 12120.0000,        inf,  9464.0000,\n",
      "        12504.0000, 52384.0000,  6840.0000, 33056.0000, 28640.0000, 29968.0000,\n",
      "               inf,        inf,        inf, 13648.0000, 17088.0000, 32112.0000,\n",
      "        53344.0000, 37184.0000, 13632.0000, 33120.0000,        inf,        inf,\n",
      "         6400.0000, 22512.0000, 18304.0000, 36096.0000, 16032.0000,  8280.0000,\n",
      "               inf,  7904.0000, 27216.0000, 32960.0000,        inf,  5808.0000,\n",
      "        60352.0000,        inf, 44256.0000, 20336.0000, 17792.0000, 15064.0000,\n",
      "               inf, 29648.0000, 20896.0000, 39008.0000,  5432.0000,  1230.0000,\n",
      "        28560.0000,        inf, 18464.0000,   139.0000, 31264.0000, 10104.0000,\n",
      "               inf,   812.0000, 25888.0000, 12656.0000,        inf,        inf,\n",
      "        21456.0000, 58208.0000,        inf,  6008.0000,        inf, 14304.0000,\n",
      "        55360.0000,   449.0000, 11304.0000, 13712.0000,        inf,        inf,\n",
      "        13736.0000,        inf,        inf, 39296.0000, 62848.0000, 13848.0000,\n",
      "               inf, 45952.0000, 41248.0000, 64480.0000,   436.7500, 16880.0000,\n",
      "        24928.0000,        inf, 30896.0000, 12864.0000, 20288.0000,        inf,\n",
      "        50720.0000, 24560.0000, 61376.0000,        inf,        inf,        inf,\n",
      "               inf, 26608.0000, 57792.0000, 51552.0000,        inf, 55680.0000,\n",
      "        27840.0000, 60128.0000, 11360.0000,        inf, 14336.0000,        inf,\n",
      "               inf,  4480.0000, 50656.0000,        inf, 32064.0000, 27136.0000,\n",
      "               inf, 38112.0000, 60480.0000,        inf, 16592.0000, 43328.0000,\n",
      "        30048.0000,        inf,        inf, 11216.0000,  7980.0000, 12160.0000,\n",
      "        12536.0000,  6308.0000, 42048.0000, 12808.0000, 23136.0000, 31184.0000,\n",
      "        22672.0000,  1229.0000, 48096.0000,        inf, 58784.0000,        inf,\n",
      "        37024.0000, 10912.0000, 52992.0000,        inf, 61248.0000, 21344.0000,\n",
      "        19184.0000, 33888.0000, 51840.0000, 26976.0000,  5656.0000,        inf,\n",
      "               inf, 13144.0000, 19760.0000, 30528.0000, 48704.0000,  2870.0000,\n",
      "        45152.0000, 65184.0000,        inf, 13312.0000, 13448.0000, 10744.0000,\n",
      "               inf, 40224.0000,        inf,        inf, 65280.0000, 62880.0000,\n",
      "        15152.0000,        inf, 63488.0000,  9648.0000,        inf,        inf,\n",
      "        36544.0000, 24592.0000,        inf, 61888.0000, 10976.0000, 61024.0000,\n",
      "        10424.0000,        inf,        inf, 11888.0000,  4376.0000, 30464.0000,\n",
      "        33472.0000, 40896.0000,  2768.0000, 45472.0000, 41760.0000, 19984.0000,\n",
      "         7712.0000,        inf,        inf, 33472.0000,        inf,        inf,\n",
      "        27744.0000, 36960.0000,        inf, 50976.0000,        inf, 37664.0000,\n",
      "               inf,        inf, 46944.0000,        inf,        inf, 41216.0000,\n",
      "        55104.0000, 15664.0000,  6536.0000, 15056.0000, 48544.0000,  3874.0000,\n",
      "         2126.0000, 61184.0000,        inf, 29248.0000, 23136.0000,        inf,\n",
      "        52320.0000,        inf, 23824.0000, 28944.0000, 24496.0000, 22352.0000,\n",
      "          357.5000, 22240.0000,        inf, 52768.0000, 65376.0000, 46016.0000,\n",
      "               inf, 36864.0000, 58880.0000,        inf, 18976.0000,        inf,\n",
      "               inf,  2710.0000, 57824.0000, 45856.0000,  1396.0000, 35904.0000,\n",
      "               inf, 12352.0000,        inf,        inf, 18416.0000, 19072.0000,\n",
      "               inf, 40960.0000,        inf, 42240.0000,        inf, 12360.0000,\n",
      "        16032.0000, 42016.0000,        inf,        inf, 26992.0000,        inf,\n",
      "        40448.0000, 19232.0000,        inf, 51840.0000, 51232.0000, 60928.0000,\n",
      "          171.5000, 20160.0000,        inf,  4592.0000,        inf, 47520.0000,\n",
      "        39904.0000, 49984.0000, 17104.0000,        inf, 15472.0000, 32032.0000,\n",
      "               inf, 48096.0000,        inf,   868.0000,        inf, 31232.0000,\n",
      "               inf,        inf, 43616.0000, 38304.0000,        inf, 15312.0000,\n",
      "               inf,        inf,  4156.0000, 20096.0000, 57632.0000,        inf,\n",
      "         9024.0000,        inf, 36992.0000,        inf, 43584.0000,        inf,\n",
      "               inf, 35744.0000], device='cuda:0', dtype=torch.float16), tensor([3.9584e+04, 4.0288e+04, 1.1720e+04,        inf, 2.3360e+04, 2.6464e+04,\n",
      "        4.5152e+04, 2.6704e+04, 3.7760e+04,        inf, 2.8256e+04,        inf,\n",
      "        4.3648e+04,        inf, 3.8048e+04, 4.5472e+04,        inf, 1.1624e+04,\n",
      "               inf,        inf, 5.2768e+04,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 6.5400e+03,\n",
      "               inf, 2.8192e+04,        inf,        inf, 4.0480e+04,        inf,\n",
      "               inf,        inf, 5.5008e+04, 5.1520e+04,        inf,        inf,\n",
      "        4.7808e+04,        inf,        inf, 4.3240e+03,        inf, 5.6960e+04,\n",
      "               inf, 5.4464e+04, 2.0944e+04,        inf,        inf,        inf,\n",
      "        1.7184e+04,        inf,        inf, 3.7216e+04,        inf,        inf,\n",
      "               inf, 2.2064e+04,        inf, 1.6660e+03, 2.4224e+04,        inf,\n",
      "               inf,        inf, 4.4000e+04,        inf,        inf, 5.8440e+03,\n",
      "        5.4496e+04,        inf,        inf,        inf, 3.4260e+03,        inf,\n",
      "               inf, 3.6160e+04,        inf,        inf, 5.8176e+04,        inf,\n",
      "               inf, 4.2816e+04,        inf,        inf,        inf, 9.1600e+03,\n",
      "               inf,        inf,        inf,        inf, 7.0280e+03,        inf,\n",
      "               inf,        inf, 3.7920e+04, 3.0608e+04,        inf,        inf,\n",
      "               inf, 7.1875e+00,        inf,        inf, 4.0768e+04, 6.1568e+04,\n",
      "               inf, 3.5008e+04,        inf, 3.4720e+04,        inf,        inf,\n",
      "        4.8224e+04, 3.6096e+04,        inf,        inf,        inf,        inf,\n",
      "        6.3328e+04, 4.8544e+04, 2.8384e+04, 5.6928e+04,        inf,        inf,\n",
      "        4.1320e+03, 3.0768e+04, 1.2010e+03,        inf, 2.8064e+04,        inf,\n",
      "               inf,        inf, 8.9920e+03,        inf,        inf, 5.2608e+04,\n",
      "        4.7640e+03,        inf,        inf, 9.3360e+03,        inf, 3.9392e+04,\n",
      "               inf, 1.3448e+04, 5.4624e+04, 4.5440e+04, 2.7344e+04, 2.8064e+04,\n",
      "        9.1920e+03, 4.7400e+03,        inf,        inf, 4.6272e+04,        inf,\n",
      "        3.9392e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 1.7150e+03, 1.7824e+04, 1.7008e+04,        inf,\n",
      "               inf,        inf, 5.8950e+02,        inf,        inf,        inf,\n",
      "        4.9248e+04,        inf, 1.1976e+04,        inf, 2.1616e+04,        inf,\n",
      "               inf, 1.0192e+04, 4.6600e+03,        inf,        inf,        inf,\n",
      "        1.2352e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 6.2496e+04, 1.7696e+04, 2.5456e+04,        inf,\n",
      "               inf, 3.7472e+04,        inf,        inf,        inf,        inf,\n",
      "               inf, 3.1920e+04, 1.7920e+04,        inf,        inf, 2.5072e+04,\n",
      "        2.7936e+04,        inf, 3.1056e+04,        inf, 4.6880e+04,        inf,\n",
      "               inf,        inf,        inf, 3.3056e+04,        inf, 3.6736e+04,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "        1.5392e+04, 4.1504e+04,        inf, 4.2496e+04, 2.4928e+04, 1.1944e+04,\n",
      "               inf, 5.3696e+04, 6.1344e+04, 5.8336e+04,        inf,        inf,\n",
      "               inf,        inf,        inf, 5.7000e+02, 4.4320e+03, 1.4104e+04,\n",
      "               inf,        inf, 1.7120e+04,        inf, 4.6112e+04, 1.0504e+04,\n",
      "               inf,        inf, 1.0992e+04, 7.6800e+02, 5.4528e+04,        inf,\n",
      "        4.7480e+03,        inf,        inf, 3.5392e+04,        inf,        inf,\n",
      "               inf, 4.6592e+04, 5.7632e+04, 2.2800e+04, 5.5712e+04, 3.9328e+04,\n",
      "        1.4104e+04, 3.9040e+04,        inf, 6.5056e+04,        inf,        inf,\n",
      "               inf,        inf,        inf, 1.4936e+04, 1.8096e+04, 3.3056e+04,\n",
      "               inf, 5.3920e+04,        inf,        inf, 2.6608e+04, 1.3808e+04,\n",
      "        1.6320e+04,        inf, 6.3936e+04, 2.4768e+04, 2.0096e+04,        inf,\n",
      "        4.9440e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "        6.0608e+04,        inf, 6.4352e+04,        inf,        inf,        inf,\n",
      "        3.6896e+04, 3.1040e+04, 6.3808e+04, 2.4784e+04,        inf, 2.4240e+04,\n",
      "               inf,        inf, 1.4872e+04,        inf, 1.6624e+04, 1.8032e+04,\n",
      "               inf, 1.1848e+04, 8.6320e+03,        inf, 1.1140e+03,        inf,\n",
      "               inf,        inf, 2.4752e+04, 2.4880e+03,        inf,        inf,\n",
      "        9.6320e+03, 3.1664e+04, 4.2200e+02,        inf, 1.6368e+04,        inf,\n",
      "        4.0896e+04,        inf, 6.7480e+03,        inf,        inf,        inf,\n",
      "               inf, 4.9248e+04,        inf,        inf,        inf,        inf,\n",
      "        3.8944e+04, 3.5872e+04, 1.5176e+04,        inf, 2.8016e+04,        inf,\n",
      "               inf, 4.9760e+04, 8.8240e+03,        inf,        inf, 1.0704e+04,\n",
      "        4.7392e+04,        inf,        inf,        inf, 5.5840e+04, 3.2576e+04,\n",
      "               inf, 6.3776e+04,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 8.0760e+03,        inf,        inf,\n",
      "        3.4944e+04,        inf,        inf,        inf, 1.1416e+04, 4.6976e+04,\n",
      "        1.3208e+04,        inf,        inf, 3.6544e+04, 6.3080e+03, 2.4016e+04,\n",
      "               inf, 3.5456e+04, 5.9720e+03, 3.4080e+04,        inf, 2.3120e+04,\n",
      "        2.7824e+04,        inf,        inf, 6.5216e+04,        inf,        inf,\n",
      "        4.1024e+04, 3.8624e+04,        inf, 4.5440e+04, 5.1264e+04, 4.5536e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 1.9696e+04,\n",
      "               inf, 8.2480e+03, 1.0104e+04, 2.7232e+04, 3.6256e+04, 2.1860e+03,\n",
      "        2.9472e+04,        inf,        inf, 3.0048e+04, 3.1700e+03,        inf,\n",
      "        2.2224e+04,        inf,        inf, 5.6032e+04, 5.0944e+04, 3.0048e+04,\n",
      "        2.0912e+04, 2.5680e+03,        inf,        inf,        inf, 3.6544e+04,\n",
      "               inf, 2.9760e+04,        inf,        inf, 4.2400e+04,        inf,\n",
      "               inf, 3.3792e+04,        inf,        inf, 9.5440e+03, 5.1808e+04,\n",
      "               inf, 5.6800e+03,        inf,        inf, 4.1632e+04, 4.6784e+04,\n",
      "               inf,        inf,        inf, 2.4224e+04,        inf, 1.2288e+04,\n",
      "        2.6304e+04,        inf,        inf,        inf, 5.2032e+04, 5.9552e+04,\n",
      "               inf, 1.1552e+04,        inf,        inf, 4.6624e+04,        inf,\n",
      "        1.9696e+04, 2.8064e+04,        inf, 5.3280e+04,        inf,        inf,\n",
      "        2.9936e+04, 4.1728e+04, 9.5200e+03,        inf, 3.0304e+04,        inf,\n",
      "        5.1904e+04, 5.0688e+04,        inf,        inf,        inf, 2.9584e+04,\n",
      "               inf,        inf,        inf, 1.2728e+04,        inf,        inf,\n",
      "               inf,        inf,        inf, 2.3360e+04, 4.1664e+04,        inf,\n",
      "        2.2352e+04,        inf,        inf,        inf, 1.1592e+04,        inf,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([34624.0000, 22144.0000,  3188.0000, 61984.0000, 14264.0000, 21728.0000,\n",
      "        25152.0000, 41760.0000, 42400.0000, 24608.0000, 34976.0000, 54176.0000,\n",
      "        14816.0000,        inf, 35008.0000, 35296.0000, 64000.0000, 46752.0000,\n",
      "               inf, 56928.0000,  4728.0000, 17328.0000, 16400.0000,        inf,\n",
      "         9688.0000, 51648.0000, 57440.0000,        inf, 18560.0000, 24336.0000,\n",
      "               inf,        inf, 50304.0000,  6128.0000, 13296.0000, 55200.0000,\n",
      "               inf, 63968.0000, 19440.0000, 29600.0000, 47008.0000, 62272.0000,\n",
      "               inf, 35840.0000,  1597.0000,  9568.0000,  7884.0000,  5084.0000,\n",
      "        30768.0000, 60864.0000,  3756.0000,   624.0000, 48896.0000,  9496.0000,\n",
      "        59552.0000, 41856.0000, 25520.0000,   182.6250, 39648.0000,  9944.0000,\n",
      "        42912.0000, 21200.0000, 33504.0000,  4488.0000, 15688.0000,        inf,\n",
      "        18528.0000, 56032.0000, 50816.0000, 10512.0000, 42624.0000,  9312.0000,\n",
      "         2786.0000, 12328.0000,  9800.0000,        inf,  2144.0000, 33248.0000,\n",
      "        18832.0000,        inf, 44992.0000, 14184.0000, 65216.0000, 29152.0000,\n",
      "        31328.0000, 28432.0000,  7080.0000,        inf, 33280.0000,  3204.0000,\n",
      "        46528.0000, 54944.0000, 57472.0000, 10416.0000, 44160.0000, 55840.0000,\n",
      "        12768.0000,  3036.0000, 47456.0000, 10952.0000, 13432.0000,   427.5000,\n",
      "               inf,  7828.0000, 62112.0000, 22752.0000, 29760.0000,  4236.0000,\n",
      "        37056.0000,  6656.0000,        inf,  2560.0000,        inf,        inf,\n",
      "        29840.0000, 16160.0000, 28304.0000, 40224.0000, 34880.0000, 37376.0000,\n",
      "        13968.0000, 30144.0000, 16320.0000, 46080.0000, 19616.0000,        inf,\n",
      "        13040.0000,  5140.0000, 13208.0000,        inf, 22640.0000,        inf,\n",
      "               inf, 65280.0000,  3764.0000, 32720.0000,  5864.0000,  3488.0000,\n",
      "         8648.0000, 17360.0000, 21792.0000, 12600.0000,        inf, 17552.0000,\n",
      "               inf, 29376.0000, 21856.0000,        inf, 39008.0000,   498.2500,\n",
      "         1114.0000,  6576.0000, 52256.0000, 42848.0000, 47072.0000,  6368.0000,\n",
      "        26304.0000, 26416.0000, 51200.0000, 42368.0000, 13608.0000,        inf,\n",
      "         1699.0000, 24656.0000, 20832.0000, 10080.0000, 32832.0000,        inf,\n",
      "        20192.0000,        inf, 13728.0000,        inf, 52416.0000,        inf,\n",
      "        33728.0000,        inf,  7952.0000, 16528.0000,  7672.0000,        inf,\n",
      "        14880.0000,  6664.0000, 56768.0000, 56736.0000, 21264.0000, 16720.0000,\n",
      "        37056.0000,  1037.0000, 10728.0000,        inf, 52800.0000,        inf,\n",
      "         6040.0000, 49312.0000,        inf, 26816.0000, 12712.0000,        inf,\n",
      "        56224.0000, 11712.0000, 60352.0000,  7992.0000,  5440.0000, 37856.0000,\n",
      "               inf, 19264.0000, 20144.0000,  6472.0000,        inf,  6876.0000,\n",
      "        19600.0000, 45472.0000,  3168.0000, 42752.0000,  5264.0000,        inf,\n",
      "               inf,        inf,        inf,  4960.0000, 21408.0000, 23440.0000,\n",
      "        41216.0000, 43200.0000, 12112.0000, 38880.0000,        inf,        inf,\n",
      "        16256.0000, 26400.0000, 50784.0000, 35072.0000,  5416.0000,   678.5000,\n",
      "               inf,  4776.0000, 32368.0000,  6080.0000,        inf, 10408.0000,\n",
      "               inf,        inf, 47904.0000, 15064.0000, 13024.0000, 19376.0000,\n",
      "               inf, 58848.0000, 20672.0000,        inf, 14640.0000,  2124.0000,\n",
      "        50720.0000, 11512.0000, 25136.0000, 52576.0000, 36544.0000, 26896.0000,\n",
      "               inf,  1461.0000, 11096.0000, 15744.0000,        inf, 36736.0000,\n",
      "        44320.0000, 31440.0000, 47040.0000, 40032.0000,        inf, 26832.0000,\n",
      "               inf,  1401.0000, 17344.0000,        inf,        inf,        inf,\n",
      "         9728.0000,        inf,        inf, 45568.0000, 47104.0000, 44064.0000,\n",
      "        57312.0000, 32032.0000, 37920.0000, 48160.0000, 13760.0000, 26800.0000,\n",
      "        27232.0000,        inf, 45120.0000, 20656.0000, 26128.0000,        inf,\n",
      "        11992.0000, 38848.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 55104.0000, 26064.0000,        inf, 51424.0000,\n",
      "         6576.0000, 46912.0000, 33792.0000, 41696.0000, 33024.0000,        inf,\n",
      "               inf,  9344.0000,        inf,        inf, 44512.0000, 36288.0000,\n",
      "               inf, 25024.0000, 60320.0000, 31360.0000, 28208.0000, 54560.0000,\n",
      "        20640.0000,        inf, 58368.0000, 19024.0000, 25664.0000, 46880.0000,\n",
      "        10544.0000, 18192.0000, 45696.0000, 15096.0000, 19264.0000, 15608.0000,\n",
      "        20160.0000,  3884.0000,        inf,        inf, 63200.0000,        inf,\n",
      "               inf, 10608.0000, 45216.0000,        inf, 34336.0000,  5288.0000,\n",
      "        44384.0000,        inf, 46208.0000,        inf, 32496.0000,        inf,\n",
      "        42752.0000, 26224.0000, 33408.0000, 46656.0000, 34656.0000,  3872.0000,\n",
      "               inf,        inf,        inf, 12824.0000, 18144.0000,  2035.0000,\n",
      "               inf, 22432.0000,        inf,        inf, 27808.0000,        inf,\n",
      "               inf,        inf, 64896.0000, 21024.0000,        inf,        inf,\n",
      "        24240.0000, 49920.0000, 44960.0000, 37952.0000, 28944.0000, 53952.0000,\n",
      "         2600.0000, 30688.0000,        inf, 26768.0000,  2876.0000, 14400.0000,\n",
      "               inf, 52256.0000, 16512.0000,        inf,        inf, 45952.0000,\n",
      "        50528.0000, 56000.0000, 29264.0000,  8112.0000, 44576.0000,        inf,\n",
      "        18096.0000, 33184.0000,        inf, 41792.0000, 22320.0000, 13320.0000,\n",
      "               inf,        inf, 40928.0000,        inf,        inf,        inf,\n",
      "               inf,  6696.0000,  1944.0000, 24576.0000, 24896.0000,  9400.0000,\n",
      "        11168.0000, 64992.0000,        inf,  5360.0000, 13608.0000,        inf,\n",
      "         9992.0000,        inf,  8856.0000,  2270.0000, 39360.0000, 16192.0000,\n",
      "        13888.0000, 40384.0000,        inf,        inf,        inf, 29552.0000,\n",
      "               inf,        inf,        inf, 23888.0000, 59040.0000,        inf,\n",
      "           84.0000,  2256.0000,        inf,  6092.0000, 24096.0000, 15264.0000,\n",
      "               inf,        inf,        inf,        inf, 54784.0000, 34720.0000,\n",
      "               inf, 38592.0000,        inf,  9200.0000,        inf, 19776.0000,\n",
      "         7708.0000,        inf,        inf, 29424.0000, 42176.0000, 59680.0000,\n",
      "        18992.0000, 18992.0000,        inf,        inf, 48000.0000,        inf,\n",
      "        39616.0000, 10000.0000,        inf, 54368.0000,        inf, 44384.0000,\n",
      "        53536.0000, 47136.0000, 32320.0000,        inf,  9712.0000,        inf,\n",
      "               inf,        inf,        inf, 14568.0000,        inf, 42176.0000,\n",
      "        38144.0000, 48320.0000, 56192.0000,        inf,        inf,   987.0000,\n",
      "        62720.0000,        inf, 14024.0000, 30832.0000,  3872.0000,        inf,\n",
      "        32192.0000,        inf, 34080.0000,        inf,        inf, 53248.0000,\n",
      "               inf, 26640.0000], device='cuda:0', dtype=torch.float16), tensor([48704.0000, 40736.0000, 29840.0000,        inf, 31696.0000, 11160.0000,\n",
      "        25008.0000, 15784.0000, 42144.0000,  8616.0000, 23968.0000,        inf,\n",
      "        37664.0000,        inf,        inf,        inf,        inf, 42272.0000,\n",
      "               inf,        inf, 38944.0000, 32384.0000,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 23344.0000,\n",
      "               inf,        inf, 29616.0000, 13104.0000,  5092.0000,        inf,\n",
      "               inf,        inf, 38624.0000, 27216.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 14488.0000,        inf, 25568.0000,\n",
      "               inf,        inf, 16912.0000,        inf,        inf,        inf,\n",
      "        57312.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 16592.0000, 16152.0000,  3982.0000, 24992.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,  7400.0000,\n",
      "               inf,        inf,        inf,        inf, 12312.0000, 35392.0000,\n",
      "               inf, 58624.0000,        inf,        inf, 44416.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 13704.0000,\n",
      "               inf,        inf,        inf,        inf,        inf, 11784.0000,\n",
      "               inf, 48864.0000, 10296.0000,  5216.0000, 39680.0000,        inf,\n",
      "               inf, 34400.0000,        inf, 65024.0000, 30688.0000,        inf,\n",
      "               inf,        inf,        inf,  1913.0000,        inf,        inf,\n",
      "               inf, 35232.0000,        inf,        inf,        inf, 61568.0000,\n",
      "               inf, 17648.0000, 36640.0000,        inf, 28688.0000,        inf,\n",
      "        26400.0000, 45248.0000, 36768.0000,        inf, 25584.0000, 47328.0000,\n",
      "               inf,        inf, 23200.0000,        inf, 53536.0000, 47712.0000,\n",
      "         1577.0000,        inf,        inf, 15696.0000,        inf, 27744.0000,\n",
      "               inf, 19536.0000, 63392.0000,        inf, 41216.0000,        inf,\n",
      "        15184.0000, 43552.0000,        inf,        inf,        inf, 60832.0000,\n",
      "         3836.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        39904.0000,        inf, 55648.0000,  9240.0000,  4984.0000,        inf,\n",
      "               inf,        inf,  6124.0000,        inf,        inf,        inf,\n",
      "               inf,        inf, 29200.0000,        inf, 31536.0000,        inf,\n",
      "               inf, 28160.0000, 45248.0000,        inf, 47168.0000, 61664.0000,\n",
      "        63232.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        30672.0000, 63392.0000,        inf, 14104.0000, 45952.0000,        inf,\n",
      "               inf,  2452.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 41376.0000,        inf,        inf, 20496.0000,\n",
      "        28528.0000,        inf, 26496.0000,        inf,  1892.0000,        inf,\n",
      "               inf,        inf,        inf,        inf, 59552.0000,  4508.0000,\n",
      "               inf,        inf, 21472.0000,        inf,        inf,        inf,\n",
      "        34016.0000,   855.5000,        inf, 41504.0000, 13016.0000,        inf,\n",
      "               inf, 49568.0000, 62304.0000,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 32208.0000, 16448.0000, 46848.0000,\n",
      "               inf,        inf, 20720.0000,        inf,  1944.0000, 12872.0000,\n",
      "               inf,        inf, 38336.0000, 52384.0000, 54176.0000,        inf,\n",
      "         6556.0000,        inf, 32128.0000,        inf,        inf, 56576.0000,\n",
      "               inf,        inf, 59936.0000, 35520.0000,        inf, 52608.0000,\n",
      "               inf, 65088.0000,        inf,        inf,        inf,        inf,\n",
      "        29616.0000,        inf,        inf,  9072.0000, 28384.0000,  2470.0000,\n",
      "        52192.0000,        inf,        inf,        inf,        inf, 50944.0000,\n",
      "        17488.0000,        inf, 50848.0000, 16832.0000, 29936.0000,        inf,\n",
      "         3196.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf, 37248.0000,\n",
      "         4284.0000, 30704.0000, 58656.0000, 24704.0000,        inf,        inf,\n",
      "               inf,        inf, 47264.0000, 57920.0000,  8904.0000, 60256.0000,\n",
      "               inf, 18368.0000, 12952.0000, 52640.0000, 41984.0000,        inf,\n",
      "               inf,        inf, 20752.0000, 29728.0000, 61344.0000,        inf,\n",
      "        24384.0000, 29312.0000, 21264.0000,        inf, 34848.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 27056.0000,        inf,        inf,        inf,        inf,\n",
      "        44640.0000,        inf, 48256.0000,        inf,        inf,        inf,\n",
      "        61792.0000, 11576.0000, 16096.0000,        inf, 26784.0000,  1238.0000,\n",
      "               inf,        inf,        inf,  5640.0000, 53792.0000,  8056.0000,\n",
      "               inf, 47296.0000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf,        inf, 33312.0000,        inf,        inf,\n",
      "        34528.0000,        inf, 42080.0000,        inf, 61120.0000, 31200.0000,\n",
      "          573.5000,        inf,        inf, 61536.0000,  7296.0000, 16640.0000,\n",
      "               inf, 44640.0000,  2324.0000, 60512.0000,        inf, 33760.0000,\n",
      "        26704.0000, 53280.0000,        inf, 43776.0000,        inf,        inf,\n",
      "        18496.0000, 27296.0000,        inf, 33792.0000, 26416.0000,  6576.0000,\n",
      "               inf,        inf, 45024.0000,        inf,        inf,        inf,\n",
      "               inf, 24448.0000,  5280.0000, 42304.0000, 29392.0000, 13688.0000,\n",
      "          701.5000,        inf,        inf,  7004.0000,  8848.0000,        inf,\n",
      "        20112.0000,        inf, 14104.0000, 57696.0000, 51136.0000,  5596.0000,\n",
      "        41568.0000, 37472.0000,        inf,        inf,        inf, 44352.0000,\n",
      "               inf,        inf,        inf, 40896.0000,        inf,        inf,\n",
      "        24512.0000, 49984.0000,        inf,        inf, 41216.0000, 46528.0000,\n",
      "               inf,        inf,        inf,        inf, 33984.0000, 56928.0000,\n",
      "               inf,        inf,        inf, 12464.0000, 40448.0000, 40512.0000,\n",
      "        15032.0000,        inf, 64000.0000, 42592.0000, 58112.0000, 59360.0000,\n",
      "        51648.0000, 14408.0000,        inf,        inf, 48832.0000,        inf,\n",
      "        55488.0000,  6152.0000,        inf,        inf,        inf, 59296.0000,\n",
      "        41568.0000, 45312.0000, 45792.0000,        inf, 26960.0000,        inf,\n",
      "        52864.0000,        inf,        inf, 48704.0000,        inf, 40256.0000,\n",
      "        35744.0000,        inf, 50592.0000, 36448.0000,        inf,        inf,\n",
      "               inf,        inf, 53216.0000, 20768.0000, 21616.0000,        inf,\n",
      "        51872.0000,        inf,        inf,        inf, 58720.0000, 61568.0000,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([[ 2096.0000,  3294.0000,  1436.0000,  ...,   713.0000,   742.5000,\n",
      "          1549.0000],\n",
      "        [ 3748.0000,   861.5000,   293.0000,  ...,  4800.0000,  3540.0000,\n",
      "          1587.0000],\n",
      "        [ 4964.0000,  3546.0000,   559.5000,  ...,  1802.0000,  1228.0000,\n",
      "          2270.0000],\n",
      "        ...,\n",
      "        [       inf, 12272.0000,        inf,  ...,        inf,        inf,\n",
      "         23696.0000],\n",
      "        [ 8416.0000,  4732.0000, 26432.0000,  ..., 12592.0000, 30624.0000,\n",
      "          1221.0000],\n",
      "        [       inf, 11384.0000, 55232.0000,  ...,        inf,        inf,\n",
      "          2568.0000]], device='cuda:0', dtype=torch.float16), tensor([  680.5000,  4792.0000,  2164.0000,  ...,        inf, 29520.0000,\n",
      "               inf], device='cuda:0', dtype=torch.float16), tensor([[26880., 26112.,  2128.,  ..., 36352., 27296., 18560.],\n",
      "        [ 1976., 15408., 32272.,  ...,    80., 52128.,  8148.],\n",
      "        [41696.,  6604., 22656.,  ..., 43808., 23168., 12008.],\n",
      "        ...,\n",
      "        [49312., 14800., 25488.,  ..., 51904., 27584., 22176.],\n",
      "        [ 4168.,    inf,    inf,  ..., 34112.,    inf, 21360.],\n",
      "        [55392., 47104.,    inf,  ..., 26448.,    inf, 31600.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([1.5392e+04, 5.1936e+04, 1.1376e+04,        inf,        inf, 7.8200e+03,\n",
      "        3.2192e+04, 7.2160e+03, 3.6544e+04,        inf, 3.2080e+04,        inf,\n",
      "               inf, 4.6144e+04, 9.6160e+03,        inf,        inf,        inf,\n",
      "               inf,        inf, 6.6040e+03, 2.2240e+04,        inf, 3.6320e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 7.5080e+03,\n",
      "               inf, 2.7424e+04, 5.5840e+03, 1.7584e+04, 5.1500e+02,        inf,\n",
      "               inf,        inf, 3.3360e+03, 9.4640e+03,        inf,        inf,\n",
      "               inf,        inf, 6.3936e+04, 4.6656e+04, 6.4224e+04, 3.8912e+04,\n",
      "               inf,        inf, 2.4416e+04,        inf, 2.0256e+04,        inf,\n",
      "        5.7472e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 4.0448e+04, 3.5168e+04, 3.9936e+04, 2.5900e+03,        inf,\n",
      "               inf, 3.3728e+04, 3.8400e+04,        inf, 6.4448e+04, 2.1072e+04,\n",
      "        3.2768e+04,        inf,        inf,        inf, 2.1232e+04, 3.0176e+04,\n",
      "               inf, 4.8512e+04,        inf,        inf,        inf,        inf,\n",
      "        6.2720e+04, 3.8080e+04, 2.3072e+04,        inf, 6.4544e+04, 4.7360e+03,\n",
      "        4.2176e+04,        inf,        inf,        inf,        inf, 3.4368e+04,\n",
      "        5.6672e+04, 5.6128e+04, 3.4112e+04, 8.1160e+03, 5.4520e+03,        inf,\n",
      "               inf,        inf,        inf, 3.5808e+04, 2.5568e+04,        inf,\n",
      "               inf, 4.8800e+04,        inf, 1.9552e+04,        inf,        inf,\n",
      "        3.2160e+04, 1.6208e+04,        inf,        inf, 9.9280e+03,        inf,\n",
      "        4.3264e+04, 1.2528e+04, 1.1368e+04, 6.4800e+04, 3.6060e+03,        inf,\n",
      "        8.1840e+03, 3.8496e+04, 1.8384e+04,        inf, 5.1616e+04,        inf,\n",
      "               inf, 2.4272e+04, 3.8496e+04,        inf, 5.0275e+02, 3.7120e+04,\n",
      "        2.9475e+02,        inf,        inf, 3.3440e+04,        inf, 2.5280e+04,\n",
      "               inf,        inf, 9.2080e+03, 9.3920e+03, 3.7824e+04, 6.3104e+04,\n",
      "        4.8960e+03, 2.8352e+04, 5.6576e+04,        inf, 9.1360e+03,        inf,\n",
      "        3.2256e+04, 6.1984e+04,        inf, 4.7296e+04, 4.3968e+04, 4.5088e+04,\n",
      "               inf,        inf,        inf, 1.7568e+04, 9.0560e+03,        inf,\n",
      "               inf,        inf, 5.9200e+04,        inf,        inf,        inf,\n",
      "        3.8848e+04,        inf, 5.4976e+04,        inf, 6.3584e+04,        inf,\n",
      "               inf, 5.4272e+04, 3.9680e+04,        inf,        inf,        inf,\n",
      "               inf, 6.0096e+04, 3.1536e+04,        inf, 5.9168e+04,        inf,\n",
      "        9.3760e+03, 5.9200e+04,        inf, 3.5168e+04,        inf,        inf,\n",
      "               inf, 4.4608e+04, 6.0320e+04,        inf, 2.8624e+04, 6.2624e+04,\n",
      "               inf, 4.0096e+04, 2.9216e+04, 3.9200e+04, 8.2320e+03, 1.0808e+04,\n",
      "        6.1344e+04,        inf, 1.2224e+04,        inf, 4.8896e+04, 5.1232e+04,\n",
      "        5.5648e+04,        inf,        inf,        inf, 6.2880e+04, 2.8400e+03,\n",
      "        4.6432e+04,        inf, 2.2560e+04,        inf,        inf,        inf,\n",
      "        4.8064e+04, 1.3000e+03, 5.0272e+04,        inf, 1.7712e+04, 6.3936e+04,\n",
      "               inf, 2.4912e+04,        inf, 3.3568e+04,        inf,        inf,\n",
      "               inf, 3.2336e+04,        inf, 5.3320e+03, 1.9888e+04, 3.1904e+04,\n",
      "               inf,        inf,        inf, 2.3344e+04, 2.6544e+04, 1.9776e+04,\n",
      "        5.9104e+04,        inf, 2.6432e+04,        inf, 5.6448e+04,        inf,\n",
      "        5.5040e+04,        inf, 2.3580e+03,        inf,        inf, 3.1040e+04,\n",
      "               inf, 6.0352e+04, 5.1104e+04, 4.5632e+04, 2.0016e+04,        inf,\n",
      "               inf, 1.8784e+04,        inf,        inf,        inf,        inf,\n",
      "        6.3320e+03, 6.4224e+04,        inf, 1.4616e+04,        inf, 4.9184e+04,\n",
      "        4.0384e+04,        inf, 3.2240e+04, 3.9328e+04, 5.7440e+04, 2.5216e+04,\n",
      "        1.8128e+04,        inf,        inf, 7.6850e+02, 2.1200e+04,        inf,\n",
      "        3.9424e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "        4.4928e+04, 4.2496e+04, 4.5664e+04, 6.2848e+04,        inf, 5.2384e+04,\n",
      "               inf, 1.2392e+04, 7.8600e+03, 3.5168e+04, 1.3800e+04,        inf,\n",
      "               inf,        inf, 2.9824e+04,        inf,        inf, 4.0608e+04,\n",
      "               inf,        inf, 9.9120e+03, 3.4656e+04, 5.6360e+03,        inf,\n",
      "               inf, 5.5584e+04, 3.8944e+04, 2.0240e+04, 4.3648e+04, 1.6944e+04,\n",
      "        2.1488e+04, 3.9520e+04, 2.9344e+04,        inf,        inf,        inf,\n",
      "        3.8816e+04,        inf,        inf,        inf,        inf, 5.6608e+04,\n",
      "               inf, 2.2580e+03, 1.9856e+04, 5.2512e+04,        inf,        inf,\n",
      "               inf,        inf, 3.1232e+04,        inf, 5.6969e+01,        inf,\n",
      "        2.0320e+04, 1.5880e+04, 6.7650e+02,        inf, 3.6224e+04, 8.4080e+03,\n",
      "        2.2368e+04,        inf, 3.5232e+04, 3.6760e+03, 3.8944e+04, 4.7840e+04,\n",
      "               inf, 1.5640e+04,        inf,        inf,        inf,        inf,\n",
      "        4.2112e+04,        inf,        inf, 1.5744e+04,        inf,        inf,\n",
      "        5.6960e+03,        inf, 1.0776e+04, 3.6448e+04,        inf, 6.7000e+02,\n",
      "               inf,        inf,        inf,        inf, 1.5256e+04, 4.1960e+03,\n",
      "               inf, 4.6496e+04, 1.2760e+04, 2.0288e+04,        inf, 3.7312e+04,\n",
      "        3.9360e+04, 5.1320e+03,        inf, 4.9664e+04,        inf,        inf,\n",
      "        1.0870e+03, 2.5104e+04, 4.1440e+04, 6.2560e+04, 4.2496e+04, 9.6700e+02,\n",
      "        6.1504e+04,        inf, 2.9360e+04,        inf,        inf,        inf,\n",
      "               inf,        inf, 2.2096e+04, 4.5600e+04, 5.3760e+03, 4.4128e+04,\n",
      "        3.1920e+04,        inf,        inf, 1.1696e+04, 3.2800e+04,        inf,\n",
      "        2.8288e+04,        inf, 1.3736e+04,        inf, 1.4336e+04, 2.7904e+04,\n",
      "               inf, 3.2864e+04,        inf,        inf,        inf, 3.6000e+04,\n",
      "               inf, 4.8768e+04,        inf, 5.3600e+04, 3.2288e+04, 5.4464e+04,\n",
      "               inf, 2.9152e+04,        inf, 3.3536e+04, 2.9328e+04, 9.3920e+03,\n",
      "               inf,        inf,        inf,        inf,        inf, 1.5632e+04,\n",
      "               inf,        inf,        inf, 3.2032e+04, 5.2560e+03, 6.4896e+04,\n",
      "        3.6608e+04,        inf,        inf, 5.2224e+04,        inf, 4.9152e+04,\n",
      "        2.0096e+04, 2.3008e+04,        inf,        inf, 4.5920e+04,        inf,\n",
      "               inf,        inf,        inf, 3.1088e+04, 6.5312e+04,        inf,\n",
      "               inf,        inf, 2.0192e+04,        inf, 1.9760e+04,        inf,\n",
      "        3.6512e+04,        inf,        inf,        inf,        inf, 5.4208e+04,\n",
      "        1.9808e+04, 5.7280e+04, 2.9136e+04, 2.2176e+04,        inf, 5.5720e+03,\n",
      "               inf,        inf, 3.4656e+04, 6.1080e+03, 7.4500e+02, 3.4720e+04,\n",
      "        4.1450e+02,        inf,        inf,        inf, 4.3712e+04, 3.2288e+04,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([[5.1650e+02, 1.2440e+03, 9.7000e+01,  ..., 6.3050e+02, 2.7600e+03,\n",
      "         2.2175e+02],\n",
      "        [6.8080e+03, 1.1562e+01, 3.9760e+03,  ..., 8.8000e+02, 3.2880e+03,\n",
      "         3.2740e+03],\n",
      "        [4.2280e+03, 5.1550e+02, 4.6600e+03,  ..., 3.4580e+03, 4.9880e+03,\n",
      "         2.8820e+03],\n",
      "        ...,\n",
      "        [4.2048e+04, 4.3296e+04, 1.5280e+04,  ...,        inf, 5.8240e+03,\n",
      "         1.6320e+03],\n",
      "        [2.6800e+03, 3.6864e+04, 7.2720e+03,  ..., 6.2840e+03, 3.6736e+04,\n",
      "         2.1728e+04],\n",
      "        [2.8816e+04, 3.0800e+04, 1.1200e+03,  ..., 2.6368e+04,        inf,\n",
      "         3.2528e+04]], device='cuda:0', dtype=torch.float16), tensor([ 1159.,  3574.,  5308.,  ..., 37152., 27664.,    inf], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[33280.,  2470., 57504.,  ...,    inf, 41568., 23232.],\n",
      "        [60032., 10192., 20000.,  ..., 26992., 30960., 52128.],\n",
      "        [27744.,  3404., 46496.,  ...,    inf, 36256., 21616.],\n",
      "        ...,\n",
      "        [53632.,  3966.,    inf,  ...,    inf, 63808., 51648.],\n",
      "        [   inf, 14776.,  7952.,  ...,    inf, 28448.,    inf],\n",
      "        [   inf,  7028.,    inf,  ...,    nan,    inf,    inf]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([2.2240e+04, 5.9296e+04, 1.7376e+04, 6.2272e+04,        inf, 2.9740e+03,\n",
      "        3.0672e+04, 1.1424e+04, 3.0240e+04,        inf, 2.4032e+04,        inf,\n",
      "               inf, 4.5856e+04, 2.0704e+04,        inf,        inf,        inf,\n",
      "               inf,        inf, 1.6768e+04, 1.3304e+04,        inf, 4.5856e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 3.4400e+03,\n",
      "               inf, 1.9504e+04, 6.8280e+03, 2.0144e+04, 6.3360e+03,        inf,\n",
      "               inf,        inf, 3.1700e+03, 1.0512e+04,        inf,        inf,\n",
      "               inf,        inf,        inf, 4.7904e+04,        inf, 3.0368e+04,\n",
      "               inf,        inf, 3.3280e+04,        inf, 4.0480e+04,        inf,\n",
      "        5.7824e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 5.7152e+04, 4.7136e+04, 4.6688e+04, 1.2648e+04,        inf,\n",
      "               inf, 4.4640e+04, 4.4384e+04,        inf,        inf, 2.6864e+04,\n",
      "        1.8832e+04,        inf,        inf,        inf, 8.7680e+03, 1.3280e+04,\n",
      "               inf, 6.5504e+04,        inf,        inf,        inf,        inf,\n",
      "               inf, 2.1728e+04, 4.6976e+04,        inf, 5.7024e+04, 3.9360e+03,\n",
      "        4.2656e+04,        inf,        inf,        inf,        inf, 4.4928e+04,\n",
      "               inf, 5.3952e+04, 3.0320e+04, 9.1680e+03, 5.7440e+03,        inf,\n",
      "               inf,        inf,        inf, 4.6880e+04, 3.1712e+04,        inf,\n",
      "               inf, 4.2432e+04,        inf, 2.9696e+04,        inf,        inf,\n",
      "        3.5104e+04, 9.2960e+03,        inf,        inf, 1.8080e+04,        inf,\n",
      "        4.8064e+04, 2.3472e+04, 1.0130e+03,        inf, 1.5200e+04,        inf,\n",
      "        1.7104e+04, 3.3120e+04, 2.8512e+04,        inf, 5.1424e+04,        inf,\n",
      "               inf, 2.8608e+04, 2.5296e+04,        inf, 2.9100e+03, 3.2208e+04,\n",
      "        1.4280e+04,        inf,        inf, 4.2656e+04,        inf, 2.9120e+04,\n",
      "               inf,        inf, 7.7760e+03, 2.8360e+03, 4.1024e+04, 6.2944e+04,\n",
      "        5.5160e+03, 2.0880e+04,        inf,        inf, 2.2040e+03,        inf,\n",
      "        3.1056e+04,        inf,        inf, 4.5792e+04, 5.0368e+04,        inf,\n",
      "               inf,        inf,        inf, 9.7440e+03, 1.5936e+04,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "        5.2224e+04,        inf, 5.6544e+04,        inf,        inf,        inf,\n",
      "        6.4064e+04,        inf, 4.5664e+04,        inf,        inf,        inf,\n",
      "               inf,        inf, 3.8880e+04,        inf, 4.8544e+04,        inf,\n",
      "        6.3680e+03,        inf,        inf, 2.8512e+04,        inf,        inf,\n",
      "               inf, 5.1552e+04,        inf,        inf, 3.7216e+04,        inf,\n",
      "               inf, 4.7584e+04, 1.6896e+04, 4.8000e+04, 2.1000e+03, 1.4464e+04,\n",
      "        5.3664e+04,        inf, 2.7300e+03,        inf, 6.2208e+04, 5.4112e+04,\n",
      "        4.3168e+04,        inf,        inf, 6.4640e+04,        inf, 1.8740e+03,\n",
      "        3.9136e+04,        inf, 1.3272e+04,        inf,        inf,        inf,\n",
      "        4.9984e+04, 2.8160e+04, 4.5536e+04, 6.0768e+04, 1.5800e+04,        inf,\n",
      "               inf, 1.8592e+04, 6.0832e+04, 2.9824e+04,        inf,        inf,\n",
      "               inf, 2.4048e+04,        inf, 1.2168e+04, 1.7344e+04, 3.2688e+04,\n",
      "               inf,        inf,        inf, 3.0224e+04, 3.0512e+04, 4.4240e+03,\n",
      "               inf,        inf, 2.3968e+04,        inf, 4.3968e+04,        inf,\n",
      "        5.2384e+04,        inf, 1.2936e+04,        inf,        inf, 5.3216e+04,\n",
      "               inf, 6.3392e+04, 5.2672e+04, 5.4048e+04, 1.6656e+04,        inf,\n",
      "               inf, 2.8320e+04,        inf,        inf,        inf,        inf,\n",
      "        7.2840e+03,        inf,        inf, 2.4928e+04,        inf, 6.3520e+04,\n",
      "        4.8832e+04, 6.3712e+04, 5.2320e+04, 5.4880e+04,        inf, 2.9456e+04,\n",
      "        2.1696e+04,        inf, 6.4864e+04, 2.2140e+03, 3.2960e+04,        inf,\n",
      "        1.5016e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "        5.4176e+04, 4.2848e+04, 3.9872e+04, 6.5440e+04,        inf, 5.1456e+04,\n",
      "               inf, 4.2720e+03, 3.5850e+02, 4.6912e+04, 1.2456e+04,        inf,\n",
      "               inf, 5.2736e+04, 3.5584e+04,        inf,        inf, 3.6256e+04,\n",
      "               inf, 5.4848e+04, 1.6400e+04, 3.5360e+04, 3.2920e+03,        inf,\n",
      "               inf, 4.5504e+04, 4.6432e+04, 2.9568e+04, 5.5456e+04, 1.3648e+04,\n",
      "        2.7104e+04, 2.7760e+04, 2.6320e+04,        inf,        inf,        inf,\n",
      "        2.8736e+04,        inf,        inf,        inf,        inf, 6.1280e+04,\n",
      "               inf, 1.6864e+04, 2.1712e+04, 5.6608e+04,        inf,        inf,\n",
      "               inf,        inf, 2.6144e+04,        inf, 8.5760e+03,        inf,\n",
      "        2.9680e+04, 4.1760e+04, 2.1520e+03,        inf, 3.5648e+04, 1.3232e+04,\n",
      "        3.3504e+04,        inf, 3.8272e+04, 4.9120e+03, 3.6608e+04, 3.8496e+04,\n",
      "               inf, 1.1952e+04,        inf,        inf,        inf,        inf,\n",
      "        4.5504e+04,        inf,        inf, 1.7248e+04,        inf, 5.7312e+04,\n",
      "        1.8800e+04,        inf, 6.3640e+03, 4.7584e+04,        inf, 1.0208e+04,\n",
      "               inf,        inf,        inf,        inf, 1.5930e+03, 5.3094e+01,\n",
      "               inf, 5.9424e+04, 1.1104e+04, 9.0080e+03,        inf, 5.2544e+04,\n",
      "        5.9232e+04, 6.6720e+03,        inf, 5.1296e+04,        inf,        inf,\n",
      "        1.0656e+04, 1.5696e+04, 5.4816e+04,        inf, 3.5136e+04, 2.6980e+03,\n",
      "        6.1888e+04,        inf, 1.3512e+04,        inf,        inf,        inf,\n",
      "               inf,        inf, 2.9504e+04, 5.5712e+04, 1.4690e+03, 4.4128e+04,\n",
      "        3.9488e+04,        inf,        inf, 3.1200e+04, 4.5376e+04,        inf,\n",
      "        3.8016e+04,        inf, 1.7488e+04,        inf, 9.4080e+03, 2.0896e+04,\n",
      "               inf, 6.8920e+03,        inf,        inf,        inf, 2.9120e+04,\n",
      "               inf, 4.2848e+04,        inf, 5.1744e+04, 3.5712e+04, 5.5424e+04,\n",
      "        6.1088e+04, 3.1200e+04,        inf, 4.5504e+04, 2.8960e+04, 1.7952e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 1.8448e+04,\n",
      "               inf,        inf,        inf, 4.1088e+04, 4.7560e+03, 6.5472e+04,\n",
      "        3.5968e+04,        inf,        inf, 4.6688e+04,        inf, 3.3600e+04,\n",
      "        1.6352e+04, 2.9536e+04,        inf,        inf, 4.0832e+04,        inf,\n",
      "               inf,        inf,        inf, 2.9936e+04, 6.0288e+04,        inf,\n",
      "               inf,        inf, 3.5104e+04,        inf, 2.0240e+04,        inf,\n",
      "        4.4448e+04,        inf,        inf, 6.3264e+04,        inf, 6.4992e+04,\n",
      "        2.1328e+04, 5.7408e+04, 3.3728e+04, 2.7024e+04,        inf, 2.5160e+03,\n",
      "               inf,        inf, 4.4544e+04, 3.9420e+03, 5.8040e+03, 2.9552e+04,\n",
      "        4.1880e+03,        inf,        inf,        inf, 4.1344e+04, 3.8816e+04,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([[13904., 12576.,  1867.,  ..., 11496., 26912.,  5236.],\n",
      "        [12344.,  8592.,  5224.,  ..., 31088.,  5896., 16608.],\n",
      "        [36704.,  3632., 42496.,  ..., 22176., 30464., 28032.],\n",
      "        ...,\n",
      "        [44608.,  1544.,    inf,  ..., 12720., 58144., 40480.],\n",
      "        [29536., 10800., 26832.,  ..., 47904., 22576., 17104.],\n",
      "        [  954.,  1785.,  3810.,  ...,   670.,  5128.,  5944.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([23488.,  9760., 41472.,  ...,    inf,  1336.,  1441.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[14872., 18496., 20736.,  ..., 11784.,  6488.,  4252.],\n",
      "        [22864.,  4064.,  1437.,  ..., 10536.,  4540.,  6312.],\n",
      "        [44032., 24512., 14976.,  ...,  8856., 21408., 18752.],\n",
      "        ...,\n",
      "        [28000., 23408., 20208.,  ..., 56576., 55808., 25664.],\n",
      "        [ 8416., 54176., 53760.,  ..., 21664., 52992., 12720.],\n",
      "        [30736., 14944., 24096.,  ...,    inf,  7504., 13088.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([ 9920.0000, 28848.0000, 45824.0000,        inf,        inf,  4002.0000,\n",
      "         2468.0000, 30880.0000, 51008.0000,        inf, 15472.0000,        inf,\n",
      "        45312.0000,  3060.0000, 21600.0000,        inf,        inf, 59040.0000,\n",
      "        56288.0000,        inf, 49408.0000,        inf,        inf, 46240.0000,\n",
      "               inf,        inf, 56288.0000,        inf,        inf, 59872.0000,\n",
      "               inf, 17280.0000, 13880.0000,  2208.0000,  7908.0000,        inf,\n",
      "               inf, 26816.0000, 44416.0000, 27424.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 47200.0000, 54016.0000, 10872.0000,\n",
      "               inf,        inf, 35456.0000,        inf,  6720.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 37792.0000,        inf, 17056.0000,  3096.0000,        inf,\n",
      "               inf,        inf, 29232.0000, 57504.0000, 48384.0000,        inf,\n",
      "        33696.0000,        inf,        inf,        inf, 15912.0000, 22976.0000,\n",
      "               inf, 45600.0000,        inf,        inf, 40416.0000,        inf,\n",
      "               inf, 48352.0000, 22256.0000,        inf, 59392.0000, 14184.0000,\n",
      "               inf,        inf, 29072.0000,        inf,        inf, 23904.0000,\n",
      "        38400.0000, 13416.0000,        inf,  9936.0000,  6456.0000,        inf,\n",
      "               inf, 63136.0000, 39168.0000, 10560.0000, 25760.0000,        inf,\n",
      "               inf, 58752.0000, 50624.0000, 38272.0000,        inf,        inf,\n",
      "        41792.0000, 41344.0000,        inf,        inf, 24592.0000,        inf,\n",
      "        39936.0000, 14192.0000,  4460.0000, 65056.0000, 21456.0000,        inf,\n",
      "         1356.0000, 23328.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 24048.0000,  2684.0000,        inf, 49120.0000, 36256.0000,\n",
      "        49696.0000, 15296.0000,        inf, 54624.0000,        inf,        inf,\n",
      "               inf,        inf,  5388.0000, 32096.0000, 12072.0000, 58112.0000,\n",
      "        23168.0000, 15512.0000, 19888.0000,        inf, 27072.0000,        inf,\n",
      "        20032.0000, 34176.0000,        inf,        inf,  4484.0000, 41056.0000,\n",
      "               inf,        inf,        inf,        inf, 56320.0000,        inf,\n",
      "               inf,        inf, 40672.0000,        inf,        inf,        inf,\n",
      "        16704.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        16528.0000, 52064.0000, 36352.0000, 56320.0000,        inf, 62816.0000,\n",
      "               inf, 65152.0000, 27088.0000,        inf, 34688.0000,        inf,\n",
      "        35872.0000, 52000.0000,        inf, 12640.0000,        inf, 61344.0000,\n",
      "        17520.0000, 13968.0000, 42944.0000,        inf,   412.7500,        inf,\n",
      "               inf, 62848.0000, 20272.0000, 42912.0000, 12960.0000,  4588.0000,\n",
      "        26160.0000,        inf, 43744.0000,        inf, 37728.0000, 61696.0000,\n",
      "        48384.0000,        inf,        inf,        inf, 27216.0000,   941.0000,\n",
      "        64544.0000,        inf, 18448.0000,        inf, 49920.0000,        inf,\n",
      "        45568.0000, 40288.0000, 42784.0000, 43232.0000, 19824.0000, 45824.0000,\n",
      "               inf,        inf,        inf, 38400.0000, 62624.0000,        inf,\n",
      "               inf, 20624.0000,        inf,   624.0000, 17472.0000, 60928.0000,\n",
      "               inf,        inf,        inf, 46080.0000, 11416.0000,  7584.0000,\n",
      "               inf,        inf, 27664.0000,        inf,        inf,        inf,\n",
      "        19456.0000,        inf, 48544.0000,        inf,        inf, 21776.0000,\n",
      "               inf,        inf, 53888.0000, 45792.0000, 41536.0000,        inf,\n",
      "               inf,  5060.0000,        inf,        inf,        inf,   303.0000,\n",
      "         7888.0000, 54016.0000,        inf,  8408.0000,        inf, 45056.0000,\n",
      "        53568.0000, 44704.0000,  8616.0000, 45792.0000, 34944.0000, 47904.0000,\n",
      "         2380.0000,        inf, 62752.0000, 38016.0000, 64288.0000,        inf,\n",
      "        11832.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        35840.0000, 33696.0000, 53152.0000, 40704.0000, 13696.0000,        inf,\n",
      "               inf, 42656.0000, 37120.0000,        inf, 40032.0000,        inf,\n",
      "               inf, 35584.0000, 39360.0000,        inf,        inf,  6476.0000,\n",
      "        64064.0000, 53696.0000, 24784.0000,  3520.0000, 30368.0000,        inf,\n",
      "               inf,        inf,        inf, 11464.0000, 29952.0000, 30816.0000,\n",
      "         6692.0000, 42560.0000,  9944.0000, 64672.0000,        inf,        inf,\n",
      "         2616.0000,        inf,        inf,        inf,        inf, 41248.0000,\n",
      "               inf, 24240.0000, 33824.0000,        inf, 43424.0000,        inf,\n",
      "               inf,        inf, 34848.0000,        inf,  4460.0000,        inf,\n",
      "        12432.0000,        inf, 40864.0000,        inf,   933.5000, 10672.0000,\n",
      "         1266.0000, 52256.0000,  3516.0000, 29408.0000, 45504.0000,        inf,\n",
      "               inf, 55200.0000,        inf,        inf,        inf,        inf,\n",
      "        58784.0000,        inf, 51264.0000, 25712.0000,        inf, 50912.0000,\n",
      "        22880.0000,        inf, 22720.0000,        inf, 36096.0000,  5808.0000,\n",
      "               inf, 30208.0000,        inf,        inf, 55264.0000, 14696.0000,\n",
      "               inf, 50048.0000, 26672.0000, 12544.0000,        inf, 56032.0000,\n",
      "        15352.0000, 13944.0000,        inf,        inf,        inf,        inf,\n",
      "         3498.0000, 10736.0000, 61056.0000,        inf,  6956.0000,  6404.0000,\n",
      "        24672.0000,        inf,   595.0000,        inf,        inf,        inf,\n",
      "               inf, 53024.0000,  3380.0000, 34912.0000, 10808.0000, 21136.0000,\n",
      "        37600.0000,        inf, 64480.0000, 40160.0000,        inf,        inf,\n",
      "         3636.0000,        inf, 24176.0000,        inf, 11024.0000, 36288.0000,\n",
      "               inf,  1820.0000,        inf, 62240.0000,        inf, 54816.0000,\n",
      "               inf, 22560.0000,        inf, 19696.0000, 51584.0000, 51328.0000,\n",
      "               inf, 61184.0000,        inf, 40064.0000, 24816.0000, 19856.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,  8328.0000,\n",
      "               inf,        inf,        inf, 18240.0000, 12888.0000, 34336.0000,\n",
      "        25072.0000,        inf, 57664.0000, 47168.0000,        inf, 19776.0000,\n",
      "         1594.0000, 10584.0000,        inf, 34848.0000, 55776.0000, 30704.0000,\n",
      "        64736.0000, 22416.0000,        inf,   329.2500, 50304.0000,        inf,\n",
      "               inf,        inf,        inf,        inf, 61760.0000,        inf,\n",
      "        60800.0000,        inf,        inf, 34656.0000, 22464.0000,        inf,\n",
      "        45536.0000, 10160.0000, 11288.0000,        inf,        inf,  1753.0000,\n",
      "               inf,        inf, 23520.0000, 29632.0000, 38816.0000, 12176.0000,\n",
      "        35264.0000,        inf, 57920.0000,        inf, 45984.0000, 44992.0000,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([ 1994.0000, 35328.0000, 10144.0000, 51328.0000, 11680.0000, 23712.0000,\n",
      "         7444.0000, 31360.0000, 29920.0000, 12096.0000, 49600.0000, 51680.0000,\n",
      "        18944.0000, 44320.0000,  6348.0000, 22480.0000,        inf, 57792.0000,\n",
      "        63552.0000, 48992.0000,  1819.0000, 12160.0000, 45856.0000, 37600.0000,\n",
      "        48416.0000,        inf, 53728.0000,        inf,        inf,  5012.0000,\n",
      "               inf, 24560.0000, 32688.0000, 19808.0000, 16912.0000, 24848.0000,\n",
      "               inf, 37440.0000, 12592.0000, 24544.0000, 56576.0000, 63136.0000,\n",
      "               inf, 34144.0000,  3820.0000, 12968.0000,  5864.0000, 26272.0000,\n",
      "        53632.0000, 39936.0000, 23472.0000, 31024.0000, 31168.0000, 12632.0000,\n",
      "        58816.0000, 27872.0000, 26624.0000, 53376.0000, 28592.0000, 46080.0000,\n",
      "         1413.0000, 10640.0000, 27920.0000,  6120.0000,  4552.0000,        inf,\n",
      "        35424.0000, 22464.0000, 33216.0000, 27504.0000,  5584.0000, 38496.0000,\n",
      "        27120.0000, 10792.0000,  6116.0000,        inf,  4784.0000, 56064.0000,\n",
      "        15944.0000, 49728.0000,        inf, 36352.0000,        inf, 10824.0000,\n",
      "        29632.0000, 14952.0000,  7396.0000, 54720.0000,  4124.0000, 14168.0000,\n",
      "        52256.0000, 47616.0000, 62496.0000, 38624.0000, 17280.0000,  5204.0000,\n",
      "        19808.0000, 18224.0000, 50112.0000,  7688.0000, 25440.0000, 23232.0000,\n",
      "               inf, 32304.0000, 22720.0000,  7808.0000,  9240.0000,        inf,\n",
      "               inf, 17680.0000, 23952.0000, 11480.0000,        inf,        inf,\n",
      "        18208.0000, 10616.0000, 23696.0000, 12168.0000, 17392.0000,        inf,\n",
      "        11208.0000, 38400.0000,  3160.0000, 43904.0000,  5796.0000,        inf,\n",
      "        23952.0000, 11712.0000,  7832.0000, 29088.0000,  9248.0000,        inf,\n",
      "        57536.0000,  3360.0000,  2746.0000,        inf, 10920.0000,  9208.0000,\n",
      "         2620.0000, 30528.0000, 30848.0000, 23584.0000, 42656.0000,  9776.0000,\n",
      "        38304.0000,  3932.0000, 14072.0000,  2506.0000, 35296.0000,  4052.0000,\n",
      "         2650.0000, 21616.0000, 32048.0000, 59488.0000,  6220.0000,  6984.0000,\n",
      "        21728.0000, 24272.0000, 33376.0000, 32800.0000,  6648.0000, 17120.0000,\n",
      "         3190.0000,        inf, 43328.0000,  6508.0000, 27296.0000,        inf,\n",
      "               inf,        inf,   175.5000,        inf, 38048.0000,        inf,\n",
      "        25440.0000,        inf, 11464.0000, 10800.0000, 26128.0000,        inf,\n",
      "        33568.0000, 14864.0000, 64864.0000,        inf,        inf, 25808.0000,\n",
      "        51424.0000,  8176.0000,  3164.0000, 39136.0000, 47936.0000,        inf,\n",
      "        29904.0000, 55680.0000, 56000.0000,  3844.0000, 26432.0000,        inf,\n",
      "        41952.0000, 43424.0000, 49184.0000,  6996.0000, 17568.0000,  9728.0000,\n",
      "        36768.0000, 18032.0000, 12224.0000, 17888.0000, 11856.0000, 14736.0000,\n",
      "         2372.0000,        inf,  5988.0000, 25168.0000,        inf, 32864.0000,\n",
      "        60256.0000,        inf,        inf, 18640.0000, 35232.0000, 11512.0000,\n",
      "        35456.0000,  4844.0000, 20304.0000, 17312.0000,        inf,        inf,\n",
      "        15168.0000, 17504.0000, 19616.0000, 47200.0000,  4588.0000, 21312.0000,\n",
      "               inf,  4764.0000, 33344.0000, 15864.0000,        inf, 27328.0000,\n",
      "        28880.0000, 47136.0000, 48960.0000, 15712.0000,  3018.0000, 13416.0000,\n",
      "        59456.0000, 48288.0000, 27968.0000, 40064.0000, 13912.0000,  1640.0000,\n",
      "        33088.0000, 27648.0000, 24832.0000,        inf, 14592.0000, 31296.0000,\n",
      "        18832.0000, 17904.0000, 13288.0000, 14752.0000,        inf, 15288.0000,\n",
      "        25088.0000,  5392.0000, 59360.0000, 42592.0000, 54304.0000, 53408.0000,\n",
      "               inf, 17168.0000,        inf,        inf,        inf,        inf,\n",
      "         3440.0000,        inf,        inf, 25264.0000,        inf,  1791.0000,\n",
      "        60896.0000, 13208.0000, 13456.0000, 25072.0000, 18336.0000, 38528.0000,\n",
      "        14832.0000,  5240.0000, 45664.0000, 12608.0000, 11440.0000, 38560.0000,\n",
      "        11360.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 24544.0000, 37760.0000,  8224.0000,        inf, 64032.0000,\n",
      "               inf,  6984.0000,  2536.0000,        inf, 21408.0000,        inf,\n",
      "               inf, 12016.0000,  5304.0000,        inf, 63808.0000, 19312.0000,\n",
      "        51680.0000,  1755.0000, 56896.0000, 14152.0000,  6408.0000, 23936.0000,\n",
      "        27568.0000, 57024.0000,        inf, 27792.0000, 33568.0000, 22960.0000,\n",
      "         9472.0000,  9208.0000,  5112.0000, 19792.0000, 38592.0000, 23760.0000,\n",
      "        29488.0000, 13904.0000,        inf,        inf, 47392.0000,        inf,\n",
      "        65024.0000, 35936.0000, 12120.0000,        inf, 25616.0000, 20400.0000,\n",
      "               inf, 36256.0000, 26528.0000,        inf, 23840.0000,        inf,\n",
      "        21056.0000, 38016.0000, 16912.0000, 45760.0000, 13832.0000,  4144.0000,\n",
      "        18256.0000, 44352.0000, 35968.0000, 19312.0000, 25792.0000, 20624.0000,\n",
      "               inf, 19344.0000,        inf,        inf, 54336.0000, 62368.0000,\n",
      "        30480.0000,        inf, 32160.0000, 44672.0000, 28864.0000, 60896.0000,\n",
      "         5996.0000, 47744.0000, 11024.0000, 16784.0000, 18608.0000, 18208.0000,\n",
      "               inf, 18736.0000,        inf, 32768.0000,  9408.0000,  1908.0000,\n",
      "        63712.0000, 38016.0000,   170.0000,  8128.0000,        inf, 54816.0000,\n",
      "               inf, 25616.0000,  3838.0000,   687.0000, 12752.0000,        inf,\n",
      "         1216.0000, 17520.0000,        inf,        inf, 26272.0000,  6904.0000,\n",
      "        33536.0000,        inf, 45600.0000,        inf,        inf,        inf,\n",
      "               inf, 22768.0000, 28864.0000, 41760.0000,  6508.0000, 47296.0000,\n",
      "        10992.0000, 25872.0000,        inf,  9592.0000,  1364.0000,        inf,\n",
      "         4768.0000,        inf,  5780.0000,  7088.0000, 45504.0000,  4924.0000,\n",
      "        31664.0000, 46400.0000,        inf, 52544.0000, 38464.0000, 16704.0000,\n",
      "               inf,        inf,        inf, 40416.0000, 38880.0000, 54144.0000,\n",
      "        24400.0000,  8416.0000,        inf,  1294.0000,  4520.0000,  4456.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,  7888.0000,\n",
      "               inf,  6536.0000,        inf,  8384.0000, 12368.0000, 49248.0000,\n",
      "        30896.0000,        inf, 58976.0000, 39232.0000, 46880.0000, 50720.0000,\n",
      "         3206.0000,  6360.0000,        inf,        inf, 21728.0000, 63296.0000,\n",
      "               inf, 43296.0000,        inf, 41312.0000,        inf, 49920.0000,\n",
      "               inf, 56352.0000, 16384.0000,        inf,  5792.0000,        inf,\n",
      "        52736.0000,        inf,        inf, 22864.0000, 59264.0000, 46880.0000,\n",
      "        22592.0000, 28592.0000, 55296.0000, 21344.0000,        inf, 13408.0000,\n",
      "        28176.0000,        inf, 30416.0000, 20240.0000, 21760.0000, 37664.0000,\n",
      "         6840.0000,        inf,  6544.0000,        inf,        inf, 22272.0000,\n",
      "               inf, 22880.0000], device='cuda:0', dtype=torch.float16), tensor([14104.0000, 50112.0000, 19600.0000,        inf,        inf,  5924.0000,\n",
      "        24960.0000,   138.8750, 34976.0000,        inf, 31024.0000,        inf,\n",
      "               inf, 52768.0000, 10872.0000,        inf,        inf,        inf,\n",
      "               inf,        inf, 22240.0000, 25024.0000,        inf, 43424.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,   930.5000,\n",
      "               inf, 21888.0000,  6000.0000, 16496.0000,   813.0000,        inf,\n",
      "               inf,        inf,  7976.0000,  5428.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 46016.0000,        inf, 38816.0000,\n",
      "               inf,        inf, 31824.0000,        inf, 36352.0000,        inf,\n",
      "        57216.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 45024.0000, 42496.0000, 49184.0000,  7112.0000,        inf,\n",
      "               inf, 37344.0000, 40736.0000,        inf,        inf, 22496.0000,\n",
      "        31040.0000,        inf,        inf,        inf, 17776.0000, 23824.0000,\n",
      "               inf, 61568.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 31824.0000, 40512.0000,        inf, 64800.0000,  5196.0000,\n",
      "        41152.0000,        inf,        inf,        inf,        inf, 56096.0000,\n",
      "               inf, 60192.0000, 27552.0000,  8168.0000,  1950.0000,        inf,\n",
      "               inf,        inf,        inf, 39776.0000, 27216.0000,        inf,\n",
      "               inf, 44704.0000,        inf, 21488.0000,        inf,        inf,\n",
      "        31248.0000, 17840.0000,        inf,        inf, 22304.0000,        inf,\n",
      "        49504.0000, 10176.0000, 13696.0000,        inf,  4356.0000,        inf,\n",
      "        10968.0000, 37792.0000, 20224.0000,        inf, 55136.0000,        inf,\n",
      "               inf, 28544.0000, 33280.0000,        inf,  2964.0000, 35872.0000,\n",
      "         4504.0000,        inf,        inf, 36896.0000,        inf, 25712.0000,\n",
      "               inf,        inf, 11376.0000,  4136.0000, 42592.0000, 60512.0000,\n",
      "         5832.0000, 23904.0000, 57472.0000,        inf, 12064.0000,        inf,\n",
      "        20800.0000, 61568.0000,        inf, 45984.0000, 50592.0000, 53088.0000,\n",
      "               inf,        inf,        inf, 26976.0000, 11392.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "        46784.0000,        inf, 53792.0000,        inf,        inf,        inf,\n",
      "               inf, 58496.0000, 52032.0000,        inf,        inf,        inf,\n",
      "               inf, 63360.0000, 35200.0000,        inf, 51456.0000,        inf,\n",
      "         3690.0000,        inf,        inf, 33216.0000,        inf,        inf,\n",
      "               inf, 45824.0000,        inf,        inf, 32336.0000,        inf,\n",
      "               inf, 52128.0000, 29264.0000, 36672.0000,   107.7500,  3116.0000,\n",
      "        55584.0000,        inf, 10552.0000,        inf, 57056.0000, 56096.0000,\n",
      "        49248.0000,        inf,        inf,        inf, 60928.0000,  2039.0000,\n",
      "        46112.0000,        inf, 24864.0000,        inf,        inf,        inf,\n",
      "        52448.0000, 10544.0000, 52288.0000, 59584.0000, 17584.0000, 62336.0000,\n",
      "               inf, 15136.0000, 65504.0000, 33824.0000,        inf,        inf,\n",
      "               inf, 24784.0000,        inf,  7680.0000, 20752.0000, 33632.0000,\n",
      "               inf,        inf,        inf, 33536.0000, 31552.0000,  8144.0000,\n",
      "               inf,        inf, 25968.0000,        inf, 52000.0000,        inf,\n",
      "        55264.0000,        inf,  5148.0000,        inf,        inf, 29344.0000,\n",
      "               inf,        inf, 54240.0000, 54240.0000, 22816.0000,        inf,\n",
      "               inf, 13744.0000,        inf,        inf,        inf,        inf,\n",
      "        10352.0000,        inf,        inf, 18448.0000, 60064.0000, 54432.0000,\n",
      "        48576.0000,        inf, 39488.0000, 51072.0000, 61472.0000, 32176.0000,\n",
      "        22704.0000,        inf,        inf,  5332.0000, 15288.0000,        inf,\n",
      "        39360.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        57728.0000, 49376.0000, 36928.0000, 61120.0000,        inf, 50624.0000,\n",
      "               inf,   637.5000,  6948.0000, 39392.0000, 13440.0000,        inf,\n",
      "               inf,        inf, 31280.0000,        inf,        inf, 36992.0000,\n",
      "               inf,        inf, 18160.0000, 36128.0000, 11120.0000,        inf,\n",
      "               inf, 53472.0000, 39648.0000, 26368.0000, 52128.0000, 12304.0000,\n",
      "        20000.0000, 35936.0000, 24672.0000,        inf,        inf,        inf,\n",
      "        32896.0000,        inf,        inf,        inf,        inf, 63744.0000,\n",
      "               inf,  8012.0000, 18432.0000, 58464.0000,        inf,        inf,\n",
      "               inf,        inf, 22112.0000,        inf,  6064.0000,        inf,\n",
      "        23952.0000, 18512.0000,  7640.0000,        inf, 40448.0000,  7200.0000,\n",
      "        22528.0000, 63360.0000, 52096.0000,   672.5000, 34592.0000, 46080.0000,\n",
      "               inf,  2220.0000,        inf,        inf,        inf,        inf,\n",
      "        43424.0000,        inf,        inf, 19760.0000,        inf,        inf,\n",
      "        12536.0000,        inf, 15232.0000, 40416.0000,        inf,  1299.0000,\n",
      "               inf,        inf,        inf,        inf,   310.5000,  2212.0000,\n",
      "               inf, 55040.0000, 15072.0000, 18544.0000,        inf, 52576.0000,\n",
      "        42912.0000, 11456.0000,        inf, 52736.0000,        inf,        inf,\n",
      "         2298.0000, 15632.0000, 56160.0000,        inf, 40768.0000, 11640.0000,\n",
      "        60000.0000,        inf, 30112.0000,        inf,        inf,        inf,\n",
      "               inf,        inf, 25152.0000, 57632.0000,  1356.0000, 44960.0000,\n",
      "        41312.0000,        inf,        inf, 19616.0000, 41536.0000,        inf,\n",
      "        34432.0000,        inf, 20800.0000,        inf,  5524.0000, 17904.0000,\n",
      "               inf, 28096.0000,        inf,        inf,        inf, 40064.0000,\n",
      "               inf, 47872.0000,        inf, 57792.0000, 32896.0000, 54720.0000,\n",
      "               inf, 33568.0000,        inf, 35040.0000, 38656.0000, 18848.0000,\n",
      "               inf,        inf,        inf,        inf,        inf, 16016.0000,\n",
      "               inf,        inf,        inf, 27184.0000, 11240.0000,        inf,\n",
      "        32352.0000,        inf,        inf, 63392.0000,        inf, 45312.0000,\n",
      "        21104.0000, 20864.0000,        inf,        inf, 39392.0000,        inf,\n",
      "               inf,        inf,        inf, 25984.0000, 54080.0000,        inf,\n",
      "               inf,        inf, 31824.0000,        inf, 21680.0000,        inf,\n",
      "        38752.0000,        inf,        inf,        inf,        inf, 58400.0000,\n",
      "        18544.0000, 62464.0000, 35328.0000, 23888.0000,        inf,  6072.0000,\n",
      "               inf,        inf, 36800.0000,   676.5000,  5388.0000, 31840.0000,\n",
      "           75.8125,        inf,        inf,        inf, 41120.0000, 38048.0000,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([ 7232.0000, 18048.0000, 14032.0000, 42016.0000,  6144.0000, 22912.0000,\n",
      "        15128.0000, 33632.0000, 32112.0000, 30992.0000, 51456.0000, 57408.0000,\n",
      "         2954.0000, 40960.0000, 15888.0000,  2280.0000,        inf, 61856.0000,\n",
      "        56960.0000, 29616.0000,  6364.0000,  3028.0000,  7696.0000, 52864.0000,\n",
      "        55040.0000,        inf,        inf,        inf, 38688.0000,  5200.0000,\n",
      "               inf, 28112.0000, 31008.0000, 28480.0000, 12528.0000, 11680.0000,\n",
      "               inf,        inf,  9704.0000, 26144.0000, 26736.0000,        inf,\n",
      "               inf,        inf,  2286.0000, 11640.0000,  8352.0000, 30512.0000,\n",
      "        63840.0000, 56448.0000, 18704.0000,        inf,  4992.0000,   978.0000,\n",
      "               inf, 15544.0000, 47552.0000, 44288.0000, 12184.0000, 62688.0000,\n",
      "        43872.0000, 16152.0000, 41760.0000,  8076.0000,  3926.0000,        inf,\n",
      "        65120.0000, 16512.0000, 38016.0000, 24656.0000,  9808.0000, 51968.0000,\n",
      "        21712.0000, 61728.0000, 15776.0000,        inf,   549.0000, 48768.0000,\n",
      "         7920.0000, 56416.0000,        inf, 57760.0000,        inf,  8784.0000,\n",
      "        36896.0000, 11944.0000, 15808.0000, 62080.0000,  6156.0000, 18160.0000,\n",
      "        55840.0000,        inf, 64160.0000,  9928.0000, 26400.0000, 10584.0000,\n",
      "        12952.0000, 11760.0000, 53856.0000,  1090.0000, 25488.0000, 20848.0000,\n",
      "               inf, 25248.0000, 43552.0000,  2480.0000,  7528.0000, 13336.0000,\n",
      "               inf, 25120.0000, 10840.0000, 21808.0000,        inf, 43552.0000,\n",
      "        15792.0000, 13480.0000, 25456.0000,  4784.0000, 20992.0000, 45152.0000,\n",
      "        16304.0000, 47904.0000,  1864.0000, 17040.0000,  4560.0000, 45504.0000,\n",
      "        11312.0000,  9648.0000, 10160.0000, 15400.0000, 16344.0000,        inf,\n",
      "               inf,  1273.0000,   407.7500,        inf,  9000.0000,  9864.0000,\n",
      "         2530.0000, 41024.0000, 31056.0000, 29536.0000, 11672.0000, 12592.0000,\n",
      "               inf, 12776.0000, 14944.0000,  2196.0000, 33216.0000,  1297.0000,\n",
      "         4564.0000, 14536.0000, 46048.0000, 17024.0000,  8848.0000,  4424.0000,\n",
      "        23760.0000, 36256.0000, 14848.0000, 41024.0000, 19216.0000,  2188.0000,\n",
      "         8592.0000, 56704.0000,        inf,  2124.0000, 31872.0000, 61888.0000,\n",
      "         7512.0000,        inf, 17344.0000,        inf, 16560.0000,        inf,\n",
      "        30016.0000,        inf, 23008.0000, 28976.0000, 29792.0000,        inf,\n",
      "        33408.0000,  3944.0000,        inf,        inf,        inf, 16480.0000,\n",
      "        59648.0000, 28368.0000,  8440.0000, 21584.0000,        inf,        inf,\n",
      "        30752.0000, 45024.0000,  8848.0000,  6952.0000, 33152.0000,        inf,\n",
      "        46112.0000, 46560.0000, 39552.0000, 38976.0000, 14696.0000, 24304.0000,\n",
      "        37664.0000, 26448.0000, 11232.0000, 24256.0000, 15328.0000, 14600.0000,\n",
      "         7048.0000,        inf, 13344.0000, 56704.0000,        inf, 39168.0000,\n",
      "        49952.0000,        inf,        inf, 17344.0000, 44704.0000, 11680.0000,\n",
      "        30416.0000,  6284.0000, 20768.0000, 41312.0000,        inf,        inf,\n",
      "        37664.0000,  6836.0000,  6056.0000,        inf,  5772.0000, 31344.0000,\n",
      "               inf, 13952.0000, 27392.0000, 16064.0000,        inf, 40544.0000,\n",
      "         4176.0000, 50400.0000, 30864.0000, 18000.0000,  2172.0000,  5360.0000,\n",
      "        54720.0000,        inf, 31568.0000, 41312.0000, 12008.0000,  1660.0000,\n",
      "         7992.0000, 23856.0000, 20544.0000,        inf,  2544.0000, 48128.0000,\n",
      "         6340.0000,  3026.0000, 17968.0000,  6964.0000,        inf, 17424.0000,\n",
      "        41728.0000, 11120.0000,        inf, 54528.0000, 59040.0000, 26864.0000,\n",
      "               inf, 17872.0000,        inf,        inf,        inf, 56864.0000,\n",
      "         6260.0000,        inf,        inf, 17584.0000,        inf, 10640.0000,\n",
      "               inf, 21264.0000, 32128.0000, 44768.0000, 14888.0000, 36832.0000,\n",
      "         2104.0000,  8960.0000, 51168.0000,  8936.0000, 11960.0000, 26528.0000,\n",
      "         4094.0000,        inf,        inf,        inf, 62496.0000,        inf,\n",
      "               inf, 16864.0000, 30384.0000,  4012.0000,        inf,        inf,\n",
      "               inf,  8328.0000,  3572.0000,        inf, 26912.0000,        inf,\n",
      "               inf, 22624.0000,  1966.0000,        inf,        inf, 38944.0000,\n",
      "        57120.0000, 21920.0000, 59680.0000, 12112.0000,  3688.0000,        inf,\n",
      "         8920.0000, 50368.0000,        inf, 31792.0000, 39584.0000, 24016.0000,\n",
      "         6820.0000,  2650.0000, 16624.0000, 53120.0000, 57376.0000, 49216.0000,\n",
      "        31456.0000,  3612.0000,        inf,        inf, 37312.0000,        inf,\n",
      "               inf, 44480.0000,  4396.0000,        inf,  2009.0000, 51648.0000,\n",
      "               inf, 29056.0000, 14616.0000,        inf, 21216.0000,        inf,\n",
      "        11632.0000, 62624.0000, 20704.0000, 64448.0000, 14680.0000,  3076.0000,\n",
      "        13384.0000, 41056.0000, 43328.0000, 18496.0000, 26736.0000, 19968.0000,\n",
      "               inf, 10256.0000,        inf,        inf, 59296.0000,        inf,\n",
      "        34400.0000,        inf, 63776.0000, 46240.0000, 36704.0000, 41440.0000,\n",
      "        18464.0000, 33568.0000,  3042.0000, 20416.0000, 16800.0000,  4272.0000,\n",
      "        50400.0000, 43296.0000,        inf, 49120.0000,  7764.0000,   614.5000,\n",
      "               inf, 28672.0000,  7760.0000,  3584.0000,        inf,        inf,\n",
      "               inf, 24864.0000, 24720.0000,  6760.0000, 15120.0000,        inf,\n",
      "         7484.0000,  4588.0000,        inf, 55776.0000, 16928.0000,  2754.0000,\n",
      "         3072.0000,        inf, 23840.0000,        inf,        inf,        inf,\n",
      "               inf, 44096.0000, 23920.0000, 52128.0000,  6376.0000, 38656.0000,\n",
      "         8304.0000, 64384.0000,        inf,   926.0000,   589.5000,        inf,\n",
      "         4632.0000,        inf,   723.0000, 20128.0000, 43872.0000,   398.0000,\n",
      "        33824.0000, 50432.0000,        inf, 52992.0000,        inf, 19136.0000,\n",
      "               inf,        inf,        inf, 43936.0000, 34784.0000,        inf,\n",
      "        12688.0000, 14208.0000,        inf,  1064.0000,  6168.0000,  2672.0000,\n",
      "               inf,        inf, 51840.0000,        inf,        inf,  6952.0000,\n",
      "               inf,   744.0000,        inf,  1240.0000,  5528.0000, 58208.0000,\n",
      "        45184.0000,        inf, 60352.0000, 24736.0000, 44448.0000, 49920.0000,\n",
      "         1690.0000,  2766.0000,        inf,        inf, 21792.0000, 52704.0000,\n",
      "               inf, 64672.0000,        inf, 33280.0000,        inf,        inf,\n",
      "               inf,  7584.0000, 11848.0000,        inf, 10816.0000,        inf,\n",
      "        37376.0000,        inf,        inf, 26336.0000, 37856.0000, 51968.0000,\n",
      "        23536.0000, 42688.0000, 52160.0000, 26400.0000,        inf, 12136.0000,\n",
      "        29184.0000,        inf, 24464.0000, 19104.0000, 20240.0000, 38880.0000,\n",
      "         3128.0000,   563.0000, 22160.0000,        inf,        inf, 22992.0000,\n",
      "               inf, 37024.0000], device='cuda:0', dtype=torch.float16), tensor([1.7456e+04, 5.4624e+04, 2.2976e+04, 6.1888e+04,        inf, 8.4480e+03,\n",
      "        2.9104e+04, 5.5760e+03, 3.4560e+04,        inf, 3.0368e+04,        inf,\n",
      "               inf, 4.6816e+04, 1.5944e+04,        inf,        inf,        inf,\n",
      "               inf,        inf, 1.6608e+04, 2.1584e+04,        inf, 4.9824e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 3.3000e+03,\n",
      "               inf, 1.4584e+04, 4.3960e+03, 2.5872e+04, 2.5020e+03,        inf,\n",
      "               inf,        inf, 8.7200e+02, 1.0336e+04,        inf,        inf,\n",
      "               inf,        inf,        inf, 4.2976e+04,        inf, 3.4624e+04,\n",
      "               inf,        inf, 3.2400e+04,        inf, 2.4720e+04,        inf,\n",
      "        5.7504e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 5.1136e+04, 4.1728e+04, 4.1184e+04, 3.6500e+03,        inf,\n",
      "               inf, 3.8304e+04, 3.6704e+04,        inf,        inf, 3.3376e+04,\n",
      "        2.1536e+04,        inf,        inf,        inf, 9.3440e+03, 2.0320e+04,\n",
      "               inf, 6.4896e+04,        inf,        inf,        inf,        inf,\n",
      "        6.5472e+04, 3.3632e+04, 2.8768e+04,        inf, 6.3936e+04, 2.7760e+03,\n",
      "        4.9184e+04,        inf,        inf,        inf,        inf, 5.4816e+04,\n",
      "        6.0608e+04, 6.0384e+04, 3.0400e+04, 1.6560e+03, 1.7540e+03,        inf,\n",
      "               inf,        inf,        inf, 4.3552e+04, 2.9200e+04,        inf,\n",
      "               inf, 4.3680e+04,        inf, 2.8528e+04,        inf,        inf,\n",
      "        3.8496e+04, 1.6736e+04,        inf,        inf, 1.4992e+04,        inf,\n",
      "        5.9968e+04, 2.1216e+04, 1.0464e+04,        inf, 1.2512e+04,        inf,\n",
      "        2.2928e+04, 4.1376e+04, 2.9600e+04,        inf, 5.6256e+04,        inf,\n",
      "               inf, 3.1168e+04, 3.1632e+04,        inf, 5.9440e+03, 1.8752e+04,\n",
      "        3.0760e+03,        inf,        inf, 4.0064e+04,        inf, 3.3280e+04,\n",
      "               inf,        inf, 1.2616e+04, 7.5900e+02, 5.0720e+04, 5.6256e+04,\n",
      "        2.7920e+03, 2.5728e+04, 5.3536e+04,        inf, 4.5720e+03,        inf,\n",
      "        2.4256e+04,        inf,        inf, 4.6400e+04, 6.1440e+04,        inf,\n",
      "               inf,        inf,        inf, 1.3504e+04, 8.5680e+03,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "        4.7872e+04,        inf, 5.1904e+04,        inf,        inf,        inf,\n",
      "               inf,        inf, 5.6352e+04,        inf,        inf,        inf,\n",
      "               inf,        inf, 4.5088e+04,        inf, 5.7856e+04,        inf,\n",
      "        6.4240e+03,        inf,        inf, 3.5200e+04,        inf,        inf,\n",
      "               inf, 3.9456e+04,        inf,        inf, 3.6928e+04,        inf,\n",
      "               inf, 5.1104e+04, 2.3136e+04, 4.1984e+04, 1.1320e+03, 2.6875e+01,\n",
      "        4.9472e+04,        inf, 8.5680e+03,        inf,        inf, 5.3696e+04,\n",
      "        4.7584e+04,        inf,        inf,        inf, 5.3952e+04, 3.4420e+03,\n",
      "        4.2336e+04,        inf, 2.4832e+04,        inf,        inf,        inf,\n",
      "        5.2192e+04, 2.1184e+04, 4.7712e+04, 6.0704e+04, 1.0584e+04, 6.4384e+04,\n",
      "               inf, 2.0496e+04, 6.4448e+04, 3.6576e+04,        inf,        inf,\n",
      "               inf, 2.4176e+04,        inf, 6.1880e+03, 1.2096e+04, 3.9360e+04,\n",
      "               inf,        inf,        inf, 3.2992e+04, 2.6800e+04, 8.9120e+03,\n",
      "               inf,        inf, 2.4480e+04,        inf, 4.4032e+04,        inf,\n",
      "        4.8096e+04,        inf, 9.0400e+03,        inf,        inf, 4.2656e+04,\n",
      "               inf,        inf, 4.7360e+04, 6.1088e+04, 3.2496e+04,        inf,\n",
      "               inf, 2.5296e+04,        inf,        inf,        inf,        inf,\n",
      "        1.3784e+04,        inf,        inf, 2.4176e+04, 6.1856e+04, 6.3200e+04,\n",
      "        5.5808e+04,        inf, 4.1952e+04, 6.3168e+04, 6.0384e+04, 2.9472e+04,\n",
      "        2.1984e+04,        inf, 6.3584e+04, 7.2920e+03, 3.2384e+04,        inf,\n",
      "        2.8640e+04,        inf,        inf,        inf,        inf,        inf,\n",
      "        6.0352e+04, 5.0848e+04, 4.3456e+04, 6.2624e+04,        inf, 5.3440e+04,\n",
      "               inf, 2.6580e+03, 9.1760e+03, 4.9984e+04, 4.8160e+03,        inf,\n",
      "               inf, 5.6256e+04, 3.7088e+04,        inf,        inf, 3.3472e+04,\n",
      "               inf,        inf, 2.3856e+04, 3.3664e+04, 1.2192e+04,        inf,\n",
      "               inf, 4.7104e+04, 3.9008e+04, 2.6320e+04, 5.9136e+04, 1.8336e+04,\n",
      "        2.6976e+04, 3.7952e+04, 3.6704e+04,        inf,        inf,        inf,\n",
      "        2.9824e+04,        inf,        inf,        inf,        inf, 6.4704e+04,\n",
      "               inf, 1.7984e+04, 2.2032e+04, 6.4256e+04,        inf,        inf,\n",
      "               inf,        inf, 2.6496e+04,        inf, 8.1400e+03,        inf,\n",
      "        2.4144e+04, 2.8752e+04, 1.9712e+02,        inf, 3.9072e+04, 1.0176e+04,\n",
      "        2.3104e+04,        inf, 4.2592e+04, 6.6200e+03, 3.3088e+04, 4.2272e+04,\n",
      "               inf, 1.3392e+04,        inf,        inf,        inf,        inf,\n",
      "        4.8128e+04,        inf,        inf, 1.9392e+04,        inf,        inf,\n",
      "        1.9584e+04,        inf, 7.7920e+03, 5.6096e+04, 6.3776e+04, 1.0040e+04,\n",
      "               inf,        inf,        inf,        inf, 3.7660e+03, 4.4080e+03,\n",
      "               inf, 4.6816e+04, 6.1400e+03, 1.9984e+04,        inf, 5.4784e+04,\n",
      "        4.6592e+04, 7.1760e+03,        inf, 5.2512e+04,        inf,        inf,\n",
      "        4.9560e+03, 1.4600e+04, 5.4112e+04,        inf, 4.8480e+04, 6.7840e+03,\n",
      "        6.1632e+04,        inf, 1.5360e+04,        inf,        inf,        inf,\n",
      "               inf,        inf, 2.2176e+04, 5.6992e+04, 6.2600e+02, 4.7232e+04,\n",
      "        4.0640e+04,        inf,        inf, 3.1824e+04, 4.3360e+04,        inf,\n",
      "        3.5936e+04,        inf, 1.9504e+04,        inf, 1.7320e+03, 2.6400e+04,\n",
      "               inf, 3.0800e+04,        inf,        inf,        inf, 3.7056e+04,\n",
      "               inf, 4.0640e+04,        inf, 5.7216e+04, 3.6384e+04, 5.4912e+04,\n",
      "        6.3872e+04, 3.1888e+04,        inf, 3.5232e+04, 3.4816e+04, 1.3144e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 1.8560e+04,\n",
      "               inf,        inf,        inf, 3.6800e+04, 2.3340e+03, 6.4928e+04,\n",
      "        4.0032e+04,        inf,        inf, 5.8720e+04, 6.5376e+04, 4.0576e+04,\n",
      "        8.1080e+03, 3.2960e+04,        inf,        inf, 4.0832e+04,        inf,\n",
      "               inf,        inf,        inf, 3.2752e+04, 5.1424e+04,        inf,\n",
      "               inf,        inf, 3.5904e+04,        inf, 2.4112e+04,        inf,\n",
      "        4.2048e+04,        inf,        inf,        inf,        inf, 6.4800e+04,\n",
      "        1.9344e+04, 6.4256e+04, 3.7408e+04, 2.7584e+04,        inf, 7.2720e+03,\n",
      "               inf,        inf, 4.4384e+04, 2.4580e+03, 2.1940e+03, 3.5776e+04,\n",
      "        1.0112e+04,        inf,        inf,        inf, 3.6928e+04, 3.6640e+04,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([18944.0000,  1818.0000, 20336.0000, 60896.0000,  9720.0000,  6184.0000,\n",
      "         3918.0000, 43040.0000, 48064.0000, 45216.0000, 14128.0000,  6888.0000,\n",
      "         8068.0000, 28384.0000, 24752.0000,  5484.0000, 54304.0000, 37568.0000,\n",
      "        12544.0000, 34624.0000,  7092.0000, 26288.0000,  3988.0000, 42880.0000,\n",
      "               inf,        inf, 43104.0000,        inf, 40896.0000, 63584.0000,\n",
      "               inf,  6680.0000, 45120.0000, 16320.0000,  3578.0000, 29728.0000,\n",
      "               inf,  2516.0000, 23344.0000, 43360.0000, 22416.0000,        inf,\n",
      "               inf,        inf, 10224.0000,  4080.0000, 36416.0000,  2564.0000,\n",
      "               inf,        inf, 11384.0000,        inf,   866.0000, 51616.0000,\n",
      "               inf, 51424.0000, 53792.0000, 18576.0000,  5048.0000,        inf,\n",
      "               inf, 10544.0000, 25408.0000,  8124.0000, 17920.0000,        inf,\n",
      "               inf,        inf, 21920.0000, 10728.0000, 21824.0000,        inf,\n",
      "         7232.0000, 39552.0000, 43904.0000,        inf,  6236.0000, 52128.0000,\n",
      "          705.0000, 37152.0000,        inf, 20624.0000, 56256.0000, 11536.0000,\n",
      "        21264.0000,  1144.0000, 22208.0000,        inf, 14720.0000, 32560.0000,\n",
      "        25920.0000,        inf, 61024.0000, 50912.0000, 17376.0000,  3776.0000,\n",
      "        19808.0000,  6168.0000,        inf,  1186.0000, 17664.0000, 46208.0000,\n",
      "               inf, 14040.0000, 25920.0000, 14240.0000,  4460.0000,  2012.0000,\n",
      "               inf, 65248.0000, 16384.0000, 24480.0000,        inf,        inf,\n",
      "        31424.0000, 17856.0000, 56864.0000,  4984.0000, 19552.0000, 16416.0000,\n",
      "        37664.0000, 13240.0000, 23200.0000, 25344.0000, 22592.0000, 22576.0000,\n",
      "        21088.0000,  4760.0000, 25824.0000, 14936.0000, 34304.0000,        inf,\n",
      "               inf, 10384.0000, 18208.0000,        inf, 28912.0000,  8400.0000,\n",
      "        14032.0000,  8736.0000, 25520.0000, 35456.0000, 19664.0000, 26832.0000,\n",
      "               inf, 16800.0000,  4944.0000, 17840.0000, 12792.0000, 10192.0000,\n",
      "        20864.0000, 16784.0000, 27904.0000,  4156.0000, 27168.0000, 19120.0000,\n",
      "        23008.0000,  5352.0000, 63264.0000, 26752.0000, 20208.0000, 15968.0000,\n",
      "        26784.0000,        inf,        inf, 24704.0000,        inf, 28784.0000,\n",
      "        57472.0000,        inf, 17680.0000, 48832.0000, 36288.0000,  5620.0000,\n",
      "        23264.0000,        inf, 44512.0000, 35616.0000, 51488.0000,        inf,\n",
      "        48672.0000,  9624.0000,        inf, 36160.0000, 43968.0000, 16184.0000,\n",
      "               inf, 52064.0000,  5616.0000, 30928.0000, 48320.0000, 41408.0000,\n",
      "        42656.0000,  7024.0000, 14488.0000, 18752.0000, 15496.0000,        inf,\n",
      "        42880.0000,   607.0000, 53984.0000, 25632.0000, 16256.0000, 54496.0000,\n",
      "        10576.0000,  8848.0000,  9336.0000, 27184.0000, 31952.0000, 15720.0000,\n",
      "         5280.0000,        inf,  9008.0000, 23120.0000, 32592.0000,        inf,\n",
      "        53376.0000,        inf,        inf, 48192.0000, 24640.0000,  9312.0000,\n",
      "               inf, 12272.0000, 17440.0000,  5108.0000, 64928.0000,        inf,\n",
      "        41792.0000,  9328.0000, 28608.0000, 39360.0000,  8856.0000, 28512.0000,\n",
      "               inf,        inf, 48256.0000, 34816.0000, 62976.0000, 56960.0000,\n",
      "          704.0000, 58944.0000, 16832.0000, 30528.0000,  2610.0000, 12616.0000,\n",
      "        10448.0000,        inf, 23952.0000, 41728.0000, 34944.0000, 11632.0000,\n",
      "        20176.0000, 12096.0000,  5496.0000, 51872.0000,  5796.0000, 22016.0000,\n",
      "               inf, 27056.0000, 24112.0000,  4648.0000,        inf,  9024.0000,\n",
      "               inf, 14856.0000, 58464.0000,        inf, 28128.0000, 20928.0000,\n",
      "        41984.0000, 28848.0000,        inf,        inf,        inf, 18912.0000,\n",
      "          938.0000, 41088.0000,        inf, 40000.0000, 33536.0000, 16560.0000,\n",
      "               inf, 41248.0000, 13112.0000, 17504.0000, 11576.0000, 23856.0000,\n",
      "         6248.0000, 13512.0000, 52640.0000, 20832.0000,        inf, 11520.0000,\n",
      "         9064.0000, 27664.0000,        inf,        inf, 11576.0000,        inf,\n",
      "               inf, 10000.0000, 25712.0000,  4480.0000,  6496.0000,        inf,\n",
      "               inf, 52736.0000, 26256.0000,        inf, 26144.0000,        inf,\n",
      "               inf, 15344.0000, 18800.0000, 39552.0000, 43680.0000,  5864.0000,\n",
      "               inf, 12176.0000, 58944.0000, 21664.0000, 10552.0000,        inf,\n",
      "        33024.0000,        inf,        inf,  6400.0000, 60128.0000, 32992.0000,\n",
      "        10744.0000, 12488.0000, 51488.0000,   693.0000, 32144.0000, 33280.0000,\n",
      "        40288.0000, 51264.0000, 28992.0000, 26704.0000, 43264.0000,        inf,\n",
      "        42176.0000,  2058.0000, 20496.0000,        inf,  9536.0000,  3342.0000,\n",
      "               inf, 60864.0000, 11896.0000,        inf, 24688.0000,        inf,\n",
      "        12400.0000,        inf, 19456.0000,        inf,  3084.0000,  7948.0000,\n",
      "         1064.0000, 11264.0000, 13240.0000, 17184.0000, 41728.0000, 35168.0000,\n",
      "        57952.0000, 30816.0000,        inf,        inf,        inf,        inf,\n",
      "        48512.0000,        inf, 20272.0000, 27344.0000, 53728.0000, 29248.0000,\n",
      "        16688.0000, 14624.0000, 20592.0000,        inf, 10960.0000, 12544.0000,\n",
      "        64512.0000,   561.5000,        inf,        inf, 25136.0000,  2892.0000,\n",
      "               inf, 42656.0000, 24480.0000,  6684.0000,        inf, 53344.0000,\n",
      "        29296.0000, 29392.0000,  5264.0000, 28784.0000, 30208.0000,        inf,\n",
      "         3008.0000, 23552.0000,        inf, 27680.0000,   218.0000, 37312.0000,\n",
      "        14480.0000,        inf, 27168.0000, 51296.0000, 49952.0000,        inf,\n",
      "               inf, 55008.0000, 15680.0000, 21600.0000,  2984.0000, 11720.0000,\n",
      "         1412.0000,        inf, 59488.0000,  2892.0000, 33728.0000,        inf,\n",
      "        14072.0000, 48544.0000, 10376.0000,  7744.0000, 36480.0000,  4856.0000,\n",
      "        49376.0000, 21120.0000,        inf, 18960.0000,        inf, 13736.0000,\n",
      "               inf, 38208.0000, 40480.0000, 15440.0000, 37312.0000,        inf,\n",
      "        21776.0000, 48768.0000,        inf, 17904.0000, 35200.0000,   568.0000,\n",
      "               inf,        inf, 55552.0000,        inf,        inf,  8568.0000,\n",
      "               inf, 37120.0000,        inf, 23808.0000, 29440.0000, 16400.0000,\n",
      "        30352.0000,        inf,        inf, 16912.0000,        inf, 34912.0000,\n",
      "         7148.0000, 24736.0000,        inf,        inf,  8832.0000, 31024.0000,\n",
      "               inf, 32736.0000,        inf, 47584.0000,        inf,        inf,\n",
      "               inf, 32992.0000,        inf,        inf, 27472.0000, 56448.0000,\n",
      "               inf,        inf,        inf,  5380.0000, 11768.0000,        inf,\n",
      "        59392.0000, 26256.0000, 28112.0000,        inf,        inf, 22160.0000,\n",
      "        33856.0000, 55360.0000, 43904.0000, 28928.0000, 12136.0000,  3206.0000,\n",
      "        26496.0000, 23520.0000,  5488.0000,        inf,        inf, 34784.0000,\n",
      "        33536.0000,  5336.0000], device='cuda:0', dtype=torch.float16), tensor([20816.0000, 27520.0000, 48032.0000,        inf,        inf,  2011.0000,\n",
      "         8124.0000, 28128.0000, 47360.0000, 62784.0000,  1284.0000,        inf,\n",
      "        52704.0000,  9136.0000, 22768.0000,        inf,        inf, 62880.0000,\n",
      "        63552.0000,        inf, 53536.0000,        inf,        inf, 50688.0000,\n",
      "               inf,        inf, 59360.0000,        inf,        inf, 51872.0000,\n",
      "               inf, 12560.0000, 12760.0000,  1447.0000,  5436.0000,        inf,\n",
      "               inf, 43520.0000, 49856.0000, 28464.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 44416.0000, 59584.0000, 11440.0000,\n",
      "               inf,        inf, 38752.0000,        inf, 12320.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 39968.0000,        inf, 17712.0000,  8728.0000,        inf,\n",
      "               inf,        inf, 36224.0000, 53632.0000, 52896.0000,        inf,\n",
      "        29920.0000,        inf,        inf,        inf, 15336.0000, 26048.0000,\n",
      "               inf, 47296.0000,        inf,        inf, 41792.0000,        inf,\n",
      "               inf, 58688.0000, 30736.0000,        inf, 65504.0000, 14960.0000,\n",
      "        61888.0000,        inf, 43872.0000,        inf,        inf, 30560.0000,\n",
      "        32416.0000, 23344.0000,        inf,  6496.0000,  8080.0000,        inf,\n",
      "               inf,        inf, 50816.0000,  9112.0000, 25824.0000,        inf,\n",
      "               inf,        inf, 54592.0000, 33600.0000,        inf,        inf,\n",
      "        43424.0000, 38048.0000,        inf,        inf, 24848.0000,        inf,\n",
      "        42816.0000, 24144.0000,  3242.0000,        inf, 14576.0000,        inf,\n",
      "         3606.0000, 23472.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 30832.0000,  3930.0000,        inf, 52992.0000, 44608.0000,\n",
      "        49920.0000,  8656.0000,        inf, 62272.0000,        inf,        inf,\n",
      "               inf,        inf,  4380.0000, 32352.0000, 17904.0000, 61216.0000,\n",
      "        18976.0000, 11880.0000, 32432.0000,        inf, 28272.0000,        inf,\n",
      "        26144.0000, 33376.0000,        inf,        inf,  1577.0000, 50688.0000,\n",
      "               inf,        inf,        inf,        inf, 61344.0000,        inf,\n",
      "               inf,        inf, 45120.0000,        inf,        inf,        inf,\n",
      "        19536.0000,        inf,        inf,        inf, 63072.0000,        inf,\n",
      "        22304.0000, 58656.0000, 42240.0000, 62976.0000,        inf,        inf,\n",
      "               inf, 63040.0000, 27440.0000,        inf, 31856.0000,        inf,\n",
      "        37408.0000, 50112.0000,        inf, 13056.0000,        inf,        inf,\n",
      "        16832.0000, 19584.0000, 55872.0000,        inf,   824.0000,        inf,\n",
      "               inf, 61920.0000, 23040.0000, 45440.0000,  5648.0000,   421.2500,\n",
      "        25312.0000,        inf, 38528.0000,        inf, 34976.0000,        inf,\n",
      "        42144.0000,        inf,        inf,        inf, 32960.0000,  4548.0000,\n",
      "        63808.0000,        inf, 15680.0000,        inf, 50976.0000,        inf,\n",
      "        50656.0000, 47328.0000, 54368.0000, 44160.0000, 17760.0000, 51456.0000,\n",
      "               inf,        inf,        inf, 41504.0000, 59872.0000,        inf,\n",
      "               inf, 24384.0000,        inf,   528.5000,  9776.0000, 64544.0000,\n",
      "               inf,        inf,        inf, 46336.0000, 16328.0000,  9840.0000,\n",
      "               inf,        inf, 25504.0000,        inf,        inf,        inf,\n",
      "        24240.0000,        inf, 48000.0000,        inf,        inf, 29072.0000,\n",
      "               inf,        inf, 58272.0000, 53088.0000, 45664.0000,        inf,\n",
      "               inf,  7792.0000,        inf,        inf,        inf,  7876.0000,\n",
      "         9936.0000, 61440.0000,        inf, 10256.0000,        inf, 50720.0000,\n",
      "               inf, 45504.0000,  1841.0000, 53856.0000, 47232.0000, 59424.0000,\n",
      "         2900.0000,        inf,        inf, 27712.0000, 65472.0000,        inf,\n",
      "         7008.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        34752.0000, 45504.0000,        inf, 39456.0000,  8712.0000,        inf,\n",
      "               inf, 37312.0000, 37504.0000,        inf, 38816.0000,        inf,\n",
      "               inf, 45856.0000, 32864.0000,        inf,        inf,  4288.0000,\n",
      "               inf, 56128.0000, 23312.0000,  7692.0000, 36288.0000,        inf,\n",
      "               inf,        inf,        inf,  8608.0000, 41568.0000, 25952.0000,\n",
      "        14576.0000, 41344.0000,  6144.0000, 64480.0000,        inf,        inf,\n",
      "         4420.0000,        inf, 62208.0000,        inf,        inf, 37536.0000,\n",
      "               inf, 21984.0000, 40960.0000,        inf, 45504.0000,        inf,\n",
      "               inf,        inf, 33824.0000,        inf,  1677.0000,        inf,\n",
      "        20720.0000,        inf, 34944.0000,        inf,  8052.0000,  5840.0000,\n",
      "         2512.0000, 42688.0000,  7408.0000, 26288.0000, 43776.0000,        inf,\n",
      "               inf, 64256.0000,        inf,        inf,        inf,        inf,\n",
      "        61888.0000,        inf, 54304.0000, 22576.0000,        inf,        inf,\n",
      "        22752.0000,        inf, 28304.0000,        inf, 22832.0000,   545.5000,\n",
      "               inf, 31920.0000,        inf,        inf, 55840.0000,  7500.0000,\n",
      "               inf, 51808.0000, 28176.0000,  9200.0000,        inf, 54784.0000,\n",
      "        16736.0000, 17760.0000,        inf,        inf,        inf,        inf,\n",
      "         1123.0000,  5488.0000,        inf,        inf, 10552.0000, 11816.0000,\n",
      "        34144.0000,        inf,  2234.0000,        inf,        inf,        inf,\n",
      "               inf, 53568.0000,  2252.0000, 32080.0000, 19168.0000, 22480.0000,\n",
      "        42560.0000,        inf,        inf, 43744.0000,        inf,        inf,\n",
      "         5408.0000,        inf, 23584.0000,        inf, 13104.0000, 46592.0000,\n",
      "               inf,  6288.0000,        inf, 62752.0000,        inf, 49728.0000,\n",
      "               inf, 17888.0000,        inf, 16800.0000, 50528.0000, 60672.0000,\n",
      "               inf,        inf,        inf, 48448.0000, 18096.0000, 15016.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,  2536.0000,\n",
      "               inf,        inf,        inf, 14144.0000, 13208.0000, 35392.0000,\n",
      "        33664.0000,        inf, 57344.0000, 58624.0000,        inf, 13296.0000,\n",
      "         1550.0000, 19792.0000,        inf, 41440.0000, 57120.0000, 35328.0000,\n",
      "               inf, 22752.0000,        inf,  3716.0000, 57248.0000,        inf,\n",
      "               inf,        inf,        inf,        inf, 51776.0000,        inf,\n",
      "               inf,        inf,        inf, 35360.0000, 19536.0000,        inf,\n",
      "        46464.0000, 11272.0000, 16448.0000,        inf,        inf,  3934.0000,\n",
      "               inf,        inf, 24720.0000, 32064.0000, 48416.0000, 14816.0000,\n",
      "        23488.0000,        inf,        inf,        inf, 51392.0000, 51776.0000,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16), tensor([1.7376e+04, 6.3840e+03, 1.9680e+04,        inf, 1.8288e+04, 3.5740e+03,\n",
      "        3.2340e+03, 4.3328e+04, 4.6304e+04, 4.0352e+04, 4.7072e+04, 7.1360e+03,\n",
      "        1.1432e+04, 3.1056e+04, 1.8560e+04, 1.0648e+04, 5.8176e+04, 3.6032e+04,\n",
      "        1.3056e+04, 2.8976e+04, 7.4920e+03, 1.9424e+04, 9.0800e+03, 4.2880e+04,\n",
      "               inf,        inf, 3.5584e+04,        inf, 4.1024e+04,        inf,\n",
      "               inf, 8.5440e+03, 4.6656e+04, 8.3680e+03, 7.7400e+02, 2.8960e+04,\n",
      "               inf, 9.8080e+03, 1.4528e+04, 4.9888e+04, 2.4816e+04,        inf,\n",
      "               inf,        inf, 8.7360e+03, 1.0176e+04, 6.0992e+04, 2.9360e+03,\n",
      "               inf,        inf, 2.1392e+04,        inf, 3.1940e+03, 4.5344e+04,\n",
      "               inf, 5.2416e+04, 5.4240e+04, 2.2736e+04, 1.0540e+03,        inf,\n",
      "               inf, 7.1360e+03, 2.5152e+04, 9.0900e+02, 2.0784e+04,        inf,\n",
      "               inf,        inf, 1.9776e+04, 1.5648e+04, 2.1296e+04,        inf,\n",
      "        8.7360e+03, 4.6656e+04, 4.2816e+04,        inf, 1.0048e+04, 5.3280e+04,\n",
      "        1.7325e+02, 3.7280e+04,        inf, 1.2264e+04, 5.0560e+04, 1.8448e+04,\n",
      "        2.0320e+04, 2.4960e+03, 2.8736e+04,        inf, 1.4720e+04, 3.3632e+04,\n",
      "        3.3120e+04,        inf, 5.7952e+04, 5.2832e+04, 1.7520e+04, 1.2048e+04,\n",
      "        2.1792e+04, 7.6560e+03,        inf, 2.0900e+02, 1.4416e+04, 5.1264e+04,\n",
      "               inf, 1.1904e+04, 1.6864e+04, 1.1768e+04, 6.5480e+03, 6.2280e+03,\n",
      "               inf, 5.4400e+04, 2.0048e+04, 2.1232e+04, 6.4032e+04,        inf,\n",
      "        3.2112e+04, 1.6528e+04, 5.4784e+04, 9.9040e+03, 1.7296e+04, 1.7040e+04,\n",
      "        4.2464e+04, 9.3280e+03, 2.1200e+04, 2.2640e+04, 2.0720e+04, 1.9696e+04,\n",
      "        1.2176e+04, 7.3125e+00, 3.7248e+04, 1.4328e+04, 3.5840e+04,        inf,\n",
      "               inf, 9.4720e+03, 1.9136e+04,        inf, 2.2672e+04, 2.5100e+03,\n",
      "        1.5112e+04, 1.1720e+04, 2.4288e+04, 3.7120e+04, 2.0864e+04, 2.7824e+04,\n",
      "               inf, 1.9040e+04, 6.8040e+03, 3.0280e+03, 2.0096e+04, 1.0536e+04,\n",
      "        1.7216e+04, 1.8720e+04, 3.3472e+04, 4.0420e+03, 3.0400e+04, 2.1328e+04,\n",
      "        2.3408e+04, 2.3460e+03,        inf, 2.3216e+04, 2.0256e+04, 1.5408e+04,\n",
      "        2.9344e+04,        inf,        inf, 2.4144e+04,        inf, 3.1056e+04,\n",
      "        5.5648e+04,        inf, 1.7552e+04, 5.2192e+04, 3.7440e+04, 5.8160e+03,\n",
      "        1.1664e+04,        inf, 4.4256e+04, 3.8144e+04, 4.7488e+04,        inf,\n",
      "        4.3008e+04, 9.3680e+03, 6.5408e+04, 3.8272e+04, 4.8736e+04, 1.8960e+04,\n",
      "        6.5312e+04, 6.1120e+04, 4.3680e+03, 2.6208e+04, 3.9072e+04, 3.9840e+04,\n",
      "        5.0784e+04, 9.1360e+03, 1.5208e+04, 1.5688e+04, 1.3792e+04,        inf,\n",
      "        4.1696e+04, 8.1760e+03, 5.0048e+04, 2.3424e+04, 1.0576e+04, 4.7584e+04,\n",
      "        8.4800e+03, 1.0840e+04, 1.0064e+04, 2.8768e+04, 3.8048e+04, 1.3616e+04,\n",
      "        8.3040e+03,        inf, 5.2080e+03, 2.6144e+04, 3.6224e+04,        inf,\n",
      "        5.3472e+04,        inf,        inf, 4.7968e+04, 2.6704e+04, 1.1112e+04,\n",
      "               inf, 1.0416e+04, 1.7648e+04, 4.7960e+03, 5.0592e+04,        inf,\n",
      "        5.0848e+04, 8.7040e+03, 3.4592e+04, 4.6816e+04, 1.0280e+04, 2.8448e+04,\n",
      "               inf,        inf, 4.6848e+04, 2.9824e+04, 5.3536e+04,        inf,\n",
      "        2.5620e+03, 5.5104e+04, 1.7424e+04, 2.8192e+04, 1.0750e+02, 1.8496e+04,\n",
      "        1.0544e+04,        inf, 2.3632e+04, 3.7728e+04, 3.9200e+04, 1.4504e+04,\n",
      "        1.9792e+04, 1.6800e+04, 6.4680e+03, 4.6208e+04, 3.0740e+03, 2.4880e+04,\n",
      "        6.0320e+04, 2.3776e+04, 3.0464e+04, 4.1920e+03,        inf, 1.3048e+04,\n",
      "               inf, 1.7408e+04, 5.7696e+04,        inf, 2.9584e+04, 2.5040e+04,\n",
      "        4.5472e+04, 2.5648e+04,        inf,        inf,        inf, 9.1360e+03,\n",
      "        5.6600e+03, 3.9392e+04,        inf, 5.0176e+04, 3.1392e+04, 1.1232e+04,\n",
      "               inf, 4.1760e+04, 1.0248e+04, 2.0176e+04, 7.9200e+03, 1.8912e+04,\n",
      "        1.4060e+03, 1.7824e+04, 5.7088e+04, 8.0600e+03,        inf, 9.5760e+03,\n",
      "        9.7760e+03, 2.3968e+04,        inf,        inf, 1.2560e+04,        inf,\n",
      "               inf, 1.6376e+04, 2.3136e+04, 1.6850e+03, 1.6576e+04,        inf,\n",
      "               inf, 3.8816e+04, 1.7104e+04,        inf, 2.5312e+04,        inf,\n",
      "               inf, 2.0512e+04, 2.0880e+04, 4.5312e+04, 4.6944e+04, 1.8890e+03,\n",
      "               inf, 1.0584e+04, 5.6064e+04, 2.4112e+04, 4.0880e+03,        inf,\n",
      "        3.2656e+04,        inf,        inf, 3.3260e+03, 6.1728e+04, 2.7632e+04,\n",
      "        3.3040e+03, 4.1880e+03, 3.5808e+04, 1.3192e+04, 2.7392e+04, 3.7280e+04,\n",
      "        4.1312e+04, 4.3776e+04, 2.9328e+04, 2.7600e+04, 4.4064e+04,        inf,\n",
      "        4.4928e+04, 4.6880e+03, 2.0816e+04,        inf, 7.3520e+03, 4.6040e+03,\n",
      "               inf, 6.0416e+04, 3.5980e+03,        inf, 1.8880e+04,        inf,\n",
      "        1.4976e+04,        inf, 2.2816e+04,        inf, 8.5750e+02, 9.6400e+03,\n",
      "        3.6120e+03, 2.1616e+04, 1.3792e+04, 2.0544e+04, 5.5360e+04, 3.2736e+04,\n",
      "        6.2784e+04, 3.8816e+04,        inf,        inf,        inf,        inf,\n",
      "        5.2256e+04,        inf, 2.2000e+04, 2.8440e+03, 4.8768e+04, 3.2960e+04,\n",
      "        2.5440e+04, 1.2240e+04, 1.7424e+04,        inf, 1.3904e+04, 2.2576e+04,\n",
      "               inf, 2.8260e+03,        inf,        inf, 2.0080e+04, 4.1400e+03,\n",
      "               inf, 3.8848e+04, 3.1552e+04, 1.6672e+04,        inf, 5.7760e+04,\n",
      "        8.3360e+03, 3.8816e+04, 9.9280e+03, 2.1376e+04, 2.8176e+04,        inf,\n",
      "        8.2400e+03, 2.4768e+04,        inf, 3.2208e+04, 2.5800e+03, 3.9904e+04,\n",
      "        1.5504e+04,        inf, 1.6992e+04, 4.5344e+04, 4.9952e+04,        inf,\n",
      "               inf, 3.2240e+04, 1.6008e+04, 2.9808e+04, 1.0830e+03, 1.5864e+04,\n",
      "        4.2825e+02, 5.8400e+04, 4.4160e+04, 7.3280e+03, 3.1008e+04,        inf,\n",
      "        1.1696e+04, 4.6912e+04, 8.6400e+03, 7.1700e+02, 4.6912e+04, 5.9880e+03,\n",
      "        3.9488e+04, 2.1360e+04,        inf, 2.5136e+04,        inf, 1.6944e+04,\n",
      "               inf, 3.6864e+04, 3.7504e+04, 1.5520e+03, 4.5824e+04,        inf,\n",
      "        3.0816e+04, 4.8800e+04,        inf, 2.1520e+04, 3.4752e+04, 4.0620e+03,\n",
      "               inf,        inf, 6.3296e+04,        inf,        inf, 5.6240e+03,\n",
      "               inf, 3.1056e+04,        inf, 1.9312e+04, 6.1056e+04, 1.3256e+04,\n",
      "        3.5840e+04,        inf,        inf, 1.6168e+04,        inf, 4.3168e+04,\n",
      "        1.1712e+04, 2.3216e+04,        inf,        inf, 7.1400e+03, 3.0384e+04,\n",
      "               inf, 3.0288e+04,        inf, 4.3136e+04,        inf, 5.6672e+04,\n",
      "               inf, 3.1264e+04,        inf,        inf, 3.4400e+04, 3.8848e+04,\n",
      "               inf,        inf,        inf, 9.5920e+03, 1.1584e+04,        inf,\n",
      "        5.7312e+04, 2.4336e+04, 2.2416e+04,        inf,        inf, 1.9344e+04,\n",
      "        4.1280e+04, 5.3696e+04, 5.0272e+04, 3.1536e+04, 4.8550e+02, 1.4168e+04,\n",
      "        2.4784e+04, 2.3648e+04, 1.0888e+04,        inf,        inf, 5.0272e+04,\n",
      "        3.4752e+04, 1.1152e+04], device='cuda:0', dtype=torch.float16), tensor([20672.0000, 27200.0000, 56448.0000,        inf,        inf,  8116.0000,\n",
      "        15592.0000, 32864.0000, 51968.0000, 56160.0000, 24336.0000,        inf,\n",
      "        56384.0000,  8416.0000, 22480.0000,        inf, 62944.0000, 51456.0000,\n",
      "        63712.0000,        inf, 65152.0000, 64096.0000,        inf, 49632.0000,\n",
      "               inf,        inf, 51328.0000,        inf,        inf,        inf,\n",
      "               inf,  9592.0000, 15712.0000,  5392.0000,  3372.0000,        inf,\n",
      "               inf, 18016.0000, 36640.0000, 37856.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 53440.0000,        inf, 21664.0000,\n",
      "               inf,        inf,        inf,        inf,  6824.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "               inf, 36576.0000,        inf, 32000.0000,  2528.0000,        inf,\n",
      "               inf,        inf, 23568.0000, 52672.0000, 48448.0000,        inf,\n",
      "        28272.0000,        inf,        inf,        inf, 32048.0000, 33216.0000,\n",
      "               inf, 52224.0000,        inf,        inf, 37824.0000,        inf,\n",
      "               inf, 47456.0000, 43360.0000,        inf, 62272.0000, 13128.0000,\n",
      "        39712.0000,        inf, 40768.0000,        inf,        inf, 24320.0000,\n",
      "        29408.0000, 18144.0000, 63264.0000,   626.5000,  9104.0000,        inf,\n",
      "               inf, 64704.0000,        inf,  6196.0000, 43904.0000,        inf,\n",
      "               inf, 58592.0000, 62016.0000, 28256.0000,        inf,        inf,\n",
      "        44480.0000, 39008.0000,        inf, 60672.0000, 18704.0000,        inf,\n",
      "        41248.0000, 27664.0000, 14152.0000,        inf,  2582.0000,        inf,\n",
      "        14288.0000, 31088.0000,        inf,        inf,        inf,        inf,\n",
      "               inf, 30784.0000, 12440.0000,        inf, 36000.0000, 36448.0000,\n",
      "        44768.0000,   772.5000,        inf,        inf,        inf,        inf,\n",
      "               inf,        inf, 10752.0000,  7600.0000,  6368.0000, 56800.0000,\n",
      "        25712.0000, 16464.0000, 46240.0000,        inf, 31056.0000,        inf,\n",
      "         9728.0000, 18832.0000,        inf, 65344.0000,  1642.0000, 53984.0000,\n",
      "               inf,        inf,        inf,        inf, 64928.0000,        inf,\n",
      "               inf,        inf, 51712.0000,        inf,        inf,        inf,\n",
      "        48320.0000,        inf,        inf,        inf, 63360.0000,        inf,\n",
      "        26448.0000,        inf, 39584.0000, 64096.0000,        inf,        inf,\n",
      "               inf,        inf, 16832.0000,        inf, 19088.0000,        inf,\n",
      "        52512.0000, 54272.0000,        inf,  1197.0000,        inf,        inf,\n",
      "        18656.0000, 24848.0000, 54176.0000,        inf, 12824.0000,        inf,\n",
      "        57280.0000,        inf, 25648.0000, 23728.0000, 13464.0000,  2862.0000,\n",
      "        18960.0000,        inf, 53344.0000,        inf, 41568.0000,        inf,\n",
      "        43648.0000,        inf,        inf,        inf, 33568.0000,  1318.0000,\n",
      "        63680.0000,        inf, 17248.0000,        inf, 30352.0000,        inf,\n",
      "        57376.0000, 49248.0000,        inf, 51584.0000, 14984.0000, 52704.0000,\n",
      "               inf,        inf, 64000.0000, 30592.0000, 57152.0000,        inf,\n",
      "               inf, 19104.0000,        inf,  5316.0000, 18720.0000,        inf,\n",
      "               inf,        inf, 65024.0000, 43552.0000, 25488.0000, 20880.0000,\n",
      "               inf,        inf, 23152.0000,        inf,        inf,        inf,\n",
      "        28064.0000,        inf, 52896.0000,        inf,        inf, 34304.0000,\n",
      "               inf,        inf, 53088.0000, 54048.0000, 59968.0000,        inf,\n",
      "               inf,  8068.0000,        inf,        inf,        inf, 21264.0000,\n",
      "        18160.0000, 59200.0000,        inf, 19696.0000, 65376.0000, 61760.0000,\n",
      "               inf, 45184.0000,   498.2500, 55392.0000, 45440.0000, 56640.0000,\n",
      "        10976.0000,        inf,        inf,  9296.0000,        inf,        inf,\n",
      "        12144.0000,        inf,        inf,        inf,        inf,        inf,\n",
      "        40832.0000, 55136.0000, 59488.0000, 33664.0000, 19984.0000,        inf,\n",
      "               inf, 26960.0000, 31152.0000,        inf, 31728.0000,        inf,\n",
      "               inf, 53120.0000, 31360.0000,        inf,        inf,  1344.0000,\n",
      "               inf,        inf, 14040.0000, 27056.0000, 32256.0000,        inf,\n",
      "               inf,        inf,        inf, 12624.0000, 50432.0000, 31696.0000,\n",
      "         4228.0000, 35968.0000,  4740.0000,        inf,        inf,        inf,\n",
      "         3120.0000,        inf,        inf,        inf,        inf, 23808.0000,\n",
      "               inf, 18128.0000, 51456.0000,        inf, 56704.0000,        inf,\n",
      "               inf,        inf, 16688.0000,        inf,  7700.0000,        inf,\n",
      "        27792.0000, 58336.0000, 46144.0000,        inf, 11368.0000, 15824.0000,\n",
      "         2045.0000, 56000.0000,  9136.0000, 18752.0000, 56288.0000,        inf,\n",
      "               inf,        inf,        inf,        inf,        inf,        inf,\n",
      "        62784.0000,        inf, 54432.0000, 10136.0000,        inf,        inf,\n",
      "        30624.0000,        inf, 22832.0000,        inf, 12144.0000,  6080.0000,\n",
      "               inf, 30016.0000,        inf,        inf, 44160.0000,  6072.0000,\n",
      "               inf, 49248.0000, 33824.0000, 19072.0000,        inf, 58208.0000,\n",
      "         3664.0000, 39680.0000,        inf, 62144.0000,        inf,        inf,\n",
      "         3852.0000,  9152.0000,        inf,        inf, 16480.0000, 13280.0000,\n",
      "        27552.0000,        inf, 17296.0000,        inf,        inf,        inf,\n",
      "               inf, 34976.0000,  7832.0000, 45184.0000, 11672.0000, 39808.0000,\n",
      "        35840.0000,        inf, 55872.0000, 57088.0000,        inf,        inf,\n",
      "        10984.0000,        inf, 22464.0000,        inf, 33184.0000, 50400.0000,\n",
      "               inf,  8592.0000,        inf, 63616.0000,        inf, 65376.0000,\n",
      "               inf, 17776.0000,        inf,  4388.0000,        inf,        inf,\n",
      "               inf,        inf,        inf, 54336.0000, 20384.0000, 20240.0000,\n",
      "               inf,        inf,        inf,        inf,        inf,  3298.0000,\n",
      "               inf,        inf,        inf,  8192.0000, 36448.0000, 29568.0000,\n",
      "        37024.0000,        inf, 59456.0000, 54240.0000,        inf, 17600.0000,\n",
      "         8824.0000, 18528.0000,        inf, 44864.0000,        inf, 33408.0000,\n",
      "        63200.0000, 18656.0000,        inf, 14224.0000, 57024.0000,        inf,\n",
      "               inf,        inf,        inf,        inf, 62272.0000, 61344.0000,\n",
      "               inf,        inf,        inf, 37056.0000, 23936.0000,        inf,\n",
      "        47328.0000, 14944.0000,  7320.0000,        inf,        inf,  2786.0000,\n",
      "               inf,        inf, 35104.0000, 31648.0000, 36256.0000,  2878.0000,\n",
      "        27520.0000,        inf,        inf,        inf, 38848.0000,        inf,\n",
      "               inf,        inf], device='cuda:0', dtype=torch.float16)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hutch state: [tensor([[ 9736.,    inf, 50816.,  ...,  1993.,    inf, 36928.],\n",
      "        [27264.,    inf,    inf,  ...,    inf,    inf,    inf],\n",
      "        [12320.,    inf, 54752.,  ..., 30384.,    inf,    inf],\n",
      "        ...,\n",
      "        [  132.,  3366.,     0.,  ...,     0.,   869.,  4320.],\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.],\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([[   inf,    inf,    inf,  ...,    inf, 39744.,    inf],\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.],\n",
      "        [   inf,    inf, 38816.,  ...,    inf,    inf, 31024.],\n",
      "        ...,\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.],\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.],\n",
      "        [    0.,     0.,     0.,  ...,     0.,     0.,     0.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([[   inf,    inf,    inf,  ...,    inf,    inf,    inf],\n",
      "        [10712.,  4148., 11552.,  ...,  4094.,  7664., 10696.],\n",
      "        [35424., 12968., 34720.,  ..., 16464., 22512., 34528.],\n",
      "        ...,\n",
      "        [ 8656.,  4352., 11368.,  ...,  3084.,  7408.,  9840.],\n",
      "        [10248.,  5372., 14240.,  ...,  2892.,  9880., 11800.],\n",
      "        [15888.,  2864., 11784.,  ...,  7504.,  8216., 12736.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([2856.0000,  906.5000, 1507.0000,  ..., 2064.0000, 3194.0000,\n",
      "        1917.0000], device='cuda:0', dtype=torch.float16), tensor([[5.0000e-01, 1.2650e+02, 6.3344e+01,  ..., 2.7675e+02, 8.8650e+02,\n",
      "         6.1400e+02],\n",
      "        [6.2344e+01, 3.2031e+01, 1.6828e+01,  ..., 4.6375e+02, 4.5775e+02,\n",
      "         4.7300e+02],\n",
      "        [2.8719e+01, 1.3625e+02, 5.1188e+01,  ..., 5.4300e+02, 1.1080e+03,\n",
      "         7.4050e+02],\n",
      "        ...,\n",
      "        [4.1720e+03, 2.5300e+03, 6.7120e+03,  ..., 4.1152e+04, 5.3056e+04,\n",
      "         5.2416e+04],\n",
      "        [2.3200e+03, 1.7320e+03, 5.1200e+03,  ..., 1.8368e+04, 1.9664e+04,\n",
      "         2.1328e+04],\n",
      "        [2.5160e+03, 1.2600e+03, 2.9720e+03,  ..., 1.9216e+04, 2.2304e+04,\n",
      "         2.0992e+04]], device='cuda:0', dtype=torch.float16), tensor([  517.0000,   470.0000,   677.5000,  ..., 46944.0000, 18752.0000,\n",
      "        19840.0000], device='cuda:0', dtype=torch.float16), tensor([[ 5288.,    94.,  8888.,  ...,  5504.,  6100.,  4936.],\n",
      "        [   inf,  9040., 27936.,  ...,    inf, 22960., 20192.],\n",
      "        [17152.,   310.,  7232.,  ..., 13752.,  7340.,  2706.],\n",
      "        ...,\n",
      "        [23776.,  6164., 21888.,  ..., 22720., 12800.,  2924.],\n",
      "        [33504.,  6328., 15456.,  ..., 17136.,  9344.,  2416.],\n",
      "        [26544.,  3438., 12664.,  ..., 18336.,  9456.,  2586.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([ 3634.0000,        inf, 16232.0000,  7176.0000, 10744.0000, 32656.0000,\n",
      "        18320.0000, 26240.0000, 15216.0000, 50976.0000, 38464.0000, 34400.0000,\n",
      "        54720.0000, 22640.0000, 10456.0000, 22032.0000, 20480.0000,  5424.0000,\n",
      "        28240.0000,  2206.0000,  1894.0000, 23328.0000, 32688.0000,  5412.0000,\n",
      "               inf,  8896.0000,        inf, 16704.0000, 30992.0000, 38912.0000,\n",
      "         4940.0000, 17056.0000,  6564.0000, 22048.0000, 28256.0000, 65504.0000,\n",
      "        26128.0000, 23232.0000, 26320.0000,        inf, 11368.0000, 41728.0000,\n",
      "        28400.0000, 58528.0000, 16024.0000, 17648.0000, 61440.0000, 17920.0000,\n",
      "        41152.0000, 12416.0000,  6736.0000, 18656.0000, 47072.0000,  1169.0000,\n",
      "        31776.0000,  1566.0000, 41920.0000, 25216.0000, 11184.0000, 61184.0000,\n",
      "        52000.0000, 42176.0000, 17792.0000, 15416.0000,  2376.0000, 33824.0000,\n",
      "        15992.0000, 17376.0000,  5192.0000, 13096.0000, 20480.0000, 37440.0000,\n",
      "        18096.0000, 38720.0000, 27376.0000,  3938.0000,        inf, 35392.0000,\n",
      "        23056.0000, 43936.0000, 46336.0000,  1594.0000, 24960.0000, 50368.0000,\n",
      "        23072.0000, 27360.0000, 13520.0000, 53472.0000, 41664.0000, 10264.0000,\n",
      "               inf,        inf, 44448.0000, 51488.0000, 18544.0000, 16928.0000,\n",
      "        62912.0000,  8784.0000,  6732.0000, 55616.0000, 18512.0000, 14960.0000,\n",
      "        23184.0000, 27136.0000,   588.5000,  7728.0000, 24688.0000,  6872.0000,\n",
      "               inf, 63712.0000, 38016.0000,  7624.0000, 45152.0000, 20096.0000,\n",
      "        20528.0000,        inf,   960.0000, 13560.0000, 34496.0000, 25888.0000,\n",
      "        36896.0000,  9464.0000,  3192.0000, 34496.0000, 30864.0000,        inf,\n",
      "        18080.0000, 64480.0000, 45984.0000, 14616.0000, 12080.0000, 50336.0000,\n",
      "         7300.0000, 23008.0000,  9904.0000, 28336.0000,        inf, 17552.0000,\n",
      "        44224.0000,   312.2500, 12232.0000, 44960.0000, 18896.0000, 20160.0000,\n",
      "        39072.0000, 16784.0000, 34400.0000, 22992.0000, 44224.0000,  1923.0000,\n",
      "        34144.0000, 29888.0000, 60352.0000, 19648.0000, 18688.0000, 38496.0000,\n",
      "        17888.0000, 40352.0000, 11736.0000, 62432.0000, 28384.0000, 49216.0000,\n",
      "        53024.0000,  3738.0000,  3454.0000, 34880.0000, 12048.0000, 40128.0000,\n",
      "               inf, 43744.0000,        inf, 15248.0000,   123.6875, 13536.0000,\n",
      "        11640.0000, 35296.0000, 26416.0000, 13800.0000, 10648.0000,  5944.0000,\n",
      "        26496.0000,  4692.0000, 37344.0000, 13728.0000, 53312.0000, 63456.0000,\n",
      "        48032.0000,        inf,   498.0000, 16432.0000, 22848.0000, 23200.0000,\n",
      "        54624.0000,        inf, 36544.0000,        inf,        inf, 17136.0000,\n",
      "        32656.0000, 37024.0000, 16072.0000,  1296.0000, 40640.0000,        inf,\n",
      "               inf, 31792.0000, 11808.0000, 29440.0000, 15904.0000,  5244.0000,\n",
      "        44736.0000,  6596.0000, 32768.0000, 52640.0000,  9064.0000,  5100.0000,\n",
      "        27056.0000, 24448.0000,        inf, 17888.0000, 42112.0000,  3820.0000,\n",
      "        45216.0000,  9024.0000, 46784.0000, 23392.0000, 23744.0000, 56320.0000,\n",
      "        49728.0000,  5936.0000,  3584.0000, 23872.0000, 22672.0000,        inf,\n",
      "         8384.0000,  6504.0000, 38432.0000, 29904.0000, 35616.0000, 22400.0000,\n",
      "        24528.0000,  8400.0000, 35840.0000,        inf, 42976.0000, 14576.0000,\n",
      "        17536.0000,        inf, 36800.0000,  2126.0000, 45248.0000, 12624.0000,\n",
      "        28128.0000, 25904.0000, 24272.0000, 34240.0000,  2992.0000, 45504.0000,\n",
      "        50464.0000,  2724.0000, 33984.0000,  3030.0000, 28848.0000, 17808.0000,\n",
      "        24368.0000, 21376.0000,        inf,  7340.0000, 36416.0000,        inf,\n",
      "         2636.0000, 42016.0000, 25040.0000, 35136.0000,  4536.0000, 56992.0000,\n",
      "        22064.0000, 24784.0000, 32736.0000,        inf, 14208.0000, 12160.0000,\n",
      "        17856.0000, 33024.0000, 47232.0000,  6636.0000, 12880.0000, 15320.0000,\n",
      "        25040.0000, 29296.0000, 33728.0000,  4840.0000, 36384.0000,  9696.0000,\n",
      "        17328.0000, 63328.0000,  5376.0000, 31824.0000, 64288.0000, 22480.0000,\n",
      "        35744.0000, 49536.0000, 17504.0000, 39872.0000, 62240.0000, 46816.0000,\n",
      "        22560.0000, 11128.0000, 53408.0000,        inf, 58912.0000, 46176.0000,\n",
      "        34624.0000, 16240.0000,  3206.0000, 17472.0000, 30560.0000, 37888.0000,\n",
      "        36544.0000, 37440.0000, 50688.0000, 54112.0000, 43680.0000, 48672.0000,\n",
      "        35808.0000,  4056.0000,   732.5000, 15208.0000, 37472.0000, 15664.0000,\n",
      "        11304.0000, 39136.0000, 45504.0000, 27504.0000,        inf, 39904.0000,\n",
      "        32960.0000, 13744.0000, 21296.0000, 48704.0000, 11848.0000,        inf,\n",
      "        32688.0000, 26640.0000, 56032.0000, 23200.0000, 12032.0000, 22752.0000,\n",
      "         3100.0000, 36448.0000,        inf, 30272.0000, 63488.0000, 48928.0000,\n",
      "        57280.0000, 22192.0000, 15312.0000, 36832.0000, 12400.0000,        inf,\n",
      "        24896.0000,  6132.0000,        inf, 21280.0000,  5140.0000,  9280.0000,\n",
      "        18160.0000, 35232.0000,        inf,  2912.0000, 12872.0000,   355.2500,\n",
      "         5472.0000, 14776.0000, 48256.0000,  5080.0000, 24752.0000, 30272.0000,\n",
      "         2968.0000, 15536.0000, 53920.0000, 11552.0000, 35648.0000, 15232.0000,\n",
      "        18384.0000, 41440.0000, 30336.0000, 61440.0000, 43072.0000,  5668.0000,\n",
      "        30976.0000,  3170.0000,  3798.0000,        inf,   555.0000, 19040.0000,\n",
      "        34304.0000,        inf,        inf, 57760.0000, 50592.0000,   611.0000,\n",
      "        14056.0000,  5464.0000, 42496.0000, 33856.0000, 41408.0000, 46272.0000,\n",
      "        18480.0000, 22992.0000,  3902.0000, 14368.0000, 21136.0000, 20736.0000,\n",
      "        15672.0000, 29456.0000,  1824.0000,        inf, 59072.0000, 34560.0000,\n",
      "        32656.0000, 48896.0000,        inf,  3366.0000, 21792.0000, 28768.0000,\n",
      "        33696.0000,        inf, 11528.0000, 28128.0000, 24272.0000, 13288.0000,\n",
      "               inf, 33632.0000,  3044.0000, 33088.0000, 62432.0000, 15592.0000,\n",
      "        56448.0000,   843.0000,  6696.0000, 43904.0000, 18784.0000,  6912.0000,\n",
      "         4212.0000, 37792.0000, 48672.0000, 20864.0000,  5640.0000,  5388.0000,\n",
      "        21072.0000, 34240.0000,  1297.0000,  7484.0000,  6312.0000, 21680.0000,\n",
      "        39072.0000, 38272.0000,        inf, 13704.0000,        inf, 17648.0000,\n",
      "        32208.0000, 42848.0000, 56160.0000,  9448.0000,  9224.0000, 22560.0000,\n",
      "        58336.0000, 47968.0000,  4436.0000, 34944.0000,        inf, 25632.0000,\n",
      "        12968.0000,  2510.0000, 25024.0000, 32480.0000, 48032.0000,        inf,\n",
      "        50112.0000, 27328.0000, 34272.0000, 55648.0000, 26816.0000,        inf,\n",
      "        63616.0000, 39648.0000, 30272.0000,        inf, 29168.0000, 22112.0000,\n",
      "        42112.0000, 31120.0000,        inf, 14712.0000, 48704.0000, 53952.0000,\n",
      "         5280.0000, 24320.0000, 26128.0000, 13880.0000, 50656.0000, 11856.0000,\n",
      "        30144.0000, 29168.0000, 16912.0000,        inf, 19120.0000, 34880.0000,\n",
      "        35488.0000, 29360.0000], device='cuda:0', dtype=torch.float16), tensor([[  573.5000,  1267.0000,    36.0625,  ...,   225.2500,  1967.0000,\n",
      "          1766.0000],\n",
      "        [ 4752.0000, 10432.0000,  1698.0000,  ..., 13280.0000, 27536.0000,\n",
      "         28928.0000],\n",
      "        [  410.7500,   118.8750,   263.0000,  ...,   851.5000,  1029.0000,\n",
      "           974.5000],\n",
      "        ...,\n",
      "        [  639.0000,   631.0000,  1517.0000,  ...,   152.5000,  3792.0000,\n",
      "           440.0000],\n",
      "        [  342.7500,   555.5000,   609.0000,  ...,   162.7500,   133.5000,\n",
      "          1024.0000],\n",
      "        [   41.0000,  1686.0000,  2266.0000,  ...,  3172.0000,   114.0000,\n",
      "          2676.0000]], device='cuda:0', dtype=torch.float16), tensor([ 1726.0000, 24384.0000,   799.5000,  ...,  1343.0000,   423.7500,\n",
      "         2160.0000], device='cuda:0', dtype=torch.float16), tensor([[3.0040e+03, 3.9960e+03, 8.5250e+01,  ..., 5.1400e+02, 3.4150e+02,\n",
      "         7.9200e+02],\n",
      "        [2.2680e+03, 1.9728e+04, 1.0780e+03,  ..., 1.1560e+04, 7.3250e+02,\n",
      "         1.5632e+04],\n",
      "        [7.9500e+02, 2.4480e+03, 2.5625e+02,  ..., 2.1840e+03, 2.4734e+01,\n",
      "         4.2680e+03],\n",
      "        ...,\n",
      "        [3.3000e+02, 5.8120e+03, 5.3500e+02,  ..., 7.4640e+03, 7.0250e+02,\n",
      "         5.2960e+03],\n",
      "        [8.2400e+02, 3.0280e+03, 4.0100e+02,  ..., 1.3490e+03, 3.3188e+01,\n",
      "         1.9940e+03],\n",
      "        [1.1000e+01, 1.4570e+03, 5.4350e+02,  ..., 4.0840e+03, 2.3600e+02,\n",
      "         4.1920e+03]], device='cuda:0', dtype=torch.float16), tensor([15752.0000,        inf,  8256.0000,  3780.0000, 13488.0000, 31936.0000,\n",
      "        12768.0000, 27248.0000, 36416.0000, 52032.0000, 36160.0000, 23072.0000,\n",
      "        43968.0000, 23680.0000, 22224.0000, 25984.0000,  6856.0000, 16208.0000,\n",
      "        16960.0000,  8120.0000, 15608.0000, 10456.0000, 37920.0000, 11360.0000,\n",
      "        58560.0000,  6644.0000, 38560.0000, 13712.0000, 34592.0000, 25472.0000,\n",
      "        18576.0000,  1533.0000, 14744.0000, 14184.0000, 17472.0000, 62624.0000,\n",
      "        30416.0000, 10000.0000, 17792.0000,        inf,  6620.0000, 42336.0000,\n",
      "        27200.0000, 42400.0000, 18592.0000,  2786.0000, 48384.0000,  9240.0000,\n",
      "        43264.0000, 11568.0000,  1658.0000,   146.7500, 26320.0000,  1611.0000,\n",
      "        21504.0000, 13688.0000, 31376.0000, 21792.0000, 16064.0000, 39776.0000,\n",
      "        41824.0000, 31072.0000, 14024.0000,  9056.0000, 13680.0000, 29376.0000,\n",
      "        26176.0000,  8816.0000, 18768.0000,  1225.0000,  5564.0000, 36992.0000,\n",
      "        25120.0000, 10128.0000, 24208.0000,  6580.0000,        inf, 22720.0000,\n",
      "         2720.0000, 11944.0000, 27456.0000, 13792.0000, 14632.0000, 29120.0000,\n",
      "        10512.0000, 31200.0000, 17424.0000, 32560.0000, 29536.0000,  9848.0000,\n",
      "        33280.0000, 54784.0000, 41216.0000, 47520.0000, 22208.0000,  2680.0000,\n",
      "        47456.0000, 20992.0000, 17392.0000, 49984.0000, 17568.0000,  6768.0000,\n",
      "        10008.0000, 19504.0000,  5492.0000, 12456.0000, 29968.0000,  8960.0000,\n",
      "        59360.0000, 38944.0000, 37920.0000, 10744.0000, 30352.0000, 21616.0000,\n",
      "        18016.0000, 58496.0000,  1086.0000, 11944.0000, 18256.0000, 19552.0000,\n",
      "        37728.0000, 16752.0000,  4396.0000, 43936.0000, 23536.0000, 60256.0000,\n",
      "        10152.0000, 51712.0000, 36832.0000, 24064.0000, 11096.0000, 28448.0000,\n",
      "         7900.0000,  7236.0000,  1110.0000, 17600.0000, 58560.0000, 26768.0000,\n",
      "        22576.0000,  5804.0000,  7216.0000, 31520.0000, 36896.0000, 20480.0000,\n",
      "        50656.0000,  5380.0000, 18544.0000, 22096.0000, 43392.0000,  4768.0000,\n",
      "        19248.0000, 20480.0000, 59360.0000, 25680.0000, 15408.0000, 23456.0000,\n",
      "         7516.0000, 15768.0000,  9008.0000, 56800.0000,  2504.0000, 46688.0000,\n",
      "        44544.0000, 10000.0000, 13968.0000, 36960.0000, 10440.0000, 40448.0000,\n",
      "               inf, 33088.0000,        inf, 14400.0000,  1386.0000, 15456.0000,\n",
      "        14920.0000, 30592.0000, 31280.0000, 19504.0000, 28640.0000,  2426.0000,\n",
      "        33760.0000, 11560.0000, 35168.0000,  6888.0000, 39456.0000, 50784.0000,\n",
      "        37568.0000, 50496.0000,  3506.0000,  7972.0000, 17040.0000,  9144.0000,\n",
      "        46688.0000, 44704.0000, 24272.0000, 60576.0000, 59840.0000, 17040.0000,\n",
      "        20160.0000,  8592.0000, 14360.0000,  1386.0000, 37504.0000, 62432.0000,\n",
      "        54016.0000, 15784.0000, 25216.0000, 17392.0000, 16912.0000, 15256.0000,\n",
      "        24592.0000,  8568.0000, 33184.0000, 38144.0000,  6044.0000, 22832.0000,\n",
      "        22960.0000, 16240.0000, 43136.0000,  8088.0000, 26416.0000, 15264.0000,\n",
      "        37184.0000,  8528.0000, 27072.0000, 17376.0000,  8360.0000, 49024.0000,\n",
      "        51040.0000,  1506.0000,  2812.0000, 22528.0000, 17232.0000, 55648.0000,\n",
      "        18032.0000,   971.5000, 37728.0000, 37920.0000, 29040.0000, 14064.0000,\n",
      "        18816.0000,  4864.0000, 12160.0000,        inf, 40256.0000, 13632.0000,\n",
      "        19568.0000, 56288.0000, 20480.0000, 11864.0000, 49792.0000, 16016.0000,\n",
      "        12024.0000, 29440.0000, 15704.0000, 24752.0000,  2152.0000, 28480.0000,\n",
      "        35616.0000,   831.5000, 33216.0000,  4800.0000, 33600.0000, 23808.0000,\n",
      "        15280.0000, 10144.0000,        inf, 14216.0000, 36384.0000,        inf,\n",
      "        25008.0000, 15592.0000, 22240.0000, 43360.0000,  9704.0000, 33632.0000,\n",
      "         1941.0000, 34656.0000, 21312.0000, 57344.0000, 14264.0000, 17120.0000,\n",
      "        19568.0000, 21728.0000, 27008.0000,  1372.0000, 24016.0000, 11040.0000,\n",
      "        12720.0000, 20944.0000, 35776.0000,  5396.0000,  9928.0000,  7652.0000,\n",
      "        23904.0000, 56064.0000, 14096.0000, 26928.0000, 51680.0000,  5872.0000,\n",
      "        30192.0000, 50176.0000, 11192.0000, 38400.0000, 43168.0000, 48736.0000,\n",
      "         5468.0000,  3532.0000, 38400.0000, 49984.0000, 52032.0000, 35200.0000,\n",
      "        20624.0000, 13840.0000, 16144.0000,  8816.0000, 15184.0000, 27136.0000,\n",
      "        23136.0000, 27472.0000, 29936.0000, 46816.0000, 38048.0000, 37216.0000,\n",
      "        17120.0000,  5164.0000,  5264.0000, 21568.0000, 25952.0000, 25216.0000,\n",
      "        16560.0000, 15136.0000, 46368.0000, 24592.0000, 46048.0000, 29616.0000,\n",
      "        17168.0000,  2116.0000, 14968.0000, 44768.0000,  4308.0000,        inf,\n",
      "        23984.0000, 21216.0000, 38656.0000, 13320.0000,  4644.0000, 18240.0000,\n",
      "        14408.0000, 36480.0000, 56032.0000, 31040.0000, 55552.0000, 36704.0000,\n",
      "        48800.0000, 28512.0000,  4456.0000, 30048.0000, 16480.0000, 58016.0000,\n",
      "        23712.0000,  6828.0000,        inf,  1155.0000,  7956.0000,  2164.0000,\n",
      "        22960.0000, 30256.0000, 52416.0000,  3996.0000, 13392.0000,  1872.0000,\n",
      "         6252.0000, 10816.0000, 29728.0000,  4948.0000,  8520.0000, 41440.0000,\n",
      "        15632.0000,  8440.0000, 29296.0000, 10048.0000, 17520.0000,  9232.0000,\n",
      "        15768.0000, 35296.0000, 22752.0000, 59744.0000, 39232.0000, 10448.0000,\n",
      "        22768.0000,  4856.0000,  3244.0000,        inf,  6572.0000, 10872.0000,\n",
      "        18944.0000,        inf,        inf, 43648.0000, 53760.0000,  2262.0000,\n",
      "         3516.0000, 11264.0000, 37472.0000, 19792.0000, 27744.0000, 45408.0000,\n",
      "        31440.0000, 27008.0000,  2784.0000, 13272.0000,   557.0000, 18672.0000,\n",
      "        10248.0000, 22784.0000,  3490.0000,        inf, 51328.0000, 18592.0000,\n",
      "        31696.0000, 36672.0000,        inf,  8720.0000, 15144.0000, 17872.0000,\n",
      "        31488.0000, 53024.0000,  4604.0000, 21008.0000, 26592.0000,  5692.0000,\n",
      "        58880.0000, 44064.0000,  2866.0000, 38784.0000, 54176.0000, 17296.0000,\n",
      "        39424.0000,  4804.0000, 30256.0000, 33184.0000, 10976.0000, 19936.0000,\n",
      "         2234.0000, 28496.0000, 42112.0000, 27936.0000,   420.2500, 10584.0000,\n",
      "        15112.0000, 21984.0000, 10960.0000, 12152.0000,  2750.0000, 18672.0000,\n",
      "         9976.0000,  5028.0000, 44992.0000, 12168.0000, 56768.0000, 19776.0000,\n",
      "        31792.0000, 31312.0000, 37088.0000,  8648.0000, 11368.0000,  3900.0000,\n",
      "        51552.0000, 34656.0000,  2045.0000, 29104.0000, 61248.0000, 20992.0000,\n",
      "         2202.0000,  7764.0000, 24928.0000, 28224.0000, 40640.0000,        inf,\n",
      "        46912.0000, 21632.0000, 18336.0000, 49120.0000, 14976.0000, 58912.0000,\n",
      "        36096.0000, 28688.0000,  7576.0000,        inf,  9120.0000, 26176.0000,\n",
      "        15664.0000, 27648.0000, 63104.0000, 17136.0000, 47392.0000, 31056.0000,\n",
      "        10712.0000, 25536.0000, 22688.0000,  3962.0000, 30160.0000,  6616.0000,\n",
      "        20848.0000, 21264.0000,  7648.0000,        inf, 21424.0000, 37280.0000,\n",
      "        10768.0000, 18880.0000], device='cuda:0', dtype=torch.float16), tensor([7.5600e+02, 4.2880e+04, 5.0520e+03, 9.0720e+03, 3.5040e+03, 6.8080e+03,\n",
      "        1.2864e+04, 8.9200e+03, 2.7560e+03, 2.2672e+04, 5.1360e+03, 5.7360e+03,\n",
      "        2.7072e+04, 5.6080e+03, 4.5280e+03, 8.8150e+02, 2.0448e+04, 5.0200e+03,\n",
      "        1.1440e+04, 8.7520e+03, 1.7620e+03, 1.1088e+04, 6.4040e+03, 7.9050e+02,\n",
      "        4.0896e+04, 3.8060e+03, 3.4816e+04, 2.6160e+03, 2.0880e+03, 4.7648e+04,\n",
      "        9.3850e+02, 6.7920e+03, 5.7320e+03, 1.0336e+04, 1.9504e+04, 1.0312e+04,\n",
      "        3.4940e+03, 9.9920e+03, 2.8760e+03, 5.0336e+04, 1.1060e+03, 5.6440e+03,\n",
      "        3.9160e+03, 2.9200e+04, 2.5580e+03, 5.5200e+03, 3.9520e+04, 2.2420e+03,\n",
      "        2.5760e+04, 6.1100e+02, 8.4320e+03, 1.1848e+04, 3.0256e+04, 3.3900e+03,\n",
      "        1.4192e+04, 2.2180e+03, 2.9440e+03, 1.1860e+03, 2.9080e+03, 3.2864e+04,\n",
      "        8.6480e+03, 1.3104e+04, 8.4640e+03, 4.0720e+03, 5.5680e+03, 3.8100e+03,\n",
      "        6.7840e+03, 3.8740e+03, 2.9880e+03, 2.3760e+04, 1.6136e+04, 5.6640e+03,\n",
      "        1.6030e+03, 1.1640e+04, 2.6208e+04, 1.5040e+04, 1.6090e+03, 4.8760e+03,\n",
      "        7.7938e+01, 4.6520e+03, 1.2088e+04, 1.7230e+03, 1.5376e+04, 4.4200e+03,\n",
      "        2.2560e+03, 8.5120e+03, 3.1620e+03, 2.8400e+04, 2.6832e+04, 2.5120e+04,\n",
      "        3.0416e+04, 2.3600e+04, 3.4580e+03, 1.5448e+04, 1.0528e+04, 1.2448e+04,\n",
      "        2.3472e+04, 5.3240e+03, 7.3520e+03, 3.2000e+04, 5.5350e+02, 9.3120e+03,\n",
      "        6.1320e+03, 3.5300e+03, 7.3000e+03, 1.2248e+04, 1.9930e+03, 8.9100e+02,\n",
      "               inf, 5.9000e+03, 1.4544e+04, 4.7100e+02, 3.2740e+03, 2.6220e+03,\n",
      "        9.0160e+03, 6.6280e+03, 9.6600e+02, 8.8400e+02, 5.8600e+02, 2.3160e+03,\n",
      "        8.2400e+03, 2.9800e+03, 3.0750e+02, 1.2896e+04, 1.8128e+04, 1.9824e+04,\n",
      "        7.1240e+03, 2.0464e+04, 9.8560e+03, 8.6160e+03, 2.7640e+03, 1.3352e+04,\n",
      "        5.0480e+03, 1.9616e+04, 1.6290e+03, 1.3592e+04, 2.0860e+03, 8.3840e+03,\n",
      "        1.5312e+04, 1.1730e+03, 4.5680e+03, 1.7472e+04, 1.5088e+04, 9.2560e+03,\n",
      "        5.4700e+02, 5.1120e+03, 1.2096e+04, 2.9480e+03, 3.4420e+03, 1.8570e+03,\n",
      "        1.4100e+03, 1.3032e+04, 1.5416e+04, 8.6400e+03, 4.5825e+02, 9.7800e+02,\n",
      "        9.5360e+03, 3.5840e+04, 1.6470e+03, 2.7440e+04, 1.6200e+04, 2.4680e+03,\n",
      "        6.2560e+04, 1.3330e+03, 4.3000e+02, 4.1840e+03, 9.3120e+03, 1.7200e+04,\n",
      "        6.6600e+03, 6.1350e+02, 1.2488e+04, 1.8000e+04, 6.5350e+02, 4.7120e+03,\n",
      "        5.4080e+03, 3.1872e+04, 9.8160e+03, 1.0984e+04, 1.0640e+04, 3.8180e+03,\n",
      "        2.1712e+04, 2.4040e+03, 9.1200e+03, 1.8920e+03, 3.1744e+04, 2.6432e+04,\n",
      "        6.7520e+03, 1.2272e+04, 4.6280e+03, 5.7880e+03, 1.4250e+03, 5.3680e+03,\n",
      "        3.1024e+04, 5.2896e+04, 2.0464e+04, 2.4000e+04, 1.1496e+04, 1.3392e+04,\n",
      "        2.6768e+04, 2.4180e+03, 2.3616e+04, 1.5510e+03, 1.6880e+04, 2.2016e+04,\n",
      "        6.6000e+03, 3.1500e+03, 4.7680e+03, 1.9728e+04, 1.3840e+04, 7.6700e+02,\n",
      "        2.1552e+04, 3.5320e+03, 6.0900e+02, 2.2620e+03, 4.2720e+03, 8.2000e+03,\n",
      "        4.0256e+04, 3.6260e+03, 4.9824e+04, 6.6320e+03, 6.7560e+03, 2.6240e+03,\n",
      "        2.2528e+04, 5.9320e+03, 4.1024e+04, 1.4888e+04, 3.4880e+04, 3.9712e+04,\n",
      "        1.8288e+04, 6.0400e+03, 4.3400e+03, 1.5872e+04, 5.1760e+03, 7.1880e+03,\n",
      "        5.1160e+03, 4.8560e+03, 7.6650e+02, 1.1288e+04, 3.6100e+03, 8.2160e+03,\n",
      "        5.6120e+03, 3.8240e+03, 1.3968e+04, 8.0280e+03, 1.4216e+04, 5.1440e+03,\n",
      "        1.8048e+04, 5.4656e+04, 2.1880e+03, 9.0850e+02, 1.8816e+04, 2.3488e+02,\n",
      "        6.2840e+03, 1.1480e+04, 8.6480e+03, 4.7080e+03, 4.5880e+03, 1.1672e+04,\n",
      "        2.5408e+04, 8.0400e+03, 1.5350e+03, 4.5800e+02, 1.1240e+04, 1.3280e+04,\n",
      "        1.0136e+04, 1.1176e+04, 6.0832e+04, 2.3812e+01, 2.1120e+04, 4.8360e+03,\n",
      "        1.1550e+03, 1.4336e+04, 1.8432e+04, 2.3392e+04, 2.0960e+03, 4.8704e+04,\n",
      "        1.0888e+04, 1.4240e+04, 1.0112e+04, 3.8656e+04, 2.6860e+03, 4.3440e+03,\n",
      "        6.6120e+03, 2.2112e+04, 9.0800e+03, 2.7725e+02, 9.0240e+03, 5.4560e+03,\n",
      "        9.0720e+03, 1.0184e+04, 1.3960e+04, 4.9840e+03, 9.1900e+02, 1.0500e+03,\n",
      "        5.2040e+03, 3.0544e+04, 2.6560e+03, 3.1440e+04, 3.3280e+04, 2.1088e+04,\n",
      "        8.8080e+03, 1.0352e+04, 6.1640e+03, 2.6624e+04, 7.3480e+03, 2.9056e+04,\n",
      "        7.4000e+03, 2.7060e+03, 1.8384e+04, 5.3632e+04, 3.9040e+04, 4.2000e+03,\n",
      "        1.1304e+04, 1.0488e+04, 1.5170e+03, 4.4320e+03, 9.3920e+03, 2.1808e+04,\n",
      "        1.4936e+04, 1.3968e+04, 2.0512e+04, 4.2640e+03, 2.1740e+03, 6.9120e+03,\n",
      "        6.6640e+03, 9.0320e+03, 2.3362e+02, 1.6384e+04, 1.6336e+04, 1.7160e+03,\n",
      "        8.2800e+03, 2.3600e+04, 3.6256e+04, 1.0094e+01, 4.5984e+04, 5.3056e+04,\n",
      "        1.4176e+04, 5.1160e+03, 1.3984e+04, 1.5152e+04, 2.4256e+04, 4.7904e+04,\n",
      "        3.0384e+04, 1.9408e+04, 4.5800e+03, 9.1040e+03, 5.8480e+03, 8.8480e+03,\n",
      "        6.2750e+02, 1.0672e+04, 8.5360e+03, 7.4960e+03, 3.3760e+04, 2.5040e+03,\n",
      "        4.1875e+02, 5.5840e+03, 1.6600e+03, 7.4640e+03, 3.9740e+03, 2.1008e+04,\n",
      "        1.5648e+04, 2.0820e+03, 8.9920e+03, 1.3440e+04, 1.3980e+03, 8.3280e+03,\n",
      "        1.7440e+03, 1.7424e+04, 1.3848e+04, 1.8510e+03, 2.2040e+03, 1.1620e+03,\n",
      "        2.8700e+02, 1.9072e+04, 2.8704e+04, 6.3520e+03, 4.1360e+03, 2.5360e+03,\n",
      "        1.5020e+03, 7.8000e+03, 4.2016e+04, 5.9640e+03, 2.0800e+04, 8.0280e+03,\n",
      "        9.8000e+03, 2.8736e+04, 1.1272e+04, 5.3100e+02, 4.8192e+04, 2.4160e+03,\n",
      "        2.2976e+04, 2.6500e+02, 2.1820e+03,        inf, 1.4790e+03, 1.6160e+04,\n",
      "        3.0880e+04, 7.7280e+03,        inf, 4.1728e+04, 3.5232e+04, 1.3280e+03,\n",
      "        1.2008e+04, 3.2420e+03, 1.8752e+04, 2.0432e+04, 1.6688e+04, 2.7600e+04,\n",
      "        1.6560e+04, 7.1200e+03, 1.5000e+03, 7.2160e+03, 9.6560e+03, 1.2584e+04,\n",
      "        7.7440e+03, 1.1880e+04, 1.5738e+02,        inf, 4.2080e+04, 5.9560e+03,\n",
      "        3.4592e+04, 3.1680e+04,        inf, 1.2580e+03, 2.0800e+04, 2.2368e+04,\n",
      "        8.7760e+03, 3.7632e+04, 3.9520e+03, 1.3224e+04, 7.7080e+03, 1.5560e+04,\n",
      "        4.2848e+04, 1.5424e+04, 8.2160e+03, 1.1736e+04, 4.8832e+04, 1.9568e+04,\n",
      "        6.3280e+03, 1.9030e+03, 8.2450e+02, 3.0288e+04, 1.2656e+04, 4.7600e+03,\n",
      "        9.8240e+03, 4.5504e+04, 3.0784e+04, 1.2056e+04, 1.9600e+04, 4.7240e+03,\n",
      "        1.4720e+04, 2.2688e+04, 1.1930e+03, 6.1840e+03, 1.2000e+03, 1.3016e+04,\n",
      "        2.6464e+04, 3.4880e+04, 1.2416e+04, 1.5120e+04, 6.5376e+04, 2.4840e+03,\n",
      "        1.4016e+04, 4.0032e+04, 2.0672e+04, 5.5040e+03, 2.6080e+03, 2.0832e+04,\n",
      "        4.1280e+04, 8.1560e+03, 1.5350e+02, 2.4896e+04, 4.7264e+04, 1.7744e+04,\n",
      "        1.4440e+04, 1.5130e+03, 9.7120e+03, 1.8592e+04, 2.4464e+04, 4.3712e+04,\n",
      "        4.1824e+04, 2.5200e+04, 1.0032e+04, 1.6000e+04, 9.9120e+03, 5.7504e+04,\n",
      "        4.0512e+04, 1.6464e+04, 3.0464e+04, 6.0704e+04, 1.1040e+04, 2.1440e+04,\n",
      "        1.3464e+04, 1.2440e+04, 5.6512e+04, 4.5000e+03, 2.0896e+04, 3.2928e+04,\n",
      "        1.8600e+03, 2.8592e+04, 1.0928e+04, 5.2760e+03, 1.7984e+04, 6.5160e+03,\n",
      "        1.2536e+04, 2.6192e+04, 1.2144e+04, 1.7056e+04, 9.7840e+03, 1.3888e+04,\n",
      "        3.5360e+04, 2.2720e+04], device='cuda:0', dtype=torch.float16), tensor([4.8640e+03,        inf, 1.3544e+04, 5.7840e+03, 8.3440e+03, 2.7504e+04,\n",
      "        1.3608e+04, 2.3056e+04, 1.3256e+04, 4.2976e+04, 3.2720e+04, 2.6544e+04,\n",
      "        4.6336e+04, 1.8560e+04, 1.0784e+04, 1.8784e+04, 1.7328e+04, 4.7520e+03,\n",
      "        2.2064e+04, 1.7850e+03, 6.6300e+02, 1.8128e+04, 2.7232e+04, 4.6840e+03,\n",
      "               inf, 8.4800e+03, 5.7216e+04, 1.2600e+04, 2.6064e+04, 3.2672e+04,\n",
      "        4.6520e+03, 1.4392e+04, 4.3360e+03, 1.9552e+04, 2.3168e+04, 5.2992e+04,\n",
      "        2.2544e+04, 1.8496e+04, 2.1440e+04,        inf, 1.0560e+04, 3.5936e+04,\n",
      "        2.2032e+04, 5.2128e+04, 1.3128e+04, 1.5912e+04, 5.0944e+04, 1.5784e+04,\n",
      "        3.3920e+04, 9.6080e+03, 5.1800e+03, 1.5392e+04, 3.7632e+04, 7.6188e+01,\n",
      "        2.8064e+04, 1.4810e+03, 3.2928e+04, 1.9984e+04, 9.1200e+03, 5.2544e+04,\n",
      "        4.3456e+04, 3.6288e+04, 1.5848e+04, 1.4816e+04, 9.2050e+02, 2.8416e+04,\n",
      "        1.4016e+04, 1.3880e+04, 4.6480e+03, 1.1360e+04, 1.7696e+04, 3.2000e+04,\n",
      "        1.4264e+04, 3.2992e+04, 2.3184e+04, 3.8140e+03,        inf, 2.9056e+04,\n",
      "        1.8448e+04, 3.7696e+04, 3.7856e+04, 1.5350e+03, 2.1376e+04, 4.2496e+04,\n",
      "        1.9968e+04, 2.4896e+04, 1.1608e+04, 4.4704e+04, 3.4752e+04, 1.0320e+04,\n",
      "        6.4128e+04, 5.5872e+04, 3.8656e+04, 4.5856e+04, 1.5840e+04, 1.5632e+04,\n",
      "        5.3696e+04, 8.0280e+03, 4.8080e+03, 4.5504e+04, 1.5528e+04, 1.2688e+04,\n",
      "        1.8256e+04, 2.1568e+04, 1.6830e+03, 6.5600e+03, 1.9360e+04, 4.3960e+03,\n",
      "        5.6640e+04, 5.2448e+04, 3.2896e+04, 5.4680e+03, 3.6320e+04, 1.7376e+04,\n",
      "        1.7536e+04, 5.7184e+04, 3.3525e+02, 1.0728e+04, 2.9088e+04, 2.1184e+04,\n",
      "        2.9904e+04, 9.0960e+03, 1.7680e+03, 2.8976e+04, 2.7360e+04, 5.6064e+04,\n",
      "        1.5336e+04, 5.2992e+04, 3.8432e+04, 1.2168e+04, 9.7200e+03, 4.2400e+04,\n",
      "        5.7960e+03, 2.0544e+04, 7.8160e+03, 2.2960e+04,        inf, 1.3288e+04,\n",
      "        3.6768e+04, 4.8675e+02, 9.6240e+03, 3.6192e+04, 1.5728e+04, 1.6560e+04,\n",
      "        3.3664e+04, 1.5344e+04, 2.7856e+04, 2.1024e+04, 3.6064e+04, 9.8700e+02,\n",
      "        3.0384e+04, 2.6112e+04, 5.2896e+04, 1.5400e+04, 1.7744e+04, 3.1680e+04,\n",
      "        1.6200e+04, 3.3536e+04, 1.2200e+04, 5.2640e+04, 2.2992e+04, 4.0768e+04,\n",
      "        4.4032e+04, 1.8610e+03, 2.4940e+03, 3.0096e+04, 1.0000e+04, 3.4016e+04,\n",
      "        6.1184e+04, 3.8784e+04,        inf, 1.3240e+04, 7.9250e+02, 8.5680e+03,\n",
      "        1.0472e+04, 2.8768e+04, 2.2672e+04, 1.1344e+04, 1.0256e+04, 3.3100e+03,\n",
      "        2.3696e+04, 2.9240e+03, 3.1376e+04, 1.2368e+04, 4.6432e+04, 5.3472e+04,\n",
      "        4.1664e+04, 5.5168e+04, 8.9500e+02, 1.3168e+04, 2.0240e+04, 1.8448e+04,\n",
      "        4.5088e+04, 5.9520e+04, 3.0304e+04,        inf, 6.1728e+04, 1.2560e+04,\n",
      "        2.8256e+04, 3.2288e+04, 1.4384e+04, 5.9219e+01, 3.3920e+04, 5.6480e+04,\n",
      "        5.7920e+04, 2.7728e+04, 9.4880e+03, 2.3456e+04, 1.3928e+04, 3.8500e+03,\n",
      "        3.7664e+04, 6.4320e+03, 2.7488e+04, 4.3968e+04, 5.8640e+03, 5.4920e+03,\n",
      "        2.4640e+04, 1.9888e+04, 5.6928e+04, 1.3864e+04, 3.7280e+04, 2.5880e+03,\n",
      "        3.8816e+04, 6.0840e+03, 3.9040e+04, 1.8672e+04, 2.2256e+04, 4.8800e+04,\n",
      "        4.1024e+04, 5.2600e+03, 2.8500e+03, 1.8880e+04, 2.0464e+04, 6.0000e+04,\n",
      "        6.6920e+03, 5.8840e+03, 3.1008e+04, 2.5680e+04, 2.9952e+04, 1.9920e+04,\n",
      "        2.2256e+04, 6.7360e+03, 2.9168e+04,        inf, 3.7344e+04, 1.2600e+04,\n",
      "        1.4360e+04, 6.4192e+04, 3.2832e+04, 1.4710e+03, 3.9104e+04, 9.3520e+03,\n",
      "        2.4288e+04, 2.3104e+04, 1.8688e+04, 2.8128e+04, 1.8300e+03, 3.9456e+04,\n",
      "        4.2560e+04, 2.4260e+03, 3.0160e+04, 3.4940e+03, 2.3168e+04, 1.2528e+04,\n",
      "        2.0208e+04, 1.7888e+04,        inf, 7.1320e+03, 3.0928e+04,        inf,\n",
      "        2.8460e+03, 3.3312e+04, 2.0640e+04, 2.9504e+04, 4.1800e+03, 4.9536e+04,\n",
      "        1.7280e+04, 2.2560e+04, 2.6704e+04, 5.9232e+04, 1.2976e+04, 9.3520e+03,\n",
      "        1.4664e+04, 2.9088e+04, 3.9072e+04, 6.1800e+03, 9.7360e+03, 1.4280e+04,\n",
      "        2.2720e+04, 2.4944e+04, 2.8672e+04, 3.5500e+03, 2.9152e+04, 9.2240e+03,\n",
      "        1.5112e+04, 5.2096e+04, 3.8700e+03, 2.8128e+04, 5.2928e+04, 1.9568e+04,\n",
      "        2.7408e+04, 4.2688e+04, 1.5448e+04, 3.1376e+04, 5.0784e+04, 4.1472e+04,\n",
      "        1.7408e+04, 8.8080e+03, 4.5664e+04, 5.7824e+04, 5.0848e+04, 3.8688e+04,\n",
      "        2.7760e+04, 1.2936e+04, 3.0260e+03, 1.3400e+04, 2.5776e+04, 3.2048e+04,\n",
      "        3.0752e+04, 3.1296e+04, 4.3744e+04, 4.5312e+04, 3.6352e+04, 4.1472e+04,\n",
      "        3.1104e+04, 4.9000e+03, 6.0450e+02, 1.3912e+04, 3.0096e+04, 1.1384e+04,\n",
      "        8.5600e+03, 3.5104e+04, 3.7856e+04, 2.3504e+04, 5.7216e+04, 3.4368e+04,\n",
      "        2.5328e+04, 9.0720e+03, 1.8416e+04, 3.9360e+04, 1.1008e+04,        inf,\n",
      "        2.6800e+04, 2.2416e+04, 4.6560e+04, 1.8992e+04, 1.0760e+04, 1.9696e+04,\n",
      "        3.4220e+03, 3.1632e+04,        inf, 2.4272e+04, 5.2032e+04, 4.2048e+04,\n",
      "        4.7808e+04, 1.9376e+04, 1.0736e+04, 3.1872e+04, 1.1680e+04, 6.0256e+04,\n",
      "        2.0944e+04, 4.3200e+03,        inf, 1.6912e+04, 5.7280e+03, 7.0560e+03,\n",
      "        1.3760e+04, 3.0064e+04, 6.0992e+04, 2.1920e+03, 1.1056e+04, 2.0880e+03,\n",
      "        3.2760e+03, 1.2456e+04, 4.1376e+04, 4.6600e+03, 2.1104e+04, 2.6624e+04,\n",
      "        1.8520e+03, 1.3832e+04, 4.5536e+04, 1.0120e+04, 2.9504e+04, 1.1680e+04,\n",
      "        1.3400e+04, 3.7088e+04, 2.4336e+04, 5.2384e+04, 3.4816e+04, 3.8860e+03,\n",
      "        2.5072e+04, 1.2570e+03, 2.6180e+03,        inf, 5.4300e+02, 1.4680e+04,\n",
      "        2.8656e+04,        inf,        inf, 4.8736e+04, 4.2272e+04, 1.3110e+03,\n",
      "        1.0872e+04, 5.6200e+03, 3.4848e+04, 3.1200e+04, 3.4464e+04, 3.8560e+04,\n",
      "        1.5584e+04, 1.9712e+04, 2.9420e+03, 1.4016e+04, 1.7104e+04, 1.7952e+04,\n",
      "        1.2272e+04, 2.5088e+04, 1.6720e+03,        inf, 4.9952e+04, 2.8848e+04,\n",
      "        2.7936e+04, 4.2048e+04,        inf, 2.2500e+03, 1.9120e+04, 2.3952e+04,\n",
      "        2.9344e+04, 5.9200e+04, 1.1032e+04, 2.5008e+04, 2.0192e+04, 1.1024e+04,\n",
      "        5.6544e+04, 2.8544e+04, 3.5720e+03, 2.6928e+04, 5.3568e+04, 1.4048e+04,\n",
      "        4.8768e+04, 3.6800e+02, 4.7160e+03, 3.5712e+04, 1.7200e+04, 4.4560e+03,\n",
      "        4.8000e+03, 3.2496e+04, 3.9648e+04, 1.7536e+04, 6.1040e+03, 3.7080e+03,\n",
      "        1.8784e+04, 3.0576e+04, 1.1330e+03, 6.8160e+03, 5.2960e+03, 1.6352e+04,\n",
      "        3.1696e+04, 3.3024e+04, 5.6320e+04, 1.3008e+04, 5.7472e+04, 1.4248e+04,\n",
      "        2.9184e+04, 3.4944e+04, 4.6688e+04, 8.2320e+03, 7.7440e+03, 1.9152e+04,\n",
      "        4.9440e+04, 4.0576e+04, 2.8520e+03, 2.8816e+04, 6.4448e+04, 2.0912e+04,\n",
      "        1.1944e+04, 3.1720e+03, 2.0240e+04, 2.7856e+04, 3.8848e+04,        inf,\n",
      "        4.1664e+04, 2.2784e+04, 2.9424e+04, 4.6976e+04, 2.2688e+04, 6.2208e+04,\n",
      "        5.3664e+04, 3.2672e+04, 2.6864e+04,        inf, 2.4896e+04, 1.8224e+04,\n",
      "        3.4144e+04, 2.6896e+04,        inf, 1.1144e+04, 4.0416e+04, 4.5120e+04,\n",
      "        2.4380e+03, 2.1328e+04, 2.2384e+04, 1.0576e+04, 4.3776e+04, 9.2080e+03,\n",
      "        2.4480e+04, 2.5024e+04, 1.4048e+04,        inf, 1.4888e+04, 2.9584e+04,\n",
      "        2.8736e+04, 2.4544e+04], device='cuda:0', dtype=torch.float16), tensor([5.7040e+03, 5.5584e+04, 3.1720e+03, 1.2320e+04, 3.7960e+03, 1.1120e+04,\n",
      "        1.2400e+04, 1.9584e+04, 1.3416e+04, 1.3376e+04, 9.0800e+03, 1.5088e+04,\n",
      "        2.8464e+04, 7.5600e+03, 1.2792e+04, 5.0560e+03, 7.0760e+03, 1.8400e+04,\n",
      "        9.9360e+03, 1.8672e+04, 1.0792e+04, 2.7980e+03, 7.0080e+03, 5.5150e+02,\n",
      "        3.0736e+04, 4.3880e+03, 2.7696e+04, 6.5960e+03, 1.3144e+04, 4.1184e+04,\n",
      "        1.7408e+04, 3.6300e+03, 1.5328e+04, 1.4152e+04, 1.7216e+04, 6.3240e+03,\n",
      "        3.0000e+03, 7.2600e+03, 1.5120e+04, 4.1184e+04, 2.1300e+03, 1.5232e+04,\n",
      "        1.3112e+04, 1.7920e+04, 4.1520e+03, 5.2720e+03, 2.9136e+04, 3.6140e+03,\n",
      "        2.1952e+04, 7.0560e+03, 8.1680e+03, 1.2580e+03, 1.1616e+04, 1.1990e+03,\n",
      "        9.2400e+03, 9.2800e+03, 8.2450e+02, 8.3850e+02, 4.0900e+03, 2.6896e+04,\n",
      "        1.3328e+04, 4.2520e+03, 4.1440e+03, 5.1720e+03, 1.1160e+03, 3.4260e+03,\n",
      "        1.2568e+04, 2.6740e+03, 9.4080e+03, 1.1720e+04, 3.1900e+03, 8.1360e+03,\n",
      "        7.1400e+02, 9.9520e+03, 3.2192e+04, 1.4184e+04, 3.0860e+03, 1.4904e+04,\n",
      "        7.5440e+03, 2.3760e+03, 9.7840e+03, 1.3296e+04, 1.1472e+04, 8.6950e+02,\n",
      "        2.2200e+03, 6.8120e+03, 1.5616e+04, 2.2080e+04, 2.7616e+04, 2.5808e+04,\n",
      "        6.4800e+03, 3.6736e+04, 9.6200e+02, 1.3120e+04, 7.5960e+03, 5.7440e+03,\n",
      "        2.7376e+04, 8.8880e+03, 5.6400e+03, 2.4656e+04, 2.2360e+03, 5.1640e+03,\n",
      "        4.7200e+02, 3.7220e+03, 5.8680e+03, 1.1592e+04, 5.3080e+03, 1.3438e+02,\n",
      "               inf, 6.4700e+02, 1.1736e+04, 4.5600e+03, 2.6960e+03, 2.0338e+02,\n",
      "        1.6240e+04, 7.0680e+03, 2.7660e+03, 9.1250e+02, 4.7560e+03, 2.1760e+03,\n",
      "        1.1632e+04, 5.2520e+03, 4.0060e+03, 3.3024e+04, 1.7984e+04, 3.8720e+03,\n",
      "        8.0920e+03, 2.6608e+04, 6.5200e+03, 1.5192e+04, 6.4440e+03, 1.5392e+04,\n",
      "        2.3220e+03, 1.0840e+04, 2.3640e+03, 2.9080e+03, 2.2288e+04, 1.6704e+04,\n",
      "        1.0240e+04, 2.4180e+03, 1.6040e+03, 1.3704e+04, 3.3632e+04, 3.2040e+03,\n",
      "        6.0480e+03, 1.6290e+03, 5.9450e+02, 6.6720e+03, 4.3400e+03, 3.9900e+03,\n",
      "        4.5200e+03, 1.8064e+04, 3.6736e+04, 1.2864e+04, 3.9400e+03, 6.0480e+03,\n",
      "        8.3920e+03, 1.5200e+04, 3.1660e+03, 2.5824e+04, 1.2260e+03, 9.3840e+03,\n",
      "               inf, 5.2320e+03, 1.6380e+03, 1.0088e+04, 9.8560e+03, 1.4552e+04,\n",
      "        6.5500e+02, 3.0140e+03, 5.9440e+03, 1.5944e+04, 1.8410e+03, 2.5980e+03,\n",
      "        1.0536e+04, 3.7120e+04, 1.1288e+04, 1.4344e+04, 1.5096e+04, 2.5620e+03,\n",
      "        3.4720e+04, 6.0080e+03, 2.2960e+04, 4.2880e+03, 3.0672e+04, 2.8272e+04,\n",
      "        8.7840e+03, 1.3920e+04, 5.6500e+02, 1.0770e+03, 1.5010e+03, 6.1080e+03,\n",
      "        2.8768e+04, 3.2080e+04, 1.3880e+04, 2.8144e+04, 1.9456e+04, 1.4664e+04,\n",
      "        2.2752e+04, 1.2800e+03, 2.5008e+04, 3.9300e+02, 1.6144e+04, 2.1808e+04,\n",
      "        4.5600e+03, 1.7940e+03, 1.6688e+04, 1.3912e+04, 2.0480e+04, 4.6880e+03,\n",
      "        9.0880e+03, 7.8160e+03, 8.4000e+03, 8.5360e+03, 3.5675e+02, 1.8720e+04,\n",
      "        4.0320e+04, 6.1560e+03, 3.4624e+04, 6.0400e+03, 6.8400e+03, 8.7520e+03,\n",
      "        8.1720e+03, 6.6280e+03, 2.2592e+04, 1.3912e+04, 2.1216e+04, 3.2144e+04,\n",
      "        3.3600e+04, 1.3440e+03, 1.8040e+03, 1.9504e+04, 9.4000e+03, 1.2920e+04,\n",
      "        9.5840e+03, 1.2580e+03, 2.3700e+03, 1.1936e+04, 6.5440e+03, 6.7880e+03,\n",
      "        7.9440e+03, 1.5090e+03, 5.9480e+03, 3.1440e+04, 2.7504e+04, 5.8160e+03,\n",
      "        2.0192e+04, 5.1552e+04, 2.9180e+03, 1.1808e+04, 1.6624e+04, 3.3000e+02,\n",
      "        5.4920e+03, 2.1744e+04, 8.6160e+03, 1.2632e+04, 8.9950e+02, 2.7100e+03,\n",
      "        1.2712e+04, 4.9520e+03, 1.6690e+03, 2.0350e+03, 1.7860e+03, 2.7792e+04,\n",
      "        1.0936e+04, 5.8000e+03, 6.0384e+04, 1.1540e+03, 1.7616e+04, 9.3440e+03,\n",
      "        1.7584e+04, 6.7760e+03, 2.7200e+04, 3.3376e+04, 1.0510e+03, 3.9008e+04,\n",
      "        2.9340e+03, 6.6280e+03, 1.1112e+04, 4.7360e+04, 5.8600e+03, 9.0080e+03,\n",
      "        8.7760e+03, 1.2008e+04, 1.8860e+03, 2.8620e+03, 1.7664e+04, 4.0200e+03,\n",
      "        2.5920e+03, 8.8160e+03, 9.5120e+03, 1.6910e+03, 3.5420e+03, 1.7180e+03,\n",
      "        1.1104e+04, 2.9232e+04, 1.2376e+04, 2.9520e+04, 4.1696e+04, 6.1560e+03,\n",
      "        2.4656e+04, 6.1480e+03, 6.4240e+03, 2.4128e+04, 1.2080e+04, 3.4400e+04,\n",
      "        6.8800e+03, 1.0780e+03, 1.3072e+04, 4.6176e+04, 5.1232e+04, 8.1280e+03,\n",
      "        9.9120e+03, 1.0536e+04, 7.6840e+03, 1.4230e+03, 2.6160e+03, 1.7024e+04,\n",
      "        6.5360e+03, 1.8080e+04, 1.1520e+04, 2.4680e+03, 3.4420e+03, 4.2900e+02,\n",
      "        8.8320e+03, 1.6688e+02, 7.3240e+03, 3.1152e+04, 2.3312e+04, 1.1656e+04,\n",
      "        7.8500e+02, 9.5840e+03, 4.8256e+04, 8.8600e+02, 3.7824e+04, 5.1712e+04,\n",
      "        9.7360e+03, 6.0880e+03, 1.3328e+04, 1.1512e+04, 2.0608e+04,        inf,\n",
      "        3.4496e+04, 1.9856e+04, 4.4240e+03, 6.5440e+03, 4.8760e+03, 1.3752e+04,\n",
      "        1.9730e+03, 5.0320e+03, 1.3432e+04, 1.2080e+04, 2.8848e+04, 1.3328e+04,\n",
      "        5.6250e+00, 9.7680e+03, 1.7580e+03, 1.5248e+04, 2.3880e+03, 2.2560e+04,\n",
      "        1.8592e+04, 2.8380e+03, 2.2480e+04, 1.3860e+03, 6.5200e+03, 2.7560e+03,\n",
      "        4.2720e+03, 1.1768e+04, 1.5448e+04, 1.0150e+03, 1.9780e+03, 3.1980e+03,\n",
      "        9.8240e+03, 1.8928e+04, 2.4688e+04, 6.7280e+03, 2.5660e+03, 1.0448e+04,\n",
      "        1.0912e+04, 7.3960e+03, 1.9248e+04, 5.7160e+03, 1.2600e+04, 5.6480e+03,\n",
      "        1.0040e+04, 3.4272e+04, 1.1328e+04, 9.3600e+03, 4.5504e+04, 3.3340e+03,\n",
      "        2.5280e+04, 3.4975e+02, 6.7240e+03,        inf, 4.1080e+03, 1.6736e+04,\n",
      "        1.6264e+04, 2.4525e+02, 6.3488e+04, 3.7024e+04, 2.7344e+04, 2.4620e+03,\n",
      "        4.9960e+03, 2.2920e+03, 3.5776e+04, 1.4984e+04, 1.5448e+04, 4.3104e+04,\n",
      "        3.4080e+04, 1.7088e+04, 2.1400e+03, 1.2584e+04, 2.0920e+03, 1.2536e+04,\n",
      "        3.1680e+03, 1.6336e+04, 4.7760e+03,        inf, 4.4256e+04, 9.7280e+03,\n",
      "        2.9904e+04, 2.8256e+04,        inf, 6.5520e+03, 1.7632e+04, 1.3752e+04,\n",
      "        1.6816e+04, 3.4944e+04, 3.1540e+03, 2.2512e+04, 1.4984e+04, 8.4000e+03,\n",
      "        5.4560e+04, 1.0968e+04, 7.7360e+03, 2.9168e+04, 6.1472e+04, 1.8912e+04,\n",
      "        4.0320e+03, 5.0760e+03, 1.3568e+04, 2.8128e+04, 8.6480e+03, 2.1728e+04,\n",
      "        8.9200e+03, 4.0512e+04, 4.4576e+04, 8.7520e+03, 8.5920e+03, 4.5160e+03,\n",
      "        1.0688e+04, 1.5128e+04, 3.2420e+03, 8.3120e+03, 4.9080e+03, 1.7696e+04,\n",
      "        5.5920e+03, 4.4600e+03, 1.0704e+04, 1.7664e+04,        inf, 2.3560e+03,\n",
      "        5.1200e+03, 3.3728e+04, 6.2800e+03, 9.2720e+03, 7.7640e+03, 4.2520e+03,\n",
      "        4.3392e+04, 4.0280e+03, 8.8750e+02, 2.7712e+04, 4.3776e+04, 1.5656e+04,\n",
      "        3.0760e+03, 1.9970e+03, 2.0000e+04, 1.6800e+04, 2.2320e+04, 5.5168e+04,\n",
      "        3.3184e+04, 1.8448e+04, 3.6020e+03, 1.0096e+04, 7.1960e+03,        inf,\n",
      "        2.5872e+04, 2.5248e+04, 1.2272e+04, 6.1024e+04, 1.7330e+03, 2.7040e+04,\n",
      "        8.7840e+03, 1.5456e+04, 4.1568e+04, 9.4720e+03, 2.8656e+04, 1.6072e+04,\n",
      "        5.6480e+03, 3.5584e+04, 2.0992e+04, 1.2460e+03, 6.9600e+03, 5.4000e+03,\n",
      "        1.3216e+04, 1.6784e+04, 1.0488e+04, 7.5000e+03, 1.6960e+04, 1.2224e+04,\n",
      "        1.1976e+04, 1.2000e+04], device='cuda:0', dtype=torch.float16), tensor([1.6704e+04,        inf, 8.4320e+03, 2.8100e+03, 1.4056e+04, 3.3600e+04,\n",
      "        1.3200e+04, 2.7984e+04, 3.7152e+04, 5.4560e+04, 3.7632e+04, 2.4480e+04,\n",
      "        4.6816e+04, 2.3456e+04, 2.2448e+04, 2.7344e+04, 6.7160e+03, 1.6064e+04,\n",
      "        1.8256e+04, 8.7280e+03, 1.6432e+04, 1.0744e+04, 3.8304e+04, 1.1584e+04,\n",
      "        6.1984e+04, 7.1960e+03, 4.0224e+04, 1.4016e+04, 3.7536e+04, 2.6736e+04,\n",
      "        1.9152e+04, 2.5540e+03, 1.4536e+04, 1.4488e+04, 1.7648e+04,        inf,\n",
      "        3.2304e+04, 9.8320e+03, 1.8784e+04,        inf, 6.9880e+03, 4.4096e+04,\n",
      "        2.9232e+04, 4.5216e+04, 1.8944e+04, 3.1700e+03, 4.9984e+04, 9.8480e+03,\n",
      "        4.4960e+04, 1.2048e+04, 1.8180e+03, 1.3220e+03, 2.6624e+04, 2.1740e+03,\n",
      "        2.4048e+04, 1.3488e+04, 3.1952e+04, 2.1856e+04, 1.7072e+04, 4.1856e+04,\n",
      "        4.1632e+04, 3.2864e+04, 1.4712e+04, 1.0328e+04, 1.4888e+04, 2.9264e+04,\n",
      "        2.8208e+04, 8.3040e+03, 2.0800e+04, 9.4950e+02, 6.2600e+03, 3.8912e+04,\n",
      "        2.5760e+04, 1.1184e+04, 2.4336e+04, 6.3240e+03,        inf, 2.4000e+04,\n",
      "        3.2620e+03, 1.2072e+04, 2.7920e+04, 1.5552e+04, 1.5680e+04, 3.0352e+04,\n",
      "        1.0408e+04, 3.2768e+04, 1.8912e+04, 3.3280e+04, 3.0304e+04, 1.0608e+04,\n",
      "        3.5168e+04, 5.6736e+04, 4.4128e+04, 4.9600e+04, 2.3840e+04, 2.6880e+03,\n",
      "        4.9536e+04, 2.3504e+04, 1.6512e+04, 5.3280e+04, 1.9184e+04, 7.3360e+03,\n",
      "        9.4880e+03, 1.9744e+04, 6.3680e+03, 1.2360e+04, 3.1120e+04, 9.2400e+03,\n",
      "        6.2848e+04, 4.0768e+04, 4.0800e+04, 1.0024e+04, 3.1680e+04, 2.3632e+04,\n",
      "        1.6736e+04, 5.9872e+04, 7.5750e+02, 1.2048e+04, 1.9616e+04, 1.8944e+04,\n",
      "        3.9584e+04, 1.7504e+04, 5.4360e+03, 4.5088e+04, 2.4960e+04, 6.1568e+04,\n",
      "        1.0776e+04, 5.4304e+04, 3.7472e+04, 2.5760e+04, 1.1480e+04, 2.9696e+04,\n",
      "        1.0104e+04, 9.0000e+03, 1.8570e+03, 1.8208e+04, 6.1472e+04, 2.7136e+04,\n",
      "        2.3888e+04, 6.1120e+03, 7.8200e+03, 3.2304e+04, 3.7344e+04, 2.2128e+04,\n",
      "        5.2096e+04, 4.5040e+03, 1.9152e+04, 2.2960e+04, 4.5632e+04, 4.1400e+03,\n",
      "        2.0080e+04, 2.1024e+04, 6.2368e+04, 2.7312e+04, 1.5936e+04, 2.5152e+04,\n",
      "        6.6200e+03, 1.5944e+04, 1.0496e+04, 5.8880e+04, 3.1620e+03, 4.7936e+04,\n",
      "        4.6400e+04, 1.0592e+04, 1.5552e+04, 3.7856e+04, 1.1080e+04, 4.2368e+04,\n",
      "               inf, 3.6224e+04,        inf, 1.4224e+04, 1.2940e+03, 1.5976e+04,\n",
      "        1.5544e+04, 3.1376e+04, 3.3152e+04, 2.2144e+04, 3.0960e+04, 2.9000e+03,\n",
      "        3.5648e+04, 1.1104e+04, 3.5904e+04, 7.2120e+03, 4.1728e+04, 5.2864e+04,\n",
      "        3.9712e+04, 5.2384e+04, 3.9460e+03, 7.8520e+03, 1.8816e+04, 8.8160e+03,\n",
      "        4.8640e+04, 4.6848e+04, 2.5520e+04, 6.3328e+04, 6.3200e+04, 1.7296e+04,\n",
      "        2.2592e+04, 9.2800e+03, 1.4376e+04, 9.9850e+02, 3.9776e+04, 6.4640e+04,\n",
      "        5.7184e+04, 1.8160e+04, 2.7536e+04, 1.8800e+04, 1.7712e+04, 1.5968e+04,\n",
      "        2.4320e+04, 9.4240e+03, 3.4880e+04, 3.9040e+04, 5.3240e+03, 2.4320e+04,\n",
      "        2.4464e+04, 1.7504e+04, 4.5824e+04, 7.3640e+03, 2.7280e+04, 1.5976e+04,\n",
      "        3.8848e+04, 8.8880e+03, 2.9216e+04, 1.9088e+04, 8.7520e+03, 5.1232e+04,\n",
      "        5.2864e+04, 9.5300e+02, 2.8820e+03, 2.3952e+04, 1.8880e+04, 5.7856e+04,\n",
      "        1.8960e+04, 2.6360e+03, 3.9744e+04, 3.9872e+04, 3.0592e+04, 1.5488e+04,\n",
      "        1.8528e+04, 4.8840e+03, 1.2912e+04,        inf, 4.2688e+04, 1.4384e+04,\n",
      "        1.9792e+04, 5.9392e+04, 2.0944e+04, 1.1432e+04, 5.2800e+04, 1.6024e+04,\n",
      "        1.3304e+04, 2.9680e+04, 1.6448e+04, 2.5056e+04, 2.2520e+03, 3.0512e+04,\n",
      "        3.6704e+04, 1.7400e+03, 3.5072e+04, 5.0280e+03, 3.5872e+04, 2.4912e+04,\n",
      "        1.6072e+04, 1.1480e+04,        inf, 1.5096e+04, 3.7216e+04,        inf,\n",
      "        2.4864e+04, 1.6560e+04, 2.3040e+04, 4.5728e+04, 1.0880e+04, 3.5008e+04,\n",
      "        1.9250e+03, 3.6544e+04, 2.2656e+04, 6.0576e+04, 1.5088e+04, 1.7120e+04,\n",
      "        2.0192e+04, 2.3328e+04, 2.8720e+04, 1.0660e+03, 2.4704e+04, 1.1408e+04,\n",
      "        1.4080e+04, 2.0384e+04, 3.6448e+04, 6.3880e+03, 1.0016e+04, 8.4160e+03,\n",
      "        2.4608e+04, 5.9552e+04, 1.5432e+04, 2.8272e+04, 5.2320e+04, 6.4960e+03,\n",
      "        3.0928e+04, 5.1232e+04, 1.2688e+04, 3.8880e+04, 4.5920e+04, 5.1616e+04,\n",
      "        5.3520e+03, 3.4740e+03, 3.9136e+04, 5.1872e+04, 5.4144e+04, 3.6576e+04,\n",
      "        2.1040e+04, 1.4264e+04, 1.7264e+04, 9.5440e+03, 1.6216e+04, 2.7840e+04,\n",
      "        2.3904e+04, 2.8000e+04, 3.2528e+04, 5.0560e+04, 4.0192e+04, 3.9040e+04,\n",
      "        1.8464e+04, 5.5840e+03, 5.3720e+03, 2.3408e+04, 2.7520e+04, 2.6080e+04,\n",
      "        1.6512e+04, 1.6480e+04, 4.8512e+04, 2.5904e+04, 4.8352e+04, 3.0320e+04,\n",
      "        1.8400e+04, 1.8730e+03, 1.5160e+04, 4.6624e+04, 4.1760e+03,        inf,\n",
      "        2.5664e+04, 2.2064e+04, 4.0480e+04, 1.3256e+04, 4.3680e+03, 1.8240e+04,\n",
      "        1.5528e+04, 3.8080e+04, 5.8784e+04, 3.3472e+04, 5.8208e+04, 3.9616e+04,\n",
      "        5.0880e+04, 2.8576e+04, 4.1400e+03, 3.1600e+04, 1.7280e+04, 6.0448e+04,\n",
      "        2.5712e+04, 7.5920e+03,        inf, 5.4350e+02, 8.2240e+03, 2.2800e+03,\n",
      "        2.2944e+04, 3.1216e+04, 5.3568e+04, 4.1520e+03, 1.4088e+04, 1.6580e+03,\n",
      "        6.5880e+03, 1.2064e+04, 3.1360e+04, 5.2840e+03, 9.3200e+03, 4.2432e+04,\n",
      "        1.6688e+04, 9.3600e+03, 2.9504e+04, 1.0344e+04, 1.8208e+04, 1.0048e+04,\n",
      "        1.6576e+04, 3.7536e+04, 2.4624e+04, 6.1792e+04, 4.1984e+04, 9.5440e+03,\n",
      "        2.3104e+04, 4.1720e+03, 4.4920e+03,        inf, 7.2280e+03, 1.1264e+04,\n",
      "        1.9664e+04,        inf,        inf, 4.6112e+04, 5.5072e+04, 2.2400e+03,\n",
      "        3.3580e+03, 1.1928e+04, 3.8880e+04, 2.2352e+04, 2.8992e+04, 4.6496e+04,\n",
      "        3.1648e+04, 2.8768e+04, 3.4160e+03, 1.3936e+04, 4.8938e+01, 1.9408e+04,\n",
      "        1.0256e+04, 2.3104e+04, 5.0080e+03,        inf, 5.3248e+04, 1.9872e+04,\n",
      "        3.3504e+04, 3.9264e+04,        inf, 9.6240e+03, 1.6800e+04, 1.8064e+04,\n",
      "        3.3632e+04, 5.6480e+04, 4.7200e+03, 2.2784e+04, 2.7456e+04, 4.3960e+03,\n",
      "        6.0192e+04, 4.4928e+04, 3.2740e+03, 4.0544e+04, 5.7856e+04, 1.8912e+04,\n",
      "        4.0416e+04, 4.8480e+03, 3.1440e+04, 3.3920e+04, 1.1496e+04, 2.0304e+04,\n",
      "        3.2880e+03, 3.0064e+04, 4.3168e+04, 2.9248e+04, 5.4050e+02, 1.0224e+04,\n",
      "        1.6768e+04, 2.3440e+04, 1.0920e+04, 1.2672e+04, 2.3160e+03, 1.9088e+04,\n",
      "        9.6240e+03, 5.6960e+03, 4.6080e+04, 1.2216e+04, 5.9328e+04, 2.0720e+04,\n",
      "        3.3536e+04, 3.2320e+04, 3.8080e+04, 9.5760e+03, 1.3160e+04, 3.5140e+03,\n",
      "        5.3856e+04, 3.6032e+04, 1.0990e+03, 2.8800e+04, 6.3296e+04, 2.1152e+04,\n",
      "        1.7300e+03, 8.1200e+03, 2.5472e+04, 2.7776e+04, 4.1408e+04,        inf,\n",
      "        5.0656e+04, 2.2976e+04, 1.9808e+04, 5.0304e+04, 1.5304e+04, 6.0672e+04,\n",
      "        3.8208e+04, 2.8848e+04, 7.9840e+03,        inf, 9.9920e+03, 2.6912e+04,\n",
      "        1.5072e+04, 2.8720e+04,        inf, 1.7088e+04, 4.9536e+04, 3.3600e+04,\n",
      "        1.0392e+04, 2.5488e+04, 2.4288e+04, 3.7240e+03, 3.2736e+04, 6.9480e+03,\n",
      "        2.2464e+04, 2.3056e+04, 6.6960e+03,        inf, 2.0992e+04, 3.9360e+04,\n",
      "        1.0704e+04, 1.8896e+04], device='cuda:0', dtype=torch.float16), tensor([[  264.2500,   769.0000,   508.2500,  ...,   360.0000,  1401.0000,\n",
      "          1300.0000],\n",
      "        [  370.2500,   664.5000,    37.3125,  ...,   347.0000,  1027.0000,\n",
      "           726.5000],\n",
      "        [   85.6875,   359.2500,   145.5000,  ...,   186.5000,   780.5000,\n",
      "           765.0000],\n",
      "        ...,\n",
      "        [11024.0000, 13224.0000,  7232.0000,  ...,  8968.0000, 21424.0000,\n",
      "          6640.0000],\n",
      "        [ 4696.0000,  4956.0000,   548.0000,  ...,  5320.0000, 10864.0000,\n",
      "          7144.0000],\n",
      "        [ 5264.0000,  3084.0000,  5596.0000,  ...,  1503.0000,  2740.0000,\n",
      "          8248.0000]], device='cuda:0', dtype=torch.float16), tensor([ 1393.0000,  1020.5000,   839.0000,  ..., 19152.0000, 10968.0000,\n",
      "         1599.0000], device='cuda:0', dtype=torch.float16), tensor([[ 7680.0000,  3256.0000,   522.0000,  ...,  2812.0000,  5824.0000,\n",
      "          3638.0000],\n",
      "        [ 3320.0000, 37024.0000, 11800.0000,  ...,  2240.0000, 26800.0000,\n",
      "          4006.0000],\n",
      "        [ 2320.0000,  3552.0000,  1346.0000,  ...,   996.0000,  4840.0000,\n",
      "           520.0000],\n",
      "        ...,\n",
      "        [ 7420.0000, 24464.0000,  6756.0000,  ...,  5088.0000, 11824.0000,\n",
      "          1974.0000],\n",
      "        [ 1882.0000,   462.5000,   621.0000,  ...,  1258.0000,   560.5000,\n",
      "           563.5000],\n",
      "        [  583.5000,  2424.0000,  1197.0000,  ...,   667.0000,  3922.0000,\n",
      "          1536.0000]], device='cuda:0', dtype=torch.float16), tensor([1.4224e+04,        inf, 1.2864e+04, 6.7160e+03, 1.2816e+04, 2.3584e+04,\n",
      "        4.9880e+03, 2.3936e+04, 4.0000e+04, 2.4256e+04, 4.3104e+04, 4.1640e+03,\n",
      "        3.6736e+04, 3.1072e+04, 3.0208e+04, 1.3504e+04, 2.3280e+04, 3.1408e+04,\n",
      "        1.1024e+04, 1.4192e+04, 1.8576e+04, 1.1190e+03, 5.3056e+04, 3.0192e+04,\n",
      "        3.2672e+04, 1.6688e+04, 3.3600e+04, 6.9920e+03, 2.1392e+04, 2.7936e+04,\n",
      "        1.5290e+03, 1.0096e+04, 6.5760e+03, 6.0200e+03, 9.0080e+03, 6.0992e+04,\n",
      "        2.6624e+04, 9.0000e+03, 3.0672e+04,        inf, 7.1720e+03, 4.9696e+04,\n",
      "        2.1312e+04, 4.5248e+04, 1.9216e+04, 5.3960e+03, 2.8384e+04, 2.6760e+03,\n",
      "        4.6464e+04, 1.7460e+03, 1.3584e+04, 8.9360e+03, 5.4600e+03, 7.3240e+03,\n",
      "        3.2832e+04, 2.2736e+04, 5.4040e+03, 6.7280e+03, 6.9080e+03, 3.8112e+04,\n",
      "        4.7008e+04, 1.4320e+04, 3.3900e+03, 7.6120e+03, 2.2112e+04, 2.9760e+04,\n",
      "        1.5168e+04, 5.7320e+03, 1.1944e+04, 1.0392e+04, 4.3920e+03, 4.4480e+04,\n",
      "        2.6112e+04, 3.5660e+03, 4.6240e+04, 1.0088e+04, 4.4448e+04, 1.8128e+04,\n",
      "        1.3984e+04, 2.2032e+04, 2.6592e+04, 9.3920e+03, 1.1312e+04, 2.0016e+04,\n",
      "        2.5960e+03, 4.2592e+04, 1.7024e+04, 2.7440e+04, 1.8224e+04, 3.7088e+04,\n",
      "        2.7072e+04, 4.4928e+04, 4.9888e+04, 3.7984e+04, 2.2496e+04, 1.4792e+04,\n",
      "        2.9008e+04, 1.6864e+04, 1.8970e+03, 4.7168e+04, 1.3576e+04, 1.8192e+04,\n",
      "        1.5816e+04, 4.5240e+03, 1.7424e+04, 1.5896e+04, 1.0352e+04, 8.7700e+02,\n",
      "        5.5872e+04, 2.1504e+04, 5.9360e+04, 7.1600e+03, 1.9920e+04, 4.4200e+03,\n",
      "        2.6048e+04, 5.9776e+04, 5.8280e+03, 2.8220e+03, 1.6440e+03, 1.4648e+04,\n",
      "        2.9248e+04, 1.1576e+04, 1.5816e+04, 3.8048e+04, 2.8288e+04, 3.8848e+04,\n",
      "        7.3160e+03, 4.5696e+04, 1.5856e+04, 5.2469e+01, 1.3250e+03, 3.9520e+04,\n",
      "        4.2325e+02, 3.4784e+04, 3.7420e+03, 1.6592e+04, 5.1456e+04, 2.4960e+04,\n",
      "        1.2456e+04, 4.0580e+03, 2.0120e+03, 1.7696e+04, 3.3184e+04, 2.3552e+04,\n",
      "        5.1680e+04, 1.5872e+04, 2.5552e+04, 3.0176e+04, 3.8528e+04, 7.0680e+03,\n",
      "        2.1936e+04, 1.1944e+04, 5.2608e+04, 7.0250e+02, 2.7152e+04, 2.5392e+04,\n",
      "        2.4784e+04, 9.8640e+03, 3.0576e+04, 4.8960e+04, 9.9200e+02, 5.3024e+04,\n",
      "        3.3792e+04, 1.1920e+04, 1.7440e+04, 3.0896e+04, 1.5416e+04, 2.8368e+04,\n",
      "               inf, 4.8928e+04,        inf, 2.2016e+04, 4.8440e+03, 5.5200e+03,\n",
      "        1.5080e+04, 3.1728e+04, 2.1824e+04, 2.1040e+04, 2.8720e+04, 6.1760e+03,\n",
      "        3.7504e+04, 9.0800e+03, 5.5008e+04, 1.1080e+04, 2.1040e+04, 6.0064e+04,\n",
      "        4.3456e+04, 4.9344e+04, 2.0900e+02, 2.2832e+04, 2.2360e+03, 1.5160e+04,\n",
      "        5.3728e+04, 4.7808e+04, 1.0664e+04, 6.0736e+04,        inf, 5.4840e+03,\n",
      "        5.8680e+03, 1.5464e+04, 2.9408e+04, 2.2176e+04, 4.3872e+04, 3.8880e+04,\n",
      "        3.7824e+04, 2.0188e+02, 1.6928e+04, 4.7160e+03, 1.1232e+04, 1.7472e+04,\n",
      "        1.8688e+04, 7.0100e+02, 2.3136e+04, 3.6704e+04, 1.0016e+04, 1.6800e+04,\n",
      "        3.6224e+04, 2.0256e+04, 5.0528e+04, 1.5904e+04, 2.8112e+04, 1.0600e+04,\n",
      "        3.7152e+04, 2.1120e+03, 1.9776e+04, 1.6016e+04, 1.9440e+04, 5.8400e+04,\n",
      "        5.1072e+04, 9.0160e+03, 4.9680e+03, 1.6352e+04, 2.2896e+04, 5.0976e+04,\n",
      "        1.8240e+04, 2.1400e+03, 4.4192e+04, 2.7776e+04, 3.5648e+04, 1.3832e+04,\n",
      "        3.4400e+04, 5.1720e+03, 1.0224e+04,        inf, 4.0896e+04, 3.0432e+04,\n",
      "        2.8384e+04, 6.2176e+04, 2.5216e+04, 1.2368e+04, 5.4496e+04, 1.0904e+04,\n",
      "        3.2272e+04, 4.2336e+04, 1.5888e+04, 6.9240e+03, 2.2448e+04, 1.7296e+04,\n",
      "        3.1584e+04, 3.7960e+03, 5.3152e+04, 1.3080e+04, 3.1216e+04, 7.8360e+03,\n",
      "        3.4400e+04, 3.4380e+03,        inf, 1.0160e+04, 4.9056e+04, 4.4768e+04,\n",
      "        2.2256e+04, 7.9800e+03, 2.4432e+04, 2.8832e+04, 6.4560e+03, 5.4112e+04,\n",
      "        9.9920e+03, 2.6784e+04, 1.2072e+04, 4.9728e+04, 7.5440e+03, 6.6760e+03,\n",
      "        1.9296e+04, 3.0592e+04, 2.5040e+04, 2.7740e+03, 7.4880e+03, 2.9960e+03,\n",
      "        1.1552e+04, 1.0864e+04, 4.7904e+04, 1.4200e+04, 1.5336e+04, 1.2016e+04,\n",
      "        2.7472e+04, 5.3472e+04, 1.2104e+04, 2.2656e+04, 3.6064e+04, 6.2000e+03,\n",
      "        1.3616e+04, 5.3824e+04, 3.9640e+03, 9.0000e+03, 2.8848e+04, 4.2496e+04,\n",
      "        5.0040e+03, 9.9920e+03, 3.2688e+04, 6.0736e+04,        inf, 1.7552e+04,\n",
      "        1.1512e+04, 1.5080e+03, 2.2512e+04, 5.7080e+03, 5.1050e+02, 2.6768e+04,\n",
      "        1.1224e+04, 2.1456e+04, 2.2032e+04, 3.4944e+04, 1.2992e+04, 1.5680e+04,\n",
      "        2.5040e+04, 2.2100e+03, 7.9240e+03, 1.9424e+04, 2.9968e+04, 1.2864e+04,\n",
      "        7.4280e+03, 7.6120e+03, 3.5168e+04, 2.2672e+04, 5.3216e+04, 2.0944e+04,\n",
      "        2.7100e+02, 1.9584e+04, 2.5616e+04, 4.2144e+04, 1.2872e+04, 6.2880e+04,\n",
      "        2.6608e+04, 3.1472e+04, 3.0368e+04, 1.7360e+04, 2.0660e+03, 4.2944e+04,\n",
      "        1.4184e+04, 3.3568e+04, 3.9872e+04, 3.4912e+04, 4.2080e+04, 3.7248e+04,\n",
      "        4.5696e+04, 2.6624e+04, 1.6090e+03, 3.2752e+04, 1.1416e+04, 4.7776e+04,\n",
      "        1.5472e+04, 9.0560e+03,        inf, 1.6290e+03, 9.9360e+03, 2.0144e+04,\n",
      "        2.7728e+04, 2.6208e+04, 3.4176e+04, 3.8280e+03, 1.9920e+04, 3.8920e+03,\n",
      "        1.2496e+04, 1.6152e+04, 2.7584e+04, 1.6570e+03, 8.1000e+03, 5.3376e+04,\n",
      "        9.2640e+03, 1.3170e+03, 2.3968e+04, 6.6560e+03, 1.4352e+04, 3.4740e+03,\n",
      "        7.0900e+02, 3.5488e+04, 9.7040e+03, 4.6048e+04, 2.8944e+04, 6.5400e+03,\n",
      "        2.7520e+04, 3.1625e+02, 1.7212e+02,        inf, 6.0240e+03, 2.2048e+04,\n",
      "        4.4096e+04, 5.0976e+04,        inf, 4.1280e+04, 4.7872e+04, 2.5812e+01,\n",
      "        6.9120e+03, 1.3712e+04, 3.6320e+04, 2.3408e+04, 3.0976e+04, 4.0768e+04,\n",
      "        3.8400e+04, 2.1104e+04, 1.5920e+04, 3.3248e+04, 7.5760e+03, 1.4224e+04,\n",
      "        5.6000e+03, 1.1680e+04, 6.8680e+03,        inf, 5.2064e+04, 9.7520e+03,\n",
      "        1.2512e+04, 3.3248e+04,        inf, 1.0768e+04, 1.9344e+04, 1.4240e+04,\n",
      "        4.1440e+04,        inf, 1.7024e+04, 4.0224e+04, 3.4144e+04, 1.2725e+02,\n",
      "        4.4576e+04, 3.6160e+04, 1.5568e+04, 3.4144e+04, 5.3696e+04, 2.3920e+04,\n",
      "        2.8512e+04, 5.8920e+03, 1.9216e+04, 2.4256e+04, 1.9968e+04, 4.8720e+03,\n",
      "        3.6512e+04, 4.2250e+02, 2.5456e+04, 2.7824e+04, 3.4600e+03, 2.1300e+03,\n",
      "        2.9440e+03, 3.4048e+04, 1.6470e+03, 2.3760e+04, 4.9760e+03, 2.1888e+04,\n",
      "        5.7080e+03, 2.8200e+03, 3.9776e+04, 1.0024e+04, 6.4960e+04, 3.9744e+04,\n",
      "        3.7376e+04, 1.7568e+04, 3.4016e+04, 4.5080e+03, 2.4740e+03, 1.0312e+04,\n",
      "        5.5136e+04, 4.0064e+04, 1.7712e+04, 8.9120e+03,        inf, 3.3088e+04,\n",
      "        8.4400e+03, 6.7480e+03, 1.3960e+04, 1.2248e+04, 2.5072e+04, 5.5680e+04,\n",
      "               inf, 1.6656e+04, 1.5904e+04, 5.8144e+04, 2.8352e+04,        inf,\n",
      "        2.2752e+04, 3.4272e+04, 2.2896e+04,        inf, 9.7920e+03, 7.6800e+03,\n",
      "        8.7680e+03, 4.8352e+04, 6.1664e+04, 2.4384e+04, 1.8688e+04, 2.9328e+04,\n",
      "        6.5120e+03, 1.4208e+04, 1.0216e+04, 3.0700e+03, 3.2080e+04, 1.1580e+03,\n",
      "        1.2504e+04, 1.9232e+04, 2.2096e+04,        inf, 5.5120e+03, 4.5888e+04,\n",
      "        8.8400e+02, 1.2712e+04], device='cuda:0', dtype=torch.float16), tensor([[18048.0000, 27472.0000,  4616.0000,  ..., 14320.0000, 30096.0000,\n",
      "         31328.0000],\n",
      "        [   93.8750,   780.5000,   201.0000,  ...,  1034.0000,  1089.0000,\n",
      "           803.0000],\n",
      "        [ 5176.0000,  7648.0000,  2037.0000,  ...,  3036.0000, 12824.0000,\n",
      "          7140.0000],\n",
      "        ...,\n",
      "        [  705.0000,   923.0000,  1879.0000,  ...,  1102.0000,  3400.0000,\n",
      "          1011.0000],\n",
      "        [ 1291.0000,   551.5000,    82.6250,  ...,   289.7500,  1106.0000,\n",
      "           203.8750],\n",
      "        [ 4132.0000,  4496.0000,  2132.0000,  ...,   808.0000,  7504.0000,\n",
      "          3448.0000]], device='cuda:0', dtype=torch.float16), tensor([31952.0000,    87.8125, 11144.0000,  ...,  1861.0000,   627.5000,\n",
      "         6408.0000], device='cuda:0', dtype=torch.float16), tensor([[ 3358.0000,  1302.0000,  4248.0000,  ...,  8112.0000,   220.0000,\n",
      "          1963.0000],\n",
      "        [29952.0000,  1847.0000,  7264.0000,  ..., 19248.0000,  2966.0000,\n",
      "          6280.0000],\n",
      "        [ 5284.0000,   783.0000,  2732.0000,  ...,  5640.0000,   488.0000,\n",
      "          1989.0000],\n",
      "        ...,\n",
      "        [15832.0000,  1094.0000,  3928.0000,  ..., 11504.0000,  1491.0000,\n",
      "          2490.0000],\n",
      "        [  143.6250,   196.6250,  2912.0000,  ...,  6112.0000,   834.0000,\n",
      "           793.0000],\n",
      "        [ 1528.0000,   300.5000,  1509.0000,  ...,  3320.0000,   454.5000,\n",
      "           924.0000]], device='cuda:0', dtype=torch.float16), tensor([6.2560e+03,        inf, 1.2432e+04, 1.4376e+04, 2.6020e+03, 1.9840e+04,\n",
      "        2.4980e+03, 1.1632e+04, 4.4448e+04, 3.9904e+04, 3.1840e+04, 1.7744e+04,\n",
      "        4.1856e+04, 2.8432e+04, 8.6080e+03, 3.6080e+03, 2.5152e+04, 1.7152e+04,\n",
      "        9.8880e+03, 2.2896e+04, 3.1632e+04, 4.7950e+02, 4.9664e+04, 2.8592e+04,\n",
      "        3.9904e+04, 2.4000e+04, 2.8640e+04, 4.9720e+03, 2.9936e+04, 1.8976e+04,\n",
      "        4.4880e+03, 1.2072e+04, 1.8400e+04, 1.1328e+04, 8.3120e+03, 5.0336e+04,\n",
      "        2.7712e+04, 1.8704e+04, 4.6656e+04,        inf, 1.2192e+04, 4.4192e+04,\n",
      "        1.5832e+04, 4.7104e+04, 1.5424e+04, 2.9520e+03, 2.8384e+04, 8.3760e+03,\n",
      "        5.5296e+04, 9.9440e+03, 2.0064e+04, 2.4048e+04, 3.5860e+03, 1.0512e+04,\n",
      "        3.2496e+04, 1.2240e+04, 2.3000e+03, 3.6680e+03, 4.3960e+03, 3.9840e+04,\n",
      "        2.7680e+04, 1.2992e+04, 8.4960e+03, 1.3816e+04, 1.2656e+04, 2.4816e+04,\n",
      "        1.0808e+04, 9.0480e+03, 1.7560e+03, 1.9104e+04, 1.4824e+04, 4.2560e+04,\n",
      "        2.6720e+04, 8.2960e+03, 3.3696e+04, 2.1580e+03, 2.4768e+04, 2.2416e+04,\n",
      "        8.0360e+03, 8.7360e+03, 2.0624e+04, 2.7800e+03, 1.8672e+04, 1.4440e+04,\n",
      "        8.8000e+02, 4.0960e+04, 2.0128e+04, 2.7232e+04, 2.2256e+04, 2.8160e+04,\n",
      "        3.7024e+04, 3.9200e+04, 3.6096e+04, 4.0192e+04, 1.8320e+04, 1.4488e+04,\n",
      "        3.1984e+04, 1.5208e+04, 2.7040e+03, 4.6880e+04, 1.7024e+04, 1.2304e+04,\n",
      "        1.9664e+04, 2.0210e+03, 8.2720e+03, 8.9120e+03, 4.2960e+03, 4.1880e+03,\n",
      "        5.5072e+04, 1.2856e+04, 3.3216e+04, 1.5560e+04, 7.9840e+03, 1.9610e+03,\n",
      "        2.8880e+04, 5.8432e+04, 1.1660e+03, 6.3240e+03, 4.7880e+03, 2.3840e+04,\n",
      "        2.8464e+04, 8.8720e+03, 1.8944e+04, 3.5136e+04, 2.9936e+04, 4.9504e+04,\n",
      "        1.1456e+04, 4.4416e+04, 1.1136e+04, 1.1152e+04, 3.7300e+03, 4.1568e+04,\n",
      "        4.5720e+03, 2.1200e+04, 2.7600e+03, 4.3640e+03, 5.0560e+04, 2.4832e+04,\n",
      "        8.5360e+03, 1.3296e+04, 1.6060e+03, 8.6240e+03, 3.4080e+04, 3.6192e+04,\n",
      "        4.5184e+04, 8.6240e+03, 1.5144e+04, 1.9952e+04, 3.3856e+04, 2.3640e+03,\n",
      "        3.9968e+04, 3.6938e+01, 5.3184e+04, 1.0504e+04, 3.3856e+04, 2.1792e+04,\n",
      "        2.1232e+04, 1.5680e+03, 2.6848e+04, 2.9776e+04, 3.6960e+03, 3.7760e+04,\n",
      "        2.3200e+04, 9.6240e+03, 1.3384e+04, 4.2432e+04, 5.2720e+03, 2.9024e+04,\n",
      "        6.0224e+04, 4.4160e+04,        inf, 2.7040e+04, 2.2800e+04, 8.5200e+02,\n",
      "        1.0752e+04, 4.0640e+04, 4.5472e+04, 1.1000e+03, 2.2688e+04, 4.2640e+03,\n",
      "        3.4496e+04, 4.8350e+02,        inf, 1.7808e+04, 1.9100e+03, 6.2816e+04,\n",
      "        1.9648e+04, 3.2608e+04, 7.9680e+03, 2.0768e+04, 2.5920e+03, 6.5080e+03,\n",
      "        5.4560e+04, 4.5600e+04, 1.1840e+04, 6.3904e+04,        inf, 1.7280e+04,\n",
      "        6.1440e+03, 3.5200e+03, 1.2968e+04, 2.4336e+04, 3.6000e+04, 5.3600e+04,\n",
      "        4.1792e+04, 9.2250e+01, 1.5288e+04, 2.5720e+03, 1.4192e+04, 4.9280e+03,\n",
      "        1.0920e+04, 3.8320e+03, 1.0144e+04, 3.4144e+04, 2.9640e+03, 2.2208e+04,\n",
      "        2.2480e+04, 7.6280e+03, 5.7152e+04, 1.8752e+04, 8.1360e+03, 1.3144e+04,\n",
      "        2.7728e+04, 1.8976e+04, 3.9008e+04, 1.8240e+04, 2.2768e+04, 5.1232e+04,\n",
      "        4.2144e+04, 8.7440e+03, 7.9440e+03, 1.1056e+04, 1.9536e+04, 4.7488e+04,\n",
      "        1.5870e+03, 2.4500e+03, 4.9568e+04, 4.1728e+04, 3.7760e+04, 1.2512e+04,\n",
      "        3.2800e+04, 9.6000e+03, 4.1760e+03, 5.6064e+04, 4.6304e+04, 3.5552e+04,\n",
      "        2.2272e+04, 4.0160e+04, 1.6672e+04, 1.5304e+04, 4.5120e+04, 2.1184e+04,\n",
      "        2.1536e+04, 5.2640e+04, 1.5304e+04, 1.4262e+02, 1.2000e+04, 9.5200e+03,\n",
      "        1.5384e+04, 3.2740e+03, 5.6160e+04, 6.1600e+03, 1.9760e+04, 6.4040e+03,\n",
      "        3.9840e+04, 1.1664e+04,        inf, 1.1808e+04, 4.4576e+04, 3.1328e+04,\n",
      "        2.4544e+04, 7.2920e+03, 3.8400e+04, 1.1496e+04, 1.1472e+04, 5.2672e+04,\n",
      "        1.1208e+04, 1.5848e+04, 1.5438e+02, 4.8128e+04, 3.7420e+03, 3.7220e+03,\n",
      "        6.9920e+03, 2.1984e+04, 1.8928e+04, 3.3560e+03, 1.4008e+04, 2.0010e+03,\n",
      "        4.8800e+03, 2.9024e+04, 3.3888e+04, 1.2296e+04, 2.3392e+04, 1.8080e+04,\n",
      "        2.3872e+04, 4.2272e+04, 1.8580e+03, 2.4272e+04, 4.2656e+04, 5.0320e+03,\n",
      "        9.5920e+03, 5.6480e+04, 1.1784e+04, 8.5600e+03, 2.1120e+04, 4.8640e+04,\n",
      "        1.8976e+04, 8.4880e+03, 1.7376e+04,        inf,        inf, 9.1200e+03,\n",
      "        1.1944e+04, 5.6920e+03, 3.0864e+04, 2.0128e+04, 2.4820e+03, 2.6736e+04,\n",
      "        2.5296e+04, 1.7488e+04, 2.5504e+04, 1.7792e+04, 9.2560e+03, 2.8608e+04,\n",
      "        2.6704e+04, 3.0480e+03, 3.1820e+03, 1.0240e+04, 1.7824e+04, 9.0000e+03,\n",
      "        7.2280e+03, 6.9160e+03, 2.7056e+04, 3.2208e+04, 5.8080e+04, 1.1368e+04,\n",
      "        8.5680e+03, 1.6752e+04, 9.0480e+03, 3.8272e+04, 8.1320e+03, 6.5248e+04,\n",
      "        3.6864e+04, 2.4304e+04, 3.3824e+04, 6.8280e+03, 8.0040e+03, 4.6560e+04,\n",
      "        6.2360e+03, 2.7888e+04, 4.7584e+04, 4.4192e+04, 3.7760e+04, 4.0256e+04,\n",
      "        3.0720e+04, 2.5568e+04, 1.0296e+04, 2.7600e+04, 3.3240e+03, 4.8416e+04,\n",
      "        2.8080e+04, 1.4688e+04,        inf, 1.0728e+04, 5.2100e+02, 1.1472e+04,\n",
      "        2.0112e+04, 2.9168e+04, 3.9584e+04, 5.8480e+03, 2.5264e+04, 9.1200e+03,\n",
      "        1.2600e+04, 1.3952e+04, 2.2400e+04, 1.3130e+03, 5.5120e+03, 3.8720e+04,\n",
      "        1.2672e+04, 3.4400e+03, 1.9728e+04, 1.2968e+04, 1.6008e+04, 1.8672e+04,\n",
      "        9.8240e+03, 2.1488e+04, 2.0820e+03, 3.6416e+04, 3.0832e+04, 1.4440e+04,\n",
      "        2.5712e+04, 2.9280e+03, 5.9000e+02,        inf, 3.0900e+03, 2.4960e+04,\n",
      "        3.3984e+04, 5.1040e+04, 5.9456e+04, 5.9680e+04, 4.2848e+04, 6.9320e+03,\n",
      "        1.7232e+04, 7.6960e+03, 3.5264e+04, 1.9500e+03, 3.2624e+04, 3.7024e+04,\n",
      "        3.1584e+04, 3.0944e+04, 1.4544e+04, 3.6576e+04, 7.0080e+03, 2.9120e+04,\n",
      "        6.5240e+03, 1.2128e+04, 1.8032e+04,        inf, 5.2032e+04, 1.2272e+04,\n",
      "        9.8640e+03, 1.2664e+04,        inf, 4.1240e+03, 2.4352e+04, 1.6784e+04,\n",
      "        4.1184e+04, 6.1728e+04, 2.2832e+04, 3.9168e+04, 2.2816e+04, 3.7840e+03,\n",
      "        3.4816e+04, 3.4752e+04, 5.3920e+03, 2.3456e+04, 5.5936e+04, 2.6768e+04,\n",
      "        4.6016e+04, 9.4800e+03, 1.3040e+04, 2.2704e+04, 2.1072e+04, 7.4150e+02,\n",
      "        3.8368e+04, 2.6460e+03, 1.6264e+04, 1.4024e+04, 3.6200e+03, 2.9520e+03,\n",
      "        2.1080e+03, 3.4944e+04, 2.6740e+03, 1.1824e+04, 3.2840e+03, 3.7184e+04,\n",
      "        4.2400e+03, 8.2960e+03, 4.4160e+04, 1.6864e+04, 5.9008e+04, 5.1296e+04,\n",
      "        3.7760e+04, 8.0960e+03, 2.3520e+04, 6.6120e+03, 1.3888e+04, 1.0728e+04,\n",
      "        4.5920e+04, 3.2736e+04, 1.5784e+04, 7.3520e+03,        inf, 4.0864e+04,\n",
      "        1.5330e+03, 5.6760e+03, 6.7250e+02, 8.8700e+02, 1.7616e+04, 6.0352e+04,\n",
      "        5.9968e+04, 8.8320e+03, 2.0528e+04, 5.0912e+04, 3.8528e+04,        inf,\n",
      "        2.2640e+04, 2.8240e+04, 1.8480e+04, 5.1072e+04, 1.1240e+04, 9.0160e+03,\n",
      "        4.1440e+03, 4.0320e+04, 4.6656e+04, 1.5344e+04, 3.7088e+04, 1.8624e+04,\n",
      "        2.7392e+04, 1.6112e+04, 3.8820e+03, 3.2540e+03, 3.5584e+04, 1.1704e+04,\n",
      "        1.0464e+04, 2.1952e+04, 1.6624e+04,        inf, 5.7200e+03, 4.8544e+04,\n",
      "        2.0290e+03, 1.0370e+03], device='cuda:0', dtype=torch.float16), tensor([7.3160e+03, 5.4912e+04, 6.7000e+02, 9.1760e+03, 3.8650e+02, 6.1400e+03,\n",
      "        8.7760e+03, 2.5728e+04, 1.2272e+04, 5.5560e+03, 7.5360e+03, 7.6120e+03,\n",
      "        1.7952e+04, 7.9960e+03, 1.1800e+04, 2.5660e+03, 1.7584e+04, 3.2096e+04,\n",
      "        6.0680e+03, 2.1856e+04, 8.3680e+03, 2.1080e+03, 2.6560e+03, 5.5438e+01,\n",
      "        6.9800e+03, 7.4640e+03, 1.8144e+04, 1.5060e+03, 1.5760e+04, 3.9200e+04,\n",
      "        8.3450e+02, 1.2630e+03, 7.5720e+03, 1.5460e+03, 9.9760e+03, 1.9232e+04,\n",
      "        4.0075e+02, 5.4480e+03, 1.7392e+04, 3.7312e+04, 3.2300e+03, 1.4168e+04,\n",
      "        1.4744e+04, 2.1024e+04, 1.8438e+02, 1.2640e+03, 2.0080e+04, 1.2010e+03,\n",
      "        2.4992e+04, 4.6280e+03, 9.2240e+03, 5.9000e+03, 6.4500e+02, 6.6900e+02,\n",
      "        1.7770e+03, 1.1688e+04, 7.0300e+02, 9.8800e+02, 3.2540e+03, 3.2752e+04,\n",
      "        2.3264e+04, 6.4100e+02, 5.9640e+03, 4.6760e+03, 2.0800e+02, 8.3760e+03,\n",
      "        4.0820e+03, 4.5880e+03, 7.7440e+03, 8.8560e+03, 4.4880e+03, 4.4280e+03,\n",
      "        3.9480e+03, 8.5120e+03, 4.2848e+04, 1.5904e+04, 1.8180e+03, 7.4360e+03,\n",
      "        2.8420e+03, 4.4840e+03, 1.0224e+04, 7.5720e+03, 4.8280e+03, 7.8240e+03,\n",
      "        2.9078e+01, 1.3256e+04, 1.3872e+04, 1.3840e+04, 1.8496e+04, 2.5648e+04,\n",
      "        4.9400e+03, 3.3088e+04, 9.2400e+02, 3.3240e+03, 7.4280e+03, 9.0960e+03,\n",
      "        2.3184e+04, 4.1920e+03, 3.4880e+03, 2.2064e+04, 3.3840e+03, 6.1720e+03,\n",
      "        2.9000e+02, 2.4400e+03, 3.4320e+03, 1.3568e+04, 8.0450e+02, 1.2670e+03,\n",
      "        5.7568e+04, 2.6800e+03, 3.8200e+03, 5.0840e+03, 1.1760e+03, 3.9000e+03,\n",
      "        1.2424e+04, 4.4960e+03, 2.8180e+03, 6.5560e+03, 3.2550e+02, 1.4680e+03,\n",
      "        1.1816e+04, 2.7180e+03, 1.1950e+03, 4.0544e+04, 3.1472e+04, 7.2320e+03,\n",
      "        8.2080e+03, 1.2640e+04, 8.0600e+03, 1.5140e+03, 2.7980e+03, 1.6336e+04,\n",
      "        5.6450e+02, 1.0810e+03, 4.3520e+03, 7.6960e+03, 2.9424e+04, 3.0400e+02,\n",
      "        5.0280e+03, 2.2520e+03, 4.5600e+02, 7.5360e+03, 3.1936e+04, 5.0120e+03,\n",
      "        7.0680e+03, 2.6320e+03, 4.6720e+03, 8.1625e+01, 4.2840e+03, 2.9860e+03,\n",
      "        4.8080e+03, 1.2520e+04, 3.4752e+04, 2.8940e+03, 2.0500e+03, 2.4560e+03,\n",
      "        3.0544e+04, 1.0584e+04, 4.3040e+03, 1.2944e+04, 1.0630e+03, 2.0000e+04,\n",
      "        4.6944e+04, 6.3320e+03, 1.2880e+03, 7.0920e+03, 1.1232e+04, 1.5200e+03,\n",
      "        3.0520e+03, 8.3360e+03, 4.4224e+04, 2.3312e+04, 2.7525e+02, 9.0375e+01,\n",
      "        9.3040e+03, 4.0160e+04, 9.4320e+03, 1.1128e+04, 2.3216e+04, 2.1300e+03,\n",
      "        2.5632e+04, 1.8260e+03, 3.8464e+04, 5.0680e+03, 2.0704e+04, 2.0880e+04,\n",
      "        1.1560e+04, 1.5632e+04, 1.0740e+03, 1.4976e+04, 1.2290e+03, 1.6890e+03,\n",
      "        2.8640e+04, 2.9792e+04, 6.3000e+03, 1.3880e+04, 2.6944e+04, 5.4800e+02,\n",
      "        4.8600e+03, 2.8260e+03, 3.9488e+04, 2.2840e+03, 1.5592e+04, 1.8048e+04,\n",
      "        6.9640e+03, 2.5800e+02, 5.2880e+03, 5.8400e+03, 1.7424e+04, 4.7960e+03,\n",
      "        1.0744e+04, 4.0040e+03, 1.0400e+04, 1.1360e+04, 5.7640e+03, 1.3736e+04,\n",
      "        3.9840e+04, 1.1656e+04, 3.4880e+04, 3.0700e+03, 1.0616e+04, 5.4560e+03,\n",
      "        4.8160e+03, 1.4090e+03, 1.4128e+04, 9.2080e+03, 2.0368e+04, 2.8400e+04,\n",
      "        4.3936e+04, 2.1800e+03, 4.3700e+02, 1.0944e+04, 1.0768e+04, 5.4040e+03,\n",
      "        1.2592e+04, 7.4900e+02, 3.0700e+03, 1.2312e+04, 2.9580e+03, 7.5280e+03,\n",
      "        1.3870e+03, 3.5800e+03, 4.0240e+03, 4.4224e+04, 2.8352e+04, 1.9024e+04,\n",
      "        2.4416e+04, 5.3152e+04, 1.2040e+03, 1.6176e+04, 1.8912e+04, 7.9320e+03,\n",
      "        7.3840e+03, 3.3984e+04, 9.6800e+03, 4.0000e+03, 1.4400e+03, 1.5780e+03,\n",
      "        1.2824e+04, 7.0720e+03, 4.7320e+03, 1.0472e+04, 4.4960e+03, 1.1640e+04,\n",
      "        1.3504e+04, 2.6460e+03, 6.3680e+04, 1.0155e+03, 1.7552e+04, 1.0152e+04,\n",
      "        1.8816e+04, 3.2380e+03, 2.5968e+04, 2.5088e+04, 7.3100e+02, 5.7056e+04,\n",
      "        4.7040e+03, 7.4440e+03, 5.4040e+03, 4.2912e+04, 2.6340e+03, 6.3040e+03,\n",
      "        1.3768e+04, 9.6320e+03, 9.4560e+03, 3.4380e+03, 4.8240e+03, 7.7350e+02,\n",
      "        1.1590e+03, 7.3960e+03, 4.0300e+03, 2.6016e+04, 1.5260e+03, 2.3800e+03,\n",
      "        1.6896e+04, 8.8640e+03, 1.0744e+04, 2.4880e+04, 3.6448e+04, 5.1440e+03,\n",
      "        1.3144e+04, 1.2910e+03, 3.0860e+03, 3.7540e+03, 1.0656e+04, 2.9056e+04,\n",
      "        2.2600e+03, 5.9760e+03, 1.3472e+04, 5.6192e+04, 4.1120e+04, 4.2360e+03,\n",
      "        3.6700e+03, 4.4640e+03, 1.8336e+04, 3.7800e+02, 3.8820e+03, 1.4304e+04,\n",
      "        6.8360e+03, 1.4600e+04, 9.7760e+03, 7.1300e+02, 4.3880e+03, 1.5840e+03,\n",
      "        1.4120e+04, 2.0960e+03, 1.8150e+03, 2.4864e+04, 2.3472e+04, 6.9880e+03,\n",
      "        4.9100e+02, 7.9800e+03, 3.6288e+04, 5.9040e+03, 2.8960e+04, 4.6848e+04,\n",
      "        8.2400e+03, 8.9040e+03, 2.2992e+04, 8.3920e+03, 2.3248e+04, 4.5536e+04,\n",
      "        3.7088e+04, 2.8208e+04, 2.8380e+03, 1.0824e+04, 5.3560e+03, 9.5920e+03,\n",
      "        1.1310e+03, 8.3900e+02, 1.0792e+04, 9.9120e+03, 3.3568e+04, 1.4944e+04,\n",
      "        9.2640e+03, 1.0032e+04, 2.8680e+03, 5.9280e+03, 1.6100e+03, 1.7380e+03,\n",
      "        1.2376e+04, 3.5900e+03, 6.2240e+04, 8.9750e+02, 1.1648e+04, 1.3632e+04,\n",
      "        1.0840e+03, 3.0280e+03, 6.1600e+03, 2.6980e+03, 2.1860e+03, 3.6960e+03,\n",
      "        1.5528e+04, 3.0976e+04, 1.5216e+04, 4.0120e+03, 2.5380e+03, 1.8928e+04,\n",
      "        6.2680e+03, 3.3800e+03, 2.0528e+04, 3.0400e+03, 1.3072e+04, 2.4900e+02,\n",
      "        3.8940e+03, 2.8208e+04, 4.3040e+03, 2.0976e+04, 3.3408e+04, 4.9080e+03,\n",
      "        2.6304e+04, 2.2312e+02, 1.8320e+03, 6.1280e+04, 4.0160e+03, 2.5984e+04,\n",
      "        4.2976e+04, 3.9200e+03,        inf, 3.3056e+04, 2.1632e+04, 2.6180e+03,\n",
      "        5.0400e+03, 4.9000e+03, 4.1856e+04, 1.4592e+04, 1.8112e+04, 3.9104e+04,\n",
      "        4.2176e+04, 8.9760e+03, 1.3640e+04, 2.6400e+04, 4.9040e+03, 5.6720e+03,\n",
      "        2.4180e+03, 3.5840e+03, 8.0200e+03,        inf, 3.6512e+04, 6.7950e+02,\n",
      "        1.5008e+04, 2.6064e+04,        inf, 5.1080e+03, 2.3840e+04, 1.3088e+04,\n",
      "        2.8192e+04, 3.0592e+04, 1.1456e+04, 3.3440e+04, 2.0960e+04, 6.2400e+03,\n",
      "        5.0784e+04, 1.1512e+04, 1.6528e+04, 1.6960e+04, 5.1360e+04, 1.9360e+04,\n",
      "        8.2050e+02, 3.3900e+03, 9.4560e+03, 2.4976e+04, 1.2576e+04, 5.8520e+03,\n",
      "        2.9040e+04, 2.4260e+03, 2.3440e+04, 5.5280e+03, 1.1344e+04, 4.1425e+02,\n",
      "        1.6920e+03, 1.8384e+04, 4.7675e+02, 1.3352e+04, 5.1080e+03, 2.1392e+04,\n",
      "        5.4040e+03, 1.9920e+03, 2.8900e+03, 1.4808e+04, 6.1536e+04, 6.6160e+03,\n",
      "        1.1936e+04, 2.1568e+04, 3.2400e+03, 4.0240e+03, 3.8800e+03, 5.4760e+03,\n",
      "        6.2304e+04, 2.4480e+03, 2.1440e+04, 9.3520e+03, 4.6016e+04, 2.1968e+04,\n",
      "        1.7220e+03, 3.3520e+03, 7.0960e+03, 1.2976e+04, 9.2000e+03, 3.3952e+04,\n",
      "        2.6704e+04, 9.2560e+03, 4.0700e+03, 7.8040e+03, 1.0824e+04,        inf,\n",
      "        1.9520e+04, 3.1472e+04, 2.2640e+04, 3.6704e+04, 9.2500e+02, 8.7680e+03,\n",
      "        4.8400e+03, 3.2640e+04, 3.9744e+04, 1.7408e+04, 1.3544e+04, 2.7900e+03,\n",
      "        2.5500e+03, 1.7504e+04, 4.1280e+03, 4.2160e+03, 3.3375e+02, 2.7800e+03,\n",
      "        6.8200e+03, 2.4624e+04, 2.2752e+04, 1.8950e+03, 4.0180e+03, 1.0448e+04,\n",
      "        2.0540e+03, 8.0480e+03], device='cuda:0', dtype=torch.float16), tensor([1.5088e+04,        inf, 1.3808e+04, 7.1480e+03, 1.3080e+04, 2.3808e+04,\n",
      "        5.1960e+03, 2.4752e+04, 4.0064e+04, 2.6304e+04, 4.4064e+04, 3.8060e+03,\n",
      "        3.7344e+04, 3.2112e+04, 2.9504e+04, 1.4144e+04, 2.4192e+04, 3.1792e+04,\n",
      "        1.1752e+04, 1.3776e+04, 1.7808e+04, 1.2680e+03, 5.4400e+04, 3.0464e+04,\n",
      "        3.2896e+04, 1.6480e+04, 3.4016e+04, 5.7480e+03, 2.2384e+04, 2.7024e+04,\n",
      "        1.3730e+03, 1.0600e+04, 6.7600e+03, 5.8920e+03, 8.3200e+03, 6.1952e+04,\n",
      "        2.7792e+04, 9.9360e+03, 3.0736e+04,        inf, 7.1080e+03, 4.9984e+04,\n",
      "        2.3184e+04, 4.5408e+04, 1.9472e+04, 5.3040e+03, 2.7904e+04, 2.3920e+03,\n",
      "        4.7328e+04, 2.2360e+03, 1.3560e+04, 1.0128e+04, 5.1400e+03, 8.1200e+03,\n",
      "        3.3440e+04, 2.1968e+04, 5.4280e+03, 7.5400e+03, 6.6640e+03, 3.9296e+04,\n",
      "        4.7616e+04, 1.4776e+04, 4.1240e+03, 7.7120e+03, 2.3344e+04, 3.0432e+04,\n",
      "        1.5424e+04, 5.5600e+03, 1.1176e+04, 1.0944e+04, 4.6440e+03, 4.4992e+04,\n",
      "        2.7552e+04, 3.5680e+03, 4.5952e+04, 1.0296e+04, 4.4864e+04, 1.8864e+04,\n",
      "        1.3600e+04, 2.1680e+04, 2.6992e+04, 9.6400e+03, 1.1440e+04, 2.0192e+04,\n",
      "        2.8560e+03, 4.4672e+04, 1.8432e+04, 2.7792e+04, 1.7952e+04, 3.7760e+04,\n",
      "        2.7120e+04, 4.5248e+04, 5.0144e+04, 3.9136e+04, 2.2512e+04, 1.5280e+04,\n",
      "        2.9504e+04, 1.6312e+04, 5.1219e+01, 4.8160e+04, 1.3776e+04, 1.8112e+04,\n",
      "        1.5424e+04, 3.5360e+03, 1.6832e+04, 1.6544e+04, 1.0768e+04, 1.4550e+03,\n",
      "        5.6896e+04, 2.2304e+04, 6.0128e+04, 6.3680e+03, 1.9984e+04, 4.1640e+03,\n",
      "        2.6304e+04, 6.1312e+04, 6.5080e+03, 2.8020e+03, 1.0235e+03, 1.4552e+04,\n",
      "        2.9728e+04, 1.0872e+04, 1.5320e+04, 3.9008e+04, 2.7632e+04, 3.9904e+04,\n",
      "        8.1760e+03, 4.6496e+04, 1.5536e+04, 3.2031e+01, 8.1950e+02, 4.1504e+04,\n",
      "        1.0310e+03, 3.5776e+04, 4.1280e+03, 1.7488e+04, 5.1808e+04, 2.5648e+04,\n",
      "        1.2176e+04, 4.1120e+03, 2.6400e+03, 1.8128e+04, 3.4144e+04, 2.4416e+04,\n",
      "        5.3376e+04, 1.5992e+04, 2.6000e+04, 3.0848e+04, 3.8016e+04, 6.3280e+03,\n",
      "        2.3040e+04, 1.1800e+04, 5.3280e+04, 1.6610e+03, 2.8528e+04, 2.6800e+04,\n",
      "        2.4272e+04, 1.0160e+04, 3.0720e+04, 5.0944e+04, 9.0650e+02, 5.3408e+04,\n",
      "        3.4048e+04, 1.2024e+04, 1.9040e+04, 3.0432e+04, 1.4856e+04, 2.8896e+04,\n",
      "               inf, 5.0080e+04,        inf, 2.1904e+04, 4.9680e+03, 5.2360e+03,\n",
      "        1.5184e+04, 3.2144e+04, 2.1712e+04, 2.1568e+04, 2.8608e+04, 5.9840e+03,\n",
      "        3.7600e+04, 8.6560e+03, 5.4560e+04, 1.0968e+04, 2.1888e+04, 6.1728e+04,\n",
      "        4.3264e+04, 5.0016e+04, 9.2562e+01, 2.2800e+04, 2.5520e+03, 1.5840e+04,\n",
      "        5.4240e+04, 4.8512e+04, 1.0920e+04, 6.2368e+04,        inf, 5.3920e+03,\n",
      "        6.4120e+03, 1.5272e+04, 3.0000e+04, 2.3104e+04, 4.4544e+04, 4.0928e+04,\n",
      "        3.8944e+04, 8.0800e+02, 1.7504e+04, 4.7320e+03, 1.1832e+04, 1.7344e+04,\n",
      "        1.8944e+04, 1.6620e+03, 2.3248e+04, 3.7440e+04, 9.3040e+03, 1.7344e+04,\n",
      "        3.6416e+04, 2.1472e+04, 5.2736e+04, 1.5936e+04, 2.8224e+04, 1.1336e+04,\n",
      "        3.7696e+04, 2.5820e+03, 1.9568e+04, 1.6544e+04, 2.0432e+04, 5.9008e+04,\n",
      "        5.1360e+04, 8.8480e+03, 4.5360e+03, 1.6736e+04, 2.3600e+04, 5.0912e+04,\n",
      "        1.9168e+04, 1.9630e+03, 4.5472e+04, 2.7664e+04, 3.6608e+04, 1.3688e+04,\n",
      "        3.3376e+04, 5.8960e+03, 1.0488e+04,        inf, 4.0960e+04, 3.0224e+04,\n",
      "        2.9152e+04, 6.3008e+04, 2.5264e+04, 1.2936e+04, 5.4912e+04, 1.1152e+04,\n",
      "        3.3440e+04, 4.2304e+04, 1.5728e+04, 6.4720e+03, 2.2240e+04, 1.7712e+04,\n",
      "        3.0576e+04, 2.7800e+03, 5.5168e+04, 1.3096e+04, 3.2480e+04, 8.0840e+03,\n",
      "        3.5616e+04, 3.4960e+03,        inf, 1.0360e+04, 5.0400e+04, 4.5888e+04,\n",
      "        2.2448e+04, 7.4600e+03, 2.5296e+04, 2.9632e+04, 6.6480e+03, 5.6032e+04,\n",
      "        9.6080e+03, 2.8128e+04, 1.2656e+04, 4.9760e+04, 8.0560e+03, 6.7520e+03,\n",
      "        1.9312e+04, 3.1376e+04, 2.5600e+04, 3.5200e+03, 7.0920e+03, 3.6680e+03,\n",
      "        1.1824e+04, 1.1064e+04, 4.8032e+04, 1.4360e+04, 1.6928e+04, 1.2368e+04,\n",
      "        2.8208e+04, 5.3888e+04, 1.1608e+04, 2.2688e+04, 3.8464e+04, 6.8840e+03,\n",
      "        1.3096e+04, 5.5168e+04, 4.0900e+03, 8.7200e+03, 2.9552e+04, 4.5024e+04,\n",
      "        5.1040e+03, 1.0208e+04, 3.3632e+04, 6.0672e+04,        inf, 1.8272e+04,\n",
      "        1.1496e+04, 1.4390e+03, 2.3088e+04, 4.6760e+03, 6.2900e+02, 2.6992e+04,\n",
      "        1.0448e+04, 2.1920e+04, 2.3216e+04, 3.5616e+04, 1.3624e+04, 1.5824e+04,\n",
      "        2.5424e+04, 2.5260e+03, 8.4320e+03, 1.8928e+04, 3.0992e+04, 1.3456e+04,\n",
      "        6.2080e+03, 7.6120e+03, 3.6192e+04, 2.3232e+04, 5.4656e+04, 2.2496e+04,\n",
      "        1.2781e-01, 1.9856e+04, 2.5216e+04, 4.4256e+04, 1.3168e+04, 6.4864e+04,\n",
      "        2.6864e+04, 3.2384e+04, 3.1056e+04, 1.7808e+04, 2.0980e+03, 4.4160e+04,\n",
      "        1.4312e+04, 3.4368e+04, 4.0864e+04, 3.5040e+04, 4.3680e+04, 3.5968e+04,\n",
      "        4.6880e+04, 2.6512e+04, 2.5700e+03, 3.2352e+04, 1.1568e+04, 4.8288e+04,\n",
      "        1.5880e+04, 9.2000e+03,        inf, 1.2750e+03, 1.0816e+04, 1.9408e+04,\n",
      "        2.8352e+04, 2.6416e+04, 3.4080e+04, 3.9700e+03, 2.0944e+04, 5.0480e+03,\n",
      "        1.2960e+04, 1.6912e+04, 2.8000e+04, 1.2150e+03, 7.9600e+03, 5.4496e+04,\n",
      "        9.0160e+03, 2.4840e+03, 2.5184e+04, 6.9720e+03, 1.4304e+04, 2.6420e+03,\n",
      "        4.5025e+02, 3.5456e+04, 9.2080e+03, 4.7072e+04, 2.9616e+04, 5.4520e+03,\n",
      "        2.7520e+04, 7.9562e+01, 4.4550e+02,        inf, 6.7160e+03, 2.2656e+04,\n",
      "        4.4608e+04, 5.2928e+04,        inf, 4.1760e+04, 4.6816e+04, 4.7625e+02,\n",
      "        7.3480e+03, 1.4296e+04, 3.6800e+04, 2.4016e+04, 3.1440e+04, 4.0352e+04,\n",
      "        3.9104e+04, 2.2480e+04, 1.7568e+04, 3.3920e+04, 8.0600e+03, 1.5048e+04,\n",
      "        6.1040e+03, 1.1840e+04, 7.8920e+03,        inf, 5.3024e+04, 9.7440e+03,\n",
      "        1.2856e+04, 3.4112e+04,        inf, 1.0392e+04, 1.9168e+04, 1.4920e+04,\n",
      "        4.1952e+04,        inf, 1.6896e+04, 4.1632e+04, 3.4432e+04, 9.1250e+01,\n",
      "        4.5248e+04, 3.7024e+04, 1.6640e+04, 3.4880e+04, 5.5456e+04, 2.5008e+04,\n",
      "        2.8400e+04, 5.2560e+03, 1.8992e+04, 2.4384e+04, 2.0000e+04, 4.7840e+03,\n",
      "        3.6960e+04, 3.3150e+02, 2.5344e+04, 2.7024e+04, 3.9700e+03, 2.5320e+03,\n",
      "        3.1320e+03, 3.4400e+04, 7.6450e+02, 2.4480e+04, 4.4800e+03, 2.2848e+04,\n",
      "        5.8600e+03, 3.2620e+03, 3.9968e+04, 1.0080e+04,        inf, 4.0160e+04,\n",
      "        3.8144e+04, 1.7280e+04, 3.4528e+04, 4.2240e+03, 2.5920e+03, 9.6800e+03,\n",
      "        5.5712e+04, 4.0448e+04, 1.8080e+04, 8.7520e+03,        inf, 3.2864e+04,\n",
      "        9.1280e+03, 6.2200e+03, 1.4264e+04, 1.3424e+04, 2.4736e+04, 5.6128e+04,\n",
      "               inf, 1.7792e+04, 1.5792e+04, 5.9136e+04, 2.8912e+04,        inf,\n",
      "        2.2480e+04, 3.5008e+04, 2.2800e+04,        inf, 8.7760e+03, 7.3760e+03,\n",
      "        8.2080e+03, 4.9024e+04, 6.2592e+04, 2.5152e+04, 1.9568e+04, 2.9696e+04,\n",
      "        6.0600e+03, 1.3656e+04, 8.7120e+03, 2.9560e+03, 3.2352e+04, 1.8850e+03,\n",
      "        1.2360e+04, 2.0144e+04, 2.2576e+04,        inf, 6.1720e+03, 4.6784e+04,\n",
      "        6.5950e+02, 1.3920e+04], device='cuda:0', dtype=torch.float16), tensor([2.6260e+03, 4.1280e+04, 1.7690e+03, 2.0320e+03, 1.5690e+03, 4.3320e+03,\n",
      "        7.8040e+03, 2.0352e+04, 3.5424e+04, 1.2952e+04, 1.3096e+04, 2.9860e+03,\n",
      "        2.9280e+04, 3.9540e+03, 3.5980e+03, 7.8700e+02, 1.8576e+04, 2.2016e+04,\n",
      "        6.7000e+03, 2.2240e+04, 7.5080e+03, 2.0230e+03, 7.4360e+03, 2.4220e+03,\n",
      "        1.4020e+03, 1.6304e+04, 1.5504e+04, 1.8740e+03, 3.3984e+04, 2.3344e+04,\n",
      "        4.1800e+03, 2.2880e+03, 1.3336e+04, 3.0720e+03, 5.9040e+03, 2.5760e+04,\n",
      "        1.1540e+03, 1.3112e+04, 9.2640e+03, 3.7088e+04, 5.0040e+03, 2.0752e+04,\n",
      "        7.5720e+03, 4.4672e+04, 5.4080e+03, 1.6640e+03, 1.4584e+04, 5.2560e+03,\n",
      "        2.8784e+04, 1.8310e+03, 2.1488e+04, 1.1944e+04, 9.2160e+03, 8.3100e+02,\n",
      "        1.2130e+03, 5.2800e+03, 1.4300e+03, 3.9550e+02, 1.8800e+03, 2.4880e+04,\n",
      "        1.6704e+04, 5.0000e+03, 3.6920e+03, 5.4000e+03, 1.0544e+04, 5.7280e+03,\n",
      "        2.3060e+03, 9.9200e+02, 1.8260e+03, 1.1776e+04, 3.0640e+03, 4.9320e+03,\n",
      "        7.6640e+03, 7.1240e+03, 2.8688e+04, 7.5480e+03, 2.9000e+03, 4.7200e+03,\n",
      "        3.9900e+02, 2.9050e+02, 4.6680e+03, 5.0040e+03, 7.2800e+03, 1.1400e+03,\n",
      "        5.5688e+01, 3.7460e+03, 9.1360e+03, 1.9744e+04, 2.1568e+04, 2.2384e+04,\n",
      "        6.8160e+03, 3.7888e+04, 2.9900e+02, 9.2850e+02, 8.5120e+03, 8.3600e+03,\n",
      "        2.1104e+04, 2.1560e+03, 5.1800e+03, 9.8480e+03, 8.9200e+03, 1.0128e+04,\n",
      "        6.1040e+03, 3.7340e+03, 2.6580e+03, 6.6400e+03, 1.8930e+03, 3.1380e+03,\n",
      "        6.0608e+04, 4.8225e+02, 3.2260e+03, 8.6000e+03, 3.1420e+03, 2.6980e+03,\n",
      "        9.8560e+03, 7.0040e+03, 1.5540e+03, 1.8310e+03, 9.7700e+02, 8.5900e+02,\n",
      "        1.0632e+04, 5.8360e+03, 4.2440e+03, 3.2960e+04, 3.2864e+04, 5.2000e+03,\n",
      "        8.2880e+03, 1.8704e+04, 9.6880e+03, 5.1840e+03, 3.9800e+02, 3.4480e+03,\n",
      "        4.0380e+03, 2.7940e+03, 2.8120e+03, 4.1250e+00, 2.5136e+04, 2.4100e+03,\n",
      "        5.3120e+03, 1.7230e+03, 3.6620e+03, 6.4960e+03, 2.6832e+04, 2.1088e+04,\n",
      "        3.6220e+03, 2.1240e+03, 3.4780e+03, 4.4280e+03, 6.0680e+03, 1.7560e+03,\n",
      "        7.8400e+03, 2.3662e+02, 1.9968e+04, 3.4280e+03, 4.6280e+03, 3.7740e+03,\n",
      "        1.5600e+04, 3.9300e+02, 2.9600e+03, 7.7160e+03, 1.0080e+03, 1.1384e+04,\n",
      "        3.5040e+04, 7.7480e+03, 3.7300e+02, 7.3950e+02, 6.1520e+03, 7.9650e+02,\n",
      "        1.7104e+04, 2.7800e+02, 4.0288e+04, 2.6896e+04, 5.5080e+03, 1.1912e+02,\n",
      "        4.2320e+03, 5.8400e+04, 2.3152e+04, 1.5900e+02, 2.7296e+04, 2.1840e+03,\n",
      "        3.0192e+04, 1.2620e+03, 6.2944e+04, 5.0960e+03, 7.2560e+03, 1.4456e+04,\n",
      "        4.4040e+03, 1.2768e+04, 5.8320e+03, 1.6336e+04, 1.2850e+02, 3.4340e+03,\n",
      "        2.1680e+04, 3.3312e+04, 1.1104e+04, 1.4472e+04, 4.7200e+04, 2.9580e+03,\n",
      "        8.6240e+03, 1.5590e+03, 2.1488e+04, 4.9360e+03, 2.5760e+04, 1.1744e+04,\n",
      "        5.6760e+03, 7.3550e+02, 5.9200e+03, 2.4360e+03, 2.6416e+04, 1.7910e+03,\n",
      "        2.6550e+02, 7.9500e+02, 8.5280e+03, 1.0470e+03, 3.4740e+03, 1.2936e+04,\n",
      "        1.9376e+04, 7.2560e+03, 3.5840e+04, 2.6640e+03, 7.0640e+03, 7.1400e+03,\n",
      "        7.1920e+03, 1.1160e+04, 2.5696e+04, 7.5760e+03, 1.8496e+04, 2.6736e+04,\n",
      "        2.6112e+04, 2.8920e+03, 5.8000e+03, 6.0520e+03, 9.6480e+03, 1.4760e+04,\n",
      "        2.5460e+03, 5.5400e+02, 5.8760e+03, 2.0112e+04, 1.8528e+04, 4.2200e+03,\n",
      "        1.0968e+04, 1.1160e+03, 2.5388e+02, 2.3840e+04, 3.2048e+04, 1.9552e+04,\n",
      "        1.8688e+04, 4.3200e+04, 1.0650e+02, 1.8960e+04, 2.9920e+04, 4.9320e+03,\n",
      "        1.1304e+04, 3.3568e+04, 4.6280e+03, 3.9275e+02, 3.1180e+03, 3.3960e+03,\n",
      "        9.4400e+03, 5.0480e+03, 1.3704e+04, 4.3080e+03, 9.4240e+03, 9.9280e+03,\n",
      "        2.2672e+04, 2.9380e+03, 4.9792e+04, 1.1310e+03, 3.9456e+04, 8.4880e+03,\n",
      "        1.6240e+04, 2.3400e+03, 3.8400e+04, 8.3840e+03, 1.3090e+03, 4.4192e+04,\n",
      "        2.0820e+03, 6.7160e+03, 5.9240e+03, 3.1920e+04, 6.8900e+02, 3.8960e+03,\n",
      "        3.6440e+03, 2.6040e+03, 6.5700e+02, 7.2500e+02, 1.0552e+04, 2.0720e+03,\n",
      "        2.4775e+02, 2.5776e+04, 5.4000e+02, 2.3888e+04, 7.1480e+03, 7.7050e+02,\n",
      "        9.6560e+03, 1.1840e+04, 3.2320e+03, 3.1712e+04, 1.5864e+04, 6.2560e+03,\n",
      "        6.3800e+03, 4.5880e+03, 1.4830e+03, 6.2200e+03, 3.3660e+03, 3.0880e+04,\n",
      "        4.5880e+03, 6.0480e+03, 4.0320e+03, 5.8976e+04, 3.5776e+04, 3.3920e+03,\n",
      "        6.7840e+03, 2.6540e+03, 3.2224e+04, 1.3936e+04, 3.7150e+02, 4.7680e+03,\n",
      "        4.1600e+03, 1.6432e+04, 6.6640e+03, 2.4012e+02, 2.9880e+03, 5.3240e+03,\n",
      "        2.5200e+04, 7.7840e+03, 4.5760e+03, 1.3688e+04, 1.8432e+04, 7.9360e+03,\n",
      "        1.9660e+03, 5.6960e+03, 2.2544e+04, 1.0680e+04, 2.2864e+04, 2.7968e+04,\n",
      "        8.9520e+03, 6.2000e+03, 4.9720e+03, 4.0120e+03, 1.6032e+04,        inf,\n",
      "        3.4560e+04, 2.1888e+04, 2.2400e+04, 1.9860e+03, 1.4075e+02, 1.5528e+04,\n",
      "        2.3380e+03, 5.9240e+03, 1.6448e+04, 8.3280e+03, 4.3840e+04, 2.5792e+04,\n",
      "        2.7300e+03, 1.0720e+04, 1.6730e+03, 1.1040e+04, 7.1550e+02, 3.8560e+03,\n",
      "        2.3664e+04, 5.1880e+03, 5.7728e+04, 2.5240e+03, 8.7550e+02, 1.3760e+04,\n",
      "        4.4760e+03, 6.9320e+03, 1.2584e+04, 1.0370e+03, 1.1816e+04, 5.2320e+03,\n",
      "        1.1792e+04, 2.3312e+04, 1.3832e+04, 3.5040e+03, 9.2200e+02, 8.2320e+03,\n",
      "        7.4920e+03, 2.4520e+03, 1.6672e+04, 4.8560e+03, 1.5672e+04, 3.5600e+02,\n",
      "        1.8360e+03, 2.6784e+04, 2.0025e+02, 2.5296e+04, 3.9680e+04, 9.6320e+03,\n",
      "        3.2752e+04, 2.7850e+02, 3.1600e+02, 4.7360e+04, 1.2060e+03, 2.2336e+04,\n",
      "        3.4592e+04, 4.4560e+03, 6.1408e+04, 3.3280e+04, 2.4432e+04, 1.3700e+03,\n",
      "        1.1480e+04, 3.8400e+03, 3.3312e+04, 2.7950e+02, 1.5768e+04, 4.5056e+04,\n",
      "        3.4240e+04, 1.2208e+04, 9.4240e+03, 2.5888e+04, 3.8900e+03, 1.4576e+04,\n",
      "        4.4300e+02, 5.5200e+03, 2.0304e+04, 6.5056e+04, 4.1408e+04, 5.3350e+02,\n",
      "        1.5952e+04, 1.3384e+04,        inf, 5.2880e+03, 4.4800e+04, 2.3920e+04,\n",
      "        2.7248e+04, 5.3568e+04, 9.9200e+03, 3.3440e+04, 1.6624e+04, 8.7360e+03,\n",
      "        4.3392e+04, 1.5848e+04, 5.8600e+03, 8.4720e+03, 5.8016e+04, 1.6328e+04,\n",
      "        3.0040e+03, 3.6160e+03, 5.9280e+03, 2.3488e+04, 9.7200e+03, 8.2450e+02,\n",
      "        3.8560e+04, 8.5200e+03, 1.5088e+04, 4.4800e+02, 4.4120e+03, 8.8350e+02,\n",
      "        1.7820e+03, 2.1328e+04, 1.3580e+03, 4.2600e+03, 3.2060e+03, 3.4624e+04,\n",
      "        3.6080e+03, 5.9560e+03, 9.9520e+03, 1.8880e+04, 4.1888e+04, 3.8260e+03,\n",
      "        2.2112e+04, 9.7600e+03, 4.7840e+03, 5.3240e+03, 1.5728e+04, 1.0000e+04,\n",
      "        4.5088e+04, 4.6080e+03, 1.9056e+04, 4.6400e+03, 2.4144e+04, 3.2416e+04,\n",
      "        5.2000e+03, 4.7960e+03, 1.9710e+03, 1.7850e+02, 8.5120e+03, 6.1184e+04,\n",
      "        2.6304e+04, 2.3840e+03, 5.3400e+02, 1.8960e+04, 9.1200e+03,        inf,\n",
      "        1.7328e+04, 2.9632e+04, 1.4232e+04, 2.3024e+04, 2.3960e+03, 1.2648e+04,\n",
      "        7.4650e+02, 2.3504e+04, 2.6624e+04, 6.2320e+03, 1.5800e+04, 6.6920e+03,\n",
      "        1.3664e+04, 1.9376e+04, 4.8600e+02, 6.2040e+03, 4.9960e+03, 1.6736e+04,\n",
      "        7.6320e+03, 3.2512e+04, 1.7264e+04, 7.6400e+03, 4.5840e+03, 1.7888e+04,\n",
      "        8.4050e+02, 3.1525e+02], device='cuda:0', dtype=torch.float16), tensor([7.6240e+03,        inf, 1.3752e+04, 1.4136e+04, 2.5600e+03, 2.1328e+04,\n",
      "        2.5280e+03, 1.2592e+04, 4.6208e+04, 4.2016e+04, 3.3824e+04, 1.8832e+04,\n",
      "        4.4352e+04, 2.9648e+04, 8.8640e+03, 3.8540e+03, 2.6608e+04, 1.8496e+04,\n",
      "        1.0456e+04, 2.4064e+04, 3.2400e+04, 8.7207e-01, 5.2160e+04, 2.9984e+04,\n",
      "        4.2368e+04, 2.4352e+04, 3.0736e+04, 6.3520e+03, 3.1344e+04, 2.0544e+04,\n",
      "        4.6480e+03, 1.3072e+04, 1.8784e+04, 1.0920e+04, 8.3200e+03, 5.3568e+04,\n",
      "        2.9312e+04, 2.0464e+04, 4.9152e+04,        inf, 1.2424e+04, 4.7168e+04,\n",
      "        1.5928e+04, 4.9472e+04, 1.6560e+04, 3.0440e+03, 3.0656e+04, 8.6960e+03,\n",
      "        5.8208e+04, 1.0424e+04, 2.1648e+04, 2.6288e+04, 4.2240e+03, 1.0944e+04,\n",
      "        3.3856e+04, 1.3520e+04, 3.0060e+03, 4.4160e+03, 4.6600e+03, 4.2528e+04,\n",
      "        2.8944e+04, 1.2952e+04, 8.5920e+03, 1.5400e+04, 1.2448e+04, 2.5840e+04,\n",
      "        1.1968e+04, 9.4320e+03, 7.6600e+02, 2.0672e+04, 1.5392e+04, 4.5280e+04,\n",
      "        2.6512e+04, 8.4640e+03, 3.4688e+04, 2.1360e+03, 2.5680e+04, 2.4112e+04,\n",
      "        8.2720e+03, 8.9840e+03, 2.2176e+04, 3.4360e+03, 2.0560e+04, 1.5120e+04,\n",
      "        1.4560e+03, 4.5056e+04, 2.1808e+04, 2.8800e+04, 2.3520e+04, 3.0048e+04,\n",
      "        3.8912e+04, 4.1728e+04, 3.9584e+04, 4.1856e+04, 1.8912e+04, 1.5856e+04,\n",
      "        3.3664e+04, 1.6368e+04, 2.8840e+03, 4.8896e+04, 1.8336e+04, 1.2112e+04,\n",
      "        2.0304e+04, 2.3040e+03, 9.6960e+03, 8.7440e+03, 4.7040e+03, 4.8480e+03,\n",
      "        5.8240e+04, 1.3496e+04, 3.5008e+04, 1.6256e+04, 8.2320e+03, 2.4360e+03,\n",
      "        3.0496e+04, 6.1952e+04, 1.5470e+03, 6.1160e+03, 4.7160e+03, 2.5168e+04,\n",
      "        3.0752e+04, 9.8560e+03, 1.9584e+04, 3.7984e+04, 3.1536e+04, 5.1648e+04,\n",
      "        1.2544e+04, 4.6720e+04, 1.1872e+04, 1.2080e+04, 3.1500e+03, 4.3840e+04,\n",
      "        4.7560e+03, 2.1776e+04, 2.9000e+03, 4.1120e+03, 5.3472e+04, 2.5536e+04,\n",
      "        9.4400e+03, 1.3984e+04, 2.1780e+03, 9.0720e+03, 3.5552e+04, 3.7568e+04,\n",
      "        4.8608e+04, 8.8160e+03, 1.6448e+04, 2.1504e+04, 3.5072e+04, 2.6260e+03,\n",
      "        4.2912e+04, 4.4425e+02, 5.5808e+04, 1.0240e+04, 3.5808e+04, 2.2288e+04,\n",
      "        2.1712e+04, 1.0920e+03, 2.7264e+04, 3.1104e+04, 2.9580e+03, 4.0864e+04,\n",
      "        2.4048e+04, 1.0456e+04, 1.5240e+04, 4.3488e+04, 5.0480e+03, 3.0624e+04,\n",
      "        6.1440e+04, 4.7520e+04,        inf, 2.8480e+04, 2.3680e+04, 3.3150e+02,\n",
      "        1.1752e+04, 4.3200e+04, 4.7712e+04, 4.5725e+02, 2.3104e+04, 4.3200e+03,\n",
      "        3.5904e+04, 2.2675e+02,        inf, 1.9200e+04, 3.1960e+03,        inf,\n",
      "        2.0672e+04, 3.5360e+04, 8.2240e+03, 2.1312e+04, 1.0330e+03, 7.3840e+03,\n",
      "        5.7856e+04, 4.7296e+04, 1.2416e+04,        inf,        inf, 1.7696e+04,\n",
      "        5.6000e+03, 4.0040e+03, 1.3120e+04, 2.5696e+04, 3.7696e+04, 5.5488e+04,\n",
      "        4.3872e+04, 6.7300e+02, 1.6088e+04, 2.0600e+03, 1.5104e+04, 4.8600e+03,\n",
      "        1.1936e+04, 3.7340e+03, 1.0392e+04, 3.6832e+04, 2.9900e+03, 2.3376e+04,\n",
      "        2.3360e+04, 9.5200e+03, 5.9168e+04, 1.9040e+04, 8.9440e+03, 1.4368e+04,\n",
      "        2.8336e+04, 1.9472e+04, 4.2208e+04, 1.9296e+04, 2.4144e+04, 5.3344e+04,\n",
      "        4.5440e+04, 9.4880e+03, 8.2160e+03, 1.1080e+04, 1.9728e+04, 5.0208e+04,\n",
      "        2.6300e+03, 1.7030e+03, 5.1392e+04, 4.3648e+04, 3.9104e+04, 1.2936e+04,\n",
      "        3.5136e+04, 1.0152e+04, 5.5840e+03, 5.9904e+04, 4.7616e+04, 3.8080e+04,\n",
      "        2.3360e+04, 4.1472e+04, 1.6608e+04, 1.6160e+04, 4.7456e+04, 2.3168e+04,\n",
      "        2.2896e+04, 5.4080e+04, 1.7808e+04, 4.8500e+02, 1.2320e+04, 1.0008e+04,\n",
      "        1.6672e+04, 2.7600e+03, 5.9264e+04, 6.2680e+03, 2.0016e+04, 7.1200e+03,\n",
      "        4.1568e+04, 1.1496e+04,        inf, 1.2472e+04, 4.7392e+04, 3.2800e+04,\n",
      "        2.5488e+04, 7.6360e+03, 3.9296e+04, 1.1712e+04, 1.1744e+04, 5.4816e+04,\n",
      "        1.1488e+04, 1.7728e+04, 3.6925e+02, 5.1136e+04, 3.7280e+03, 3.9360e+03,\n",
      "        7.5320e+03, 2.2960e+04, 1.9840e+04, 4.2720e+03, 1.4840e+04, 1.5230e+03,\n",
      "        4.1240e+03, 3.0672e+04, 3.5200e+04, 1.2616e+04, 2.4544e+04, 1.8736e+04,\n",
      "        2.5216e+04, 4.4352e+04, 2.2160e+03, 2.6544e+04, 4.3552e+04, 5.1480e+03,\n",
      "        9.1680e+03, 6.0320e+04, 1.3152e+04, 9.2080e+03, 2.1696e+04, 5.2256e+04,\n",
      "        1.9664e+04, 7.9680e+03, 1.9040e+04,        inf,        inf, 9.9920e+03,\n",
      "        1.3184e+04, 6.5960e+03, 3.3120e+04, 2.2112e+04, 2.8600e+03, 2.7744e+04,\n",
      "        2.6432e+04, 1.7184e+04, 2.7216e+04, 1.8992e+04, 1.0360e+04, 3.0208e+04,\n",
      "        2.8896e+04, 2.7640e+03, 3.2100e+03, 1.0864e+04, 1.8624e+04, 9.4800e+03,\n",
      "        7.9840e+03, 7.4840e+03, 2.8176e+04, 3.4336e+04, 6.1888e+04, 1.2704e+04,\n",
      "        9.3200e+03, 1.7856e+04, 1.0992e+04, 3.9680e+04, 7.6560e+03,        inf,\n",
      "        3.9232e+04, 2.6336e+04, 3.6352e+04, 6.9240e+03, 8.1040e+03, 4.8800e+04,\n",
      "        6.5920e+03, 2.9968e+04, 4.9888e+04, 4.7520e+04, 4.0768e+04, 4.2560e+04,\n",
      "        3.2720e+04, 2.7920e+04, 1.0216e+04, 2.8832e+04, 3.7020e+03, 5.0176e+04,\n",
      "        2.9584e+04, 1.5496e+04,        inf, 1.0264e+04, 8.0500e+02, 1.2136e+04,\n",
      "        2.0064e+04, 3.1488e+04, 4.0608e+04, 5.9720e+03, 2.5840e+04, 9.6320e+03,\n",
      "        1.3464e+04, 1.4096e+04, 2.4064e+04, 1.4160e+03, 6.3480e+03, 4.1184e+04,\n",
      "        1.2936e+04, 3.8000e+03, 2.0944e+04, 1.3544e+04, 1.6784e+04, 1.9200e+04,\n",
      "        1.0072e+04, 2.1984e+04, 1.8780e+03, 3.9392e+04, 3.3344e+04, 1.5352e+04,\n",
      "        2.7536e+04, 4.0000e+03, 1.7720e+03,        inf, 3.0920e+03, 2.6720e+04,\n",
      "        3.6224e+04, 5.4496e+04, 6.3552e+04, 6.3104e+04, 4.3104e+04, 7.0080e+03,\n",
      "        1.8064e+04, 8.1080e+03, 3.6128e+04, 1.6990e+03, 3.3856e+04, 3.9264e+04,\n",
      "        3.3888e+04, 3.2384e+04, 1.5920e+04, 3.9712e+04, 7.7880e+03, 3.0720e+04,\n",
      "        6.5360e+03, 1.3080e+04, 1.8336e+04,        inf, 5.5104e+04, 1.2768e+04,\n",
      "        1.0592e+04, 1.3072e+04,        inf, 4.7440e+03, 2.5600e+04, 1.8272e+04,\n",
      "        4.3616e+04, 6.3840e+04, 2.4080e+04, 4.1312e+04, 2.3520e+04, 2.7600e+03,\n",
      "        3.6608e+04, 3.7216e+04, 5.5200e+03, 2.4848e+04, 5.8368e+04, 2.8144e+04,\n",
      "        4.8544e+04, 1.0304e+04, 1.3064e+04, 2.4096e+04, 2.1888e+04, 6.0750e+02,\n",
      "        4.1024e+04, 2.6620e+03, 1.8128e+04, 1.4784e+04, 3.2600e+03, 3.3700e+03,\n",
      "        1.2850e+03, 3.7632e+04, 3.4360e+03, 1.3000e+04, 3.8220e+03, 3.9424e+04,\n",
      "        4.5840e+03, 8.7760e+03, 4.5824e+04, 1.7424e+04, 6.1792e+04, 5.4688e+04,\n",
      "        4.0064e+04, 8.2880e+03, 2.5536e+04, 7.2400e+03, 1.4952e+04, 1.1904e+04,\n",
      "        4.8160e+04, 3.4112e+04, 1.6816e+04, 7.1440e+03,        inf, 4.2848e+04,\n",
      "        1.5250e+03, 6.2280e+03, 8.1100e+02, 9.0938e+01, 1.8000e+04, 6.4032e+04,\n",
      "        6.2656e+04, 1.0400e+04, 2.1152e+04, 5.4080e+04, 4.0832e+04,        inf,\n",
      "        2.4064e+04, 2.9696e+04, 2.0016e+04, 5.4496e+04, 1.1200e+04, 1.0048e+04,\n",
      "        5.2040e+03, 4.2912e+04, 4.8768e+04, 1.5792e+04, 3.9264e+04, 1.9872e+04,\n",
      "        2.8528e+04, 1.6672e+04, 3.8940e+03, 3.0920e+03, 3.7440e+04, 1.2200e+04,\n",
      "        1.0536e+04, 2.3392e+04, 1.7744e+04,        inf, 5.4800e+03, 5.1200e+04,\n",
      "        2.8100e+03, 1.2810e+03], device='cuda:0', dtype=torch.float16), tensor([4.7120e+03, 4.2720e+04, 3.2980e+03, 1.2900e+02, 8.7109e+00, 3.5780e+03,\n",
      "        6.4120e+03, 2.1872e+04, 3.2464e+04, 1.2496e+04, 1.1592e+04, 9.5300e+02,\n",
      "        3.0448e+04, 2.4780e+03, 4.8600e+03, 1.8780e+03, 1.8400e+04, 1.9200e+04,\n",
      "        4.1680e+03, 2.0896e+04, 5.5640e+03, 3.0540e+03, 5.9960e+03, 1.1960e+03,\n",
      "        2.9440e+03, 1.6576e+04, 1.7744e+04, 1.1080e+03, 3.0608e+04, 1.9904e+04,\n",
      "        2.2640e+03, 2.8280e+03, 1.2040e+04, 5.1040e+03, 7.5480e+03, 2.7920e+04,\n",
      "        4.1425e+02, 1.2272e+04, 8.3200e+03, 3.9200e+04, 4.0020e+03, 1.9264e+04,\n",
      "        6.6880e+03, 4.7936e+04, 6.4000e+03, 2.9380e+03, 1.5184e+04, 4.3880e+03,\n",
      "        2.7472e+04, 8.2200e+02, 2.0240e+04, 1.0432e+04, 1.0472e+04, 7.9400e+02,\n",
      "        2.4380e+03, 4.1080e+03, 5.0850e+02, 1.0205e+03, 2.3160e+03, 2.3360e+04,\n",
      "        1.5352e+04, 4.1880e+03, 3.0880e+03, 5.9720e+03, 9.3280e+03, 8.3200e+03,\n",
      "        3.5020e+03, 5.0600e+02, 2.8740e+03, 1.0272e+04, 2.8240e+03, 6.2800e+03,\n",
      "        6.1320e+03, 6.2080e+03, 2.8080e+04, 7.1400e+03, 1.0510e+03, 5.8800e+03,\n",
      "        1.9170e+03, 6.7150e+02, 4.0780e+03, 2.9920e+03, 6.1200e+03, 5.7850e+02,\n",
      "        6.7688e+01, 4.3040e+03, 7.8320e+03, 2.2288e+04, 1.8752e+04, 2.0544e+04,\n",
      "        5.9840e+03, 4.0960e+04, 7.8750e+02, 6.2650e+02, 9.1520e+03, 1.0928e+04,\n",
      "        1.9136e+04, 1.5080e+03, 4.2960e+03, 1.0872e+04, 7.7360e+03, 7.4480e+03,\n",
      "        8.1640e+03, 4.2000e+03, 1.3870e+03, 8.6640e+03, 1.3540e+03, 2.9900e+03,\n",
      "        5.8528e+04, 3.6100e+02, 3.8560e+03, 9.8720e+03, 3.0200e+03, 1.3880e+03,\n",
      "        1.1296e+04, 6.3560e+03, 4.5281e+01, 3.2000e+03, 1.7140e+03, 1.8120e+03,\n",
      "        1.1616e+04, 4.8040e+03, 5.0320e+03, 3.0944e+04, 3.2368e+04, 6.0560e+03,\n",
      "        7.1360e+03, 1.8576e+04, 7.9520e+03, 5.3440e+03, 9.5875e+01, 3.1540e+03,\n",
      "        2.8240e+03, 2.4580e+03, 3.3960e+03, 1.0680e+03, 2.7040e+04, 3.9720e+03,\n",
      "        5.4680e+03, 1.0960e+03, 4.3800e+03, 6.4680e+03, 2.8784e+04, 2.2768e+04,\n",
      "        3.1500e+03, 1.4440e+03, 4.5840e+03, 5.1000e+03, 6.5440e+03, 3.0600e+03,\n",
      "        6.8280e+03, 1.3190e+03, 2.0960e+04, 3.4860e+03, 3.6360e+03, 4.3480e+03,\n",
      "        1.5664e+04, 4.6225e+02, 3.5960e+03, 6.8440e+03, 9.8400e+02, 1.1912e+04,\n",
      "        3.8592e+04, 7.6000e+03, 8.6600e+02, 8.6050e+02, 8.8240e+03, 1.2730e+03,\n",
      "        1.6360e+04, 1.2820e+03, 4.1408e+04, 2.6128e+04, 6.0960e+03, 8.4850e+02,\n",
      "        3.5140e+03, 5.7856e+04, 2.3472e+04, 1.4300e+02, 2.7904e+04, 3.4580e+03,\n",
      "        3.1296e+04, 1.6400e+03, 6.5184e+04, 4.7360e+03, 4.8080e+03, 1.4088e+04,\n",
      "        5.1560e+03, 1.2008e+04, 5.3320e+03, 1.6048e+04, 1.8325e+02, 5.2560e+03,\n",
      "        2.1360e+04, 3.3376e+04, 9.9760e+03, 1.4344e+04, 4.8064e+04, 1.8690e+03,\n",
      "        6.2160e+03, 2.3140e+03, 1.9088e+04, 4.9960e+03, 2.5920e+04, 1.1832e+04,\n",
      "        4.9360e+03, 4.8100e+02, 7.2440e+03, 1.6730e+03, 2.8256e+04, 1.6200e+03,\n",
      "        1.2600e+03, 7.9100e+02, 7.8280e+03, 6.8450e+02, 2.3120e+03, 1.4032e+04,\n",
      "        1.9776e+04, 9.8160e+03, 3.6864e+04, 3.4180e+03, 7.3760e+03, 7.0680e+03,\n",
      "        6.4480e+03, 1.0352e+04, 2.6976e+04, 6.9160e+03, 1.9168e+04, 2.7088e+04,\n",
      "        2.5200e+04, 3.1000e+03, 5.0760e+03, 5.4600e+03, 8.9040e+03, 1.4440e+04,\n",
      "        1.6590e+03, 1.0215e+03, 6.5040e+03, 2.0672e+04, 1.7904e+04, 4.2960e+03,\n",
      "        1.0496e+04, 1.3040e+03, 1.7212e+02, 2.2288e+04, 3.2512e+04, 1.9024e+04,\n",
      "        1.8336e+04, 4.2304e+04, 3.2775e+02, 1.7616e+04, 3.0176e+04, 5.2120e+03,\n",
      "        1.1448e+04, 3.3440e+04, 4.7840e+03, 4.4800e+02, 2.2580e+03, 1.7970e+03,\n",
      "        1.0760e+04, 3.6940e+03, 1.5248e+04, 7.4640e+03, 7.8000e+03, 8.0960e+03,\n",
      "        2.2320e+04, 1.5810e+03, 4.8672e+04, 1.6820e+03, 3.8112e+04, 6.9480e+03,\n",
      "        1.4024e+04, 1.3480e+03, 3.5264e+04, 1.0040e+04, 3.3562e+01, 4.4320e+04,\n",
      "        2.9540e+03, 8.0640e+03, 4.9400e+03, 3.3536e+04, 4.4120e+03, 2.4320e+03,\n",
      "        3.0220e+03, 3.9620e+03, 2.2340e+03, 2.7860e+03, 8.5840e+03, 3.9575e+02,\n",
      "        9.8950e+02, 2.6272e+04, 1.7930e+03, 2.0848e+04, 4.7040e+03, 4.0250e+02,\n",
      "        1.1808e+04, 1.2704e+04, 4.8440e+03, 3.4368e+04, 1.6800e+04, 6.7400e+03,\n",
      "        4.3880e+03, 6.6520e+03, 3.6380e+03, 3.9820e+03, 4.9240e+03, 3.3312e+04,\n",
      "        6.2480e+03, 7.3320e+03, 3.4380e+03, 6.1088e+04, 3.7440e+04, 5.1000e+03,\n",
      "        4.4680e+03, 2.4060e+03, 3.0752e+04, 1.2960e+04, 1.7550e+03, 2.1200e+03,\n",
      "        5.3720e+03, 1.4648e+04, 8.2000e+03, 1.7770e+03, 2.2220e+03, 6.8720e+03,\n",
      "        2.6016e+04, 8.5040e+03, 3.8200e+03, 1.5208e+04, 1.7248e+04, 7.9320e+03,\n",
      "        2.1460e+03, 6.6800e+03, 2.0544e+04, 1.1504e+04, 2.2496e+04, 3.4016e+04,\n",
      "        9.7760e+03, 6.5640e+03, 5.9360e+03, 3.0680e+03, 1.8352e+04,        inf,\n",
      "        3.3888e+04, 1.9936e+04, 2.2032e+04, 2.4120e+03, 5.0156e+01, 1.5008e+04,\n",
      "        1.3790e+03, 4.6560e+03, 1.5416e+04, 8.2160e+03, 4.2976e+04, 2.5408e+04,\n",
      "        3.5040e+03, 9.3040e+03, 1.9440e+03, 9.7520e+03, 2.5860e+03, 4.8440e+03,\n",
      "        2.1600e+04, 4.3400e+03, 5.8432e+04, 3.3980e+03, 1.8760e+03, 9.8160e+03,\n",
      "        5.4440e+03, 8.1480e+03, 1.1624e+04, 9.2350e+02, 9.8080e+03, 5.3440e+03,\n",
      "        8.6880e+03, 1.7920e+04, 1.1880e+04, 9.6950e+02, 2.1380e+03, 1.0056e+04,\n",
      "        5.2840e+03, 4.5520e+03, 2.0000e+04, 2.7280e+03, 1.7984e+04, 5.5900e+02,\n",
      "        3.3940e+03, 2.2016e+04, 2.3260e+03, 2.5984e+04, 3.6768e+04, 1.0672e+04,\n",
      "        3.6448e+04, 1.5238e+02, 3.5260e+03, 4.9792e+04, 3.4760e+03, 2.4128e+04,\n",
      "        3.8048e+04, 3.8100e+03, 5.8848e+04, 3.4464e+04, 2.3072e+04, 4.5575e+02,\n",
      "        8.8000e+03, 3.8660e+03, 3.5904e+04, 1.1960e+03, 1.4192e+04, 4.9696e+04,\n",
      "        3.7248e+04, 1.4560e+04, 1.1904e+04, 2.4720e+04, 6.5840e+03, 1.2592e+04,\n",
      "        1.2430e+03, 7.8640e+03, 2.4384e+04,        inf, 4.3936e+04, 5.0350e+02,\n",
      "        1.2592e+04, 1.0616e+04,        inf, 8.5920e+03, 3.8848e+04, 2.1216e+04,\n",
      "        2.4864e+04, 5.0560e+04, 1.1096e+04, 3.0960e+04, 1.9072e+04, 1.2896e+04,\n",
      "        4.8064e+04, 1.7280e+04, 9.6880e+03, 1.0224e+04, 5.4464e+04, 1.4624e+04,\n",
      "        3.3760e+03, 5.6760e+03, 7.4920e+03, 2.6448e+04, 7.9720e+03, 3.7920e+03,\n",
      "        3.5680e+04, 3.0900e+02, 1.6080e+04, 1.8690e+03, 2.4000e+03, 3.6100e+02,\n",
      "        1.6130e+03, 1.9952e+04, 8.1950e+02, 2.3100e+03, 2.4260e+03, 3.8368e+04,\n",
      "        6.4280e+03, 2.9340e+03, 1.0296e+04, 1.5808e+04, 4.0064e+04, 4.3960e+03,\n",
      "        2.0560e+04, 1.3984e+04, 5.7920e+03, 3.8400e+03, 1.8016e+04, 1.4112e+04,\n",
      "        4.2080e+04, 3.9800e+03, 2.3568e+04, 7.1560e+03, 2.3424e+04, 3.5104e+04,\n",
      "        7.0440e+03, 8.4000e+03, 4.1440e+03, 2.1060e+03, 1.0544e+04,        inf,\n",
      "        2.4656e+04, 2.8500e+03, 1.1500e+03, 2.0912e+04, 1.0032e+04,        inf,\n",
      "        1.9248e+04, 3.2576e+04, 1.2008e+04, 2.1952e+04, 2.4400e+03, 1.6152e+04,\n",
      "        2.1120e+03, 2.5072e+04, 2.4816e+04, 7.3760e+03, 1.4504e+04, 4.8360e+03,\n",
      "        1.4952e+04, 2.5776e+04, 1.0850e+03, 2.7460e+03, 5.9200e+03, 1.1984e+04,\n",
      "        6.3800e+03, 2.8624e+04, 2.0560e+04, 6.9400e+03, 6.7120e+03, 1.7808e+04,\n",
      "        4.6320e+03, 1.5050e+03], device='cuda:0', dtype=torch.float16), tensor([7.8320e+03,        inf, 1.4920e+04, 1.3768e+04, 1.7730e+03, 2.2560e+04,\n",
      "        3.3240e+03, 1.3016e+04, 4.4128e+04, 4.3168e+04, 3.2960e+04, 1.8608e+04,\n",
      "        4.3584e+04, 2.9856e+04, 9.1680e+03, 3.6700e+03, 2.7968e+04, 1.7440e+04,\n",
      "        9.6080e+03, 2.4096e+04, 3.1648e+04, 1.3600e+02, 5.2128e+04, 2.9856e+04,\n",
      "        4.2912e+04, 2.3248e+04, 3.1376e+04, 6.8360e+03, 2.8976e+04, 1.9536e+04,\n",
      "        4.2960e+03, 1.1568e+04, 1.9104e+04, 1.0968e+04, 8.1040e+03, 5.5264e+04,\n",
      "        2.8464e+04, 2.0688e+04, 5.0176e+04,        inf, 1.3456e+04, 4.8192e+04,\n",
      "        1.7744e+04, 5.1072e+04, 1.6608e+04, 2.7420e+03, 2.9776e+04, 9.1440e+03,\n",
      "        5.8464e+04, 9.8720e+03, 2.2384e+04, 2.6192e+04, 4.0840e+03, 1.1624e+04,\n",
      "        3.3408e+04, 1.4032e+04, 2.0270e+03, 2.4400e+03, 3.5120e+03, 4.3520e+04,\n",
      "        2.9200e+04, 1.1536e+04, 6.2400e+03, 1.4376e+04, 1.0528e+04, 2.7984e+04,\n",
      "        1.2064e+04, 7.6240e+03, 3.4250e+01, 2.1072e+04, 1.3504e+04, 4.6752e+04,\n",
      "        2.7568e+04, 9.3840e+03, 3.2400e+04, 7.5800e+02, 2.7152e+04, 2.5888e+04,\n",
      "        9.4640e+03, 8.0240e+03, 2.3728e+04, 1.9630e+03, 1.9328e+04, 1.4024e+04,\n",
      "        7.7650e+02, 4.7936e+04, 1.9888e+04, 3.1232e+04, 2.1232e+04, 2.8400e+04,\n",
      "        3.7344e+04, 4.4416e+04, 3.9072e+04, 4.1120e+04, 1.9600e+04, 1.7680e+04,\n",
      "        3.1680e+04, 1.6320e+04, 2.9520e+03, 4.9536e+04, 1.7344e+04, 1.0376e+04,\n",
      "        2.2160e+04, 2.0090e+03, 8.6560e+03, 1.0608e+04, 4.8920e+03, 3.1480e+03,\n",
      "        5.7184e+04, 1.5160e+04, 3.5680e+04, 1.7408e+04, 9.2720e+03, 1.5340e+03,\n",
      "        3.0144e+04, 6.3296e+04, 3.0080e+03, 5.2600e+03, 4.9960e+03, 2.6528e+04,\n",
      "        3.2688e+04, 8.4560e+03, 2.0496e+04, 3.6608e+04, 3.0848e+04, 5.2672e+04,\n",
      "        1.0992e+04, 4.7488e+04, 1.0408e+04, 1.1552e+04, 4.8840e+03, 4.5184e+04,\n",
      "        4.5920e+03, 2.1808e+04, 2.8840e+03, 5.5480e+03, 5.4432e+04, 2.8656e+04,\n",
      "        8.9520e+03, 1.3040e+04, 1.9080e+03, 9.9040e+03, 3.6672e+04, 3.9616e+04,\n",
      "        4.8032e+04, 8.9760e+03, 1.8144e+04, 2.2576e+04, 3.5008e+04, 3.5200e+03,\n",
      "        4.0416e+04, 1.3000e+03, 5.5776e+04, 1.0144e+04, 3.5776e+04, 2.2304e+04,\n",
      "        2.2800e+04, 1.6870e+03, 2.6064e+04, 3.0544e+04, 8.6400e+02, 4.1568e+04,\n",
      "        2.6384e+04, 9.1360e+03, 1.6312e+04, 4.2304e+04, 7.8240e+03, 3.0880e+04,\n",
      "        6.2240e+04, 4.8064e+04,        inf, 2.7872e+04, 2.4320e+04, 6.3400e+02,\n",
      "        1.0424e+04, 4.2720e+04, 4.7296e+04, 3.6150e+02, 2.2800e+04, 5.7600e+03,\n",
      "        3.6512e+04, 6.6550e+02,        inf, 1.9200e+04, 9.6050e+02,        inf,\n",
      "        2.0448e+04, 3.5456e+04, 6.6000e+03, 2.1312e+04, 1.4130e+03, 9.0320e+03,\n",
      "        5.9360e+04, 4.7552e+04, 1.1616e+04, 6.5280e+04,        inf, 1.6240e+04,\n",
      "        4.4880e+03, 3.6500e+03, 1.1904e+04, 2.4832e+04, 3.8528e+04, 5.4432e+04,\n",
      "        4.5376e+04, 3.0975e+02, 1.7264e+04, 1.8800e+03, 1.6136e+04, 3.5160e+03,\n",
      "        1.2416e+04, 3.6360e+03, 1.0080e+04, 3.5680e+04, 4.0180e+03, 2.3648e+04,\n",
      "        2.2752e+04, 1.2160e+04, 6.0064e+04, 1.8896e+04, 8.1120e+03, 1.3000e+04,\n",
      "        2.8016e+04, 1.9312e+04, 4.3264e+04, 1.9376e+04, 2.4208e+04, 5.3216e+04,\n",
      "        4.4832e+04, 1.0184e+04, 7.8960e+03, 1.0384e+04, 1.8384e+04, 5.1296e+04,\n",
      "        2.1420e+03, 2.7520e+03, 5.2128e+04, 4.3712e+04, 3.8432e+04, 1.1656e+04,\n",
      "        3.6192e+04, 9.3200e+03, 7.6760e+03, 5.8464e+04, 4.8224e+04, 3.8944e+04,\n",
      "        2.3552e+04, 4.1184e+04, 1.6784e+04, 1.5512e+04, 4.7552e+04, 2.3664e+04,\n",
      "        2.2208e+04, 5.5040e+04, 2.0192e+04, 2.6325e+02, 1.2616e+04, 1.0424e+04,\n",
      "        1.7008e+04, 3.7520e+03, 5.8592e+04, 8.2480e+03, 2.0688e+04, 6.8720e+03,\n",
      "        4.2720e+04, 1.1816e+04,        inf, 1.1240e+04, 4.7424e+04, 3.2144e+04,\n",
      "        2.6016e+04, 8.4720e+03, 3.8560e+04, 1.1616e+04, 1.1240e+04, 5.3024e+04,\n",
      "        1.0752e+04, 1.6976e+04, 8.1062e+01, 5.1360e+04, 4.6560e+03, 3.5200e+03,\n",
      "        8.5200e+03, 2.1776e+04, 2.0352e+04, 5.9520e+03, 1.4280e+04, 1.9820e+03,\n",
      "        4.2160e+03, 3.0432e+04, 3.4976e+04, 1.1464e+04, 2.3392e+04, 1.7616e+04,\n",
      "        2.6096e+04, 4.1984e+04, 2.2340e+03, 2.7440e+04, 4.3232e+04, 5.3320e+03,\n",
      "        9.2400e+03, 6.0288e+04, 1.2424e+04, 8.1600e+03, 2.1184e+04, 5.3280e+04,\n",
      "        1.8336e+04, 7.6200e+03, 2.0160e+04,        inf,        inf, 1.0088e+04,\n",
      "        1.3256e+04, 7.5760e+03, 3.2672e+04, 2.2352e+04, 2.7060e+03, 2.7424e+04,\n",
      "        2.4864e+04, 1.7888e+04, 2.6848e+04, 1.8128e+04, 9.7280e+03, 2.9440e+04,\n",
      "        2.9632e+04, 2.4240e+03, 2.6560e+03, 1.1384e+04, 1.8912e+04, 1.1008e+04,\n",
      "        8.0440e+03, 8.0840e+03, 2.7648e+04, 3.4176e+04, 6.2368e+04, 1.4656e+04,\n",
      "        1.0096e+04, 1.7440e+04, 1.1920e+04, 4.2464e+04, 8.5840e+03,        inf,\n",
      "        3.9712e+04, 2.6384e+04, 3.5136e+04, 8.3520e+03, 7.9200e+03, 4.8832e+04,\n",
      "        5.6880e+03, 2.7552e+04, 5.0496e+04, 4.8608e+04, 4.0544e+04, 4.1088e+04,\n",
      "        3.3920e+04, 2.6304e+04, 1.2472e+04, 2.7248e+04, 5.5240e+03, 5.1584e+04,\n",
      "        2.7776e+04, 1.4216e+04,        inf, 9.8000e+03, 1.1160e+03, 8.9520e+03,\n",
      "        2.1152e+04, 3.2800e+04, 3.8816e+04, 8.0120e+03, 2.3456e+04, 1.2344e+04,\n",
      "        1.0976e+04, 1.1312e+04, 2.1840e+04, 8.1100e+02, 8.4400e+03, 4.3008e+04,\n",
      "        1.0544e+04, 5.1760e+03, 2.3904e+04, 1.0480e+04, 1.9040e+04, 1.8832e+04,\n",
      "        1.2264e+04, 1.8400e+04, 3.5660e+03, 3.9648e+04, 3.1344e+04, 1.6784e+04,\n",
      "        3.0192e+04, 4.1800e+03, 5.2880e+03,        inf, 5.5440e+03, 2.8768e+04,\n",
      "        3.9040e+04, 5.6192e+04, 6.1408e+04, 6.4512e+04, 4.1472e+04, 7.1040e+03,\n",
      "        1.5440e+04, 7.1160e+03, 3.7696e+04, 6.0150e+02, 3.1424e+04, 4.2336e+04,\n",
      "        3.6128e+04, 3.5552e+04, 1.8336e+04, 3.8272e+04, 1.1024e+04, 2.8592e+04,\n",
      "        6.7600e+03, 1.4616e+04, 2.1696e+04,        inf, 5.7056e+04, 1.2736e+04,\n",
      "        8.4640e+03, 1.0672e+04,        inf, 7.6720e+03, 2.2720e+04, 1.6624e+04,\n",
      "        4.0640e+04, 6.0672e+04, 2.5024e+04, 3.8912e+04, 2.5680e+04, 5.9280e+03,\n",
      "        4.0224e+04, 3.9104e+04, 7.8120e+03, 2.6640e+04, 5.5744e+04, 2.6912e+04,\n",
      "        4.6592e+04, 1.2648e+04, 1.4568e+04, 2.6352e+04, 1.9344e+04, 2.5480e+03,\n",
      "        3.8464e+04, 1.5540e+03, 1.8608e+04, 1.6480e+04, 4.0800e+03, 1.9320e+03,\n",
      "        1.6760e+03, 3.6384e+04, 2.6540e+03, 1.0920e+04, 3.3080e+03, 4.2368e+04,\n",
      "        6.8640e+03, 6.0840e+03, 4.4160e+04, 1.5176e+04, 6.0160e+04, 5.6928e+04,\n",
      "        3.8464e+04, 1.1416e+04, 2.6000e+04, 5.3480e+03, 1.6832e+04, 1.5232e+04,\n",
      "        4.5664e+04, 3.6224e+04, 2.0176e+04, 9.4160e+03,        inf, 4.5472e+04,\n",
      "        1.6888e+02, 1.0272e+04, 1.2920e+03, 1.1670e+03, 2.0080e+04,        inf,\n",
      "        6.0704e+04, 1.0544e+04, 1.9232e+04, 5.7312e+04, 4.1120e+04,        inf,\n",
      "        2.5696e+04, 3.1520e+04, 1.7472e+04, 5.3792e+04, 1.1736e+04, 1.2640e+04,\n",
      "        6.3760e+03, 4.4768e+04, 4.6656e+04, 1.6816e+04, 3.7888e+04, 1.6216e+04,\n",
      "        3.0176e+04, 2.0384e+04, 2.6960e+03, 1.8725e+02, 3.7024e+04, 8.8640e+03,\n",
      "        9.0320e+03, 2.1040e+04, 1.9808e+04,        inf, 7.2800e+03, 5.2768e+04,\n",
      "        5.8440e+03, 2.2520e+03], device='cuda:0', dtype=torch.float16), tensor([[  718.5000,   573.5000,    85.4375,  ...,  2009.0000,  1325.0000,\n",
      "          1435.0000],\n",
      "        [  110.1250,   626.0000,   202.7500,  ...,   876.5000,   549.0000,\n",
      "           116.0000],\n",
      "        [ 1244.0000,   841.0000,   567.5000,  ...,  2230.0000,  1705.0000,\n",
      "           665.0000],\n",
      "        ...,\n",
      "        [ 7148.0000, 11544.0000,  3604.0000,  ..., 27776.0000, 23360.0000,\n",
      "         28128.0000],\n",
      "        [ 5480.0000, 24224.0000,  8008.0000,  ..., 21248.0000, 31280.0000,\n",
      "         39808.0000],\n",
      "        [ 2796.0000,  4608.0000,   675.0000,  ..., 11984.0000, 16352.0000,\n",
      "         14264.0000]], device='cuda:0', dtype=torch.float16), tensor([ 1490.0000,   521.5000,  1271.0000,  ..., 27712.0000, 35584.0000,\n",
      "        14352.0000], device='cuda:0', dtype=torch.float16), tensor([[20944., 15232., 37088.,  ..., 13200.,  6132.,  3886.],\n",
      "        [ 6816., 19072., 13200.,  ..., 10352.,    inf, 30016.],\n",
      "        [29808.,  9008., 29312.,  ..., 24640., 12024.,  3032.],\n",
      "        ...,\n",
      "        [27472.,  6868., 21600.,  ...,  4256., 32288., 11240.],\n",
      "        [13928., 14240.,  4312.,  ...,   288., 18864., 12696.],\n",
      "        [35616.,  3712., 35712.,  ...,   312., 35200., 15400.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([6.7920e+03,        inf, 3.1344e+04,        inf,        inf, 5.6992e+04,\n",
      "        4.8992e+04, 1.4944e+04, 2.3184e+04, 4.2320e+03, 2.5300e+03, 3.1456e+04,\n",
      "        3.0784e+04, 4.7296e+04,        inf,        inf, 1.3880e+04, 2.2240e+04,\n",
      "        3.8304e+04, 5.0336e+04, 3.0120e+03, 5.0848e+04,        inf,        inf,\n",
      "        5.6672e+04,        inf, 3.1211e+00, 2.5440e+04,        inf, 5.9120e+03,\n",
      "        3.0800e+04, 5.6440e+03, 1.3664e+04,        inf, 6.4064e+04, 1.1576e+04,\n",
      "               inf, 4.8128e+04,        inf,        inf,        inf,        inf,\n",
      "               inf, 4.3168e+04, 1.8592e+04, 1.5224e+04, 4.2624e+04, 3.1552e+04,\n",
      "        1.9808e+04, 1.2790e+03, 1.9808e+04, 6.3136e+04, 1.7984e+04,        inf,\n",
      "               inf,        inf, 3.5168e+04, 3.4560e+04, 2.9104e+04, 3.1040e+03,\n",
      "               inf,        inf, 4.4512e+04, 2.4240e+04,        inf,        inf,\n",
      "        6.2624e+04, 4.7520e+04, 4.7776e+04, 4.4352e+04, 3.5552e+04,        inf,\n",
      "        5.7568e+04,        inf, 3.8944e+04,        inf, 5.3792e+04, 4.4480e+04,\n",
      "        4.0320e+04, 6.0480e+04, 2.1168e+04, 7.1000e+03, 3.8944e+04,        inf,\n",
      "        6.7760e+03, 4.5984e+04, 2.1552e+04,        inf,        inf, 4.2176e+04,\n",
      "        1.5656e+04, 3.2336e+04, 4.6976e+04,        inf, 9.5680e+03, 1.4952e+04,\n",
      "        3.0736e+04, 4.3072e+04, 6.4672e+04, 5.9040e+04, 2.5968e+04, 1.6464e+04,\n",
      "        1.9552e+04,        inf,        inf, 3.7632e+04, 7.7000e+03,        inf,\n",
      "               inf, 4.0260e+03,        inf, 1.0288e+04, 8.3680e+03, 4.4928e+04,\n",
      "        2.3200e+04, 2.1696e+04, 2.2528e+04, 1.7520e+04, 4.9632e+04, 5.7312e+04,\n",
      "        1.8576e+04,        inf,        inf,        inf,        inf, 6.1440e+04,\n",
      "               inf, 3.1344e+04, 3.1392e+04, 6.1760e+04, 5.5680e+03,        inf,\n",
      "               inf, 1.1296e+04, 4.7072e+04, 7.5960e+03, 6.7320e+03, 5.7920e+04,\n",
      "               inf, 5.9936e+04, 3.8368e+04,        inf, 5.0272e+04,        inf,\n",
      "        5.5776e+04, 6.2496e+04, 1.8320e+04, 3.0128e+04, 4.0736e+04, 5.2832e+04,\n",
      "        5.7056e+04, 2.6928e+04, 1.5592e+04, 1.9232e+04, 1.9728e+04, 2.1840e+04,\n",
      "        4.1920e+04, 1.5064e+04,        inf,        inf, 3.7504e+04, 3.2432e+04,\n",
      "               inf,        inf, 5.3824e+04,        inf,        inf, 5.0624e+04,\n",
      "               inf,        inf, 5.0680e+03, 2.9792e+04, 3.7500e+03, 5.7760e+04,\n",
      "        5.5280e+03,        inf, 1.1240e+03, 3.6768e+04, 1.2096e+04, 1.0744e+04,\n",
      "        6.2880e+04, 6.2752e+04, 3.1920e+04, 6.3648e+04, 2.5888e+04,        inf,\n",
      "        1.2928e+04, 1.9104e+04, 5.5584e+04, 2.2768e+04, 4.1568e+04, 6.2112e+04,\n",
      "        3.8368e+04, 5.0144e+04, 5.7920e+04, 2.6736e+04, 3.0704e+04, 4.0864e+04,\n",
      "        3.7600e+04,        inf, 1.7488e+04, 1.6016e+04,        inf,        inf,\n",
      "        4.8096e+04,        inf, 6.4160e+04,        inf, 1.9136e+04, 4.8096e+04,\n",
      "               inf,        inf, 2.0176e+04, 1.9104e+04,        inf, 1.2048e+04,\n",
      "        5.9264e+04,        inf, 9.5920e+03,        inf, 3.8432e+04,        inf,\n",
      "               inf,        inf,        inf, 6.5024e+04,        inf,        inf,\n",
      "               inf, 2.8240e+04, 6.1600e+03, 2.3264e+04,        inf, 2.8736e+04,\n",
      "        1.4784e+04, 6.3880e+03, 5.9584e+04,        inf, 4.5888e+04, 5.4944e+04,\n",
      "        2.0976e+04, 3.9680e+04,        inf, 3.0368e+04, 4.6720e+03,        inf,\n",
      "        2.7712e+04, 5.3376e+04, 8.6880e+03,        inf, 3.7088e+04, 5.4272e+04,\n",
      "        8.2080e+03, 1.8672e+04,        inf, 3.5584e+04, 1.6608e+04, 3.9264e+04,\n",
      "        1.4744e+04,        inf,        inf, 1.6320e+04, 5.5232e+04, 5.5520e+04,\n",
      "        6.2176e+04,        inf,        inf, 4.0256e+04,        inf, 2.4384e+04,\n",
      "        2.6960e+04,        inf, 1.9536e+04, 9.7440e+03, 1.7120e+04, 2.3860e+03,\n",
      "               inf, 6.4480e+04,        inf,        inf, 2.1648e+04, 4.2304e+04,\n",
      "        4.1376e+04, 3.3472e+04,        inf,        inf, 2.9264e+04, 1.1192e+04,\n",
      "        5.0496e+04, 3.8272e+04, 4.7200e+03, 3.8600e+03,        inf,        inf,\n",
      "        3.5200e+04, 4.8000e+04,        inf, 1.6720e+04, 8.5280e+03,        inf,\n",
      "        1.4180e+03, 2.6528e+04,        inf, 4.2368e+04, 1.0730e+03, 3.2432e+04,\n",
      "        4.8480e+03, 6.0064e+04, 5.1360e+04, 6.4064e+04, 2.4832e+04, 1.3984e+04,\n",
      "        3.0112e+04, 5.0368e+04, 6.4736e+04, 1.3840e+04,        inf, 3.2480e+04,\n",
      "        3.2608e+04,        inf, 3.1312e+04,        inf, 4.4000e+03, 3.8304e+04,\n",
      "               inf, 6.7440e+03,        inf, 1.7728e+04, 4.0896e+04,        inf,\n",
      "        6.7960e+03,        inf,        inf,        inf, 4.0416e+04,        inf,\n",
      "        1.2568e+04, 4.3840e+04, 5.4016e+04,        inf,        inf, 1.7280e+04,\n",
      "               inf,        inf,        inf, 3.3440e+04, 2.6620e+03, 4.1880e+03,\n",
      "        5.1424e+04,        inf,        inf, 2.2256e+04, 1.3376e+04, 2.4304e+04,\n",
      "        2.4700e+03,        inf, 4.2432e+04, 1.9376e+04, 4.5664e+04,        inf,\n",
      "        1.6912e+04, 3.1792e+04,        inf, 3.1824e+04,        inf, 8.7440e+03,\n",
      "        1.5384e+04,        inf,        inf, 3.7696e+04,        inf, 4.2240e+03,\n",
      "        1.3464e+04, 4.8576e+04,        inf, 4.1568e+04, 4.9664e+04, 7.8280e+03,\n",
      "        3.5008e+04, 2.1744e+04, 2.1216e+04, 2.8944e+04, 9.4320e+03, 9.2080e+03,\n",
      "        1.5256e+04, 1.7200e+04, 1.6512e+04, 5.7408e+04, 4.9344e+04,        inf,\n",
      "        3.7792e+04, 4.5184e+04, 3.3600e+04,        inf, 2.8464e+04, 4.1568e+04,\n",
      "        2.8920e+03, 2.2080e+04, 1.8080e+04, 4.3264e+04,        inf, 2.1872e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 5.3248e+04,\n",
      "               inf,        inf, 8.3600e+03, 5.7376e+04, 5.0464e+04, 1.2520e+04,\n",
      "        3.0784e+04, 4.3584e+04, 5.3568e+04, 2.2544e+04,        inf, 8.3840e+03,\n",
      "               inf, 3.4336e+04,        inf, 4.2784e+04,        inf, 1.0344e+04,\n",
      "        5.1440e+03,        inf, 5.7344e+04, 6.0544e+04, 4.1088e+04,        inf,\n",
      "        3.9392e+04,        inf, 2.6608e+04,        inf,        inf, 5.1712e+04,\n",
      "        2.5600e+04, 1.6030e+03, 6.0000e+04, 3.0112e+04, 3.5552e+04, 7.4700e+02,\n",
      "        9.8720e+03, 6.1152e+04,        inf, 3.7472e+04, 6.1240e+03,        inf,\n",
      "        4.4928e+04, 4.9312e+04, 4.3648e+04, 5.2480e+04, 6.2816e+04, 2.1344e+04,\n",
      "               inf, 2.9264e+04, 2.7584e+04, 4.2304e+04, 1.1120e+04,        inf,\n",
      "        8.3760e+03, 3.9776e+04,        inf, 4.1888e+04, 1.2248e+04, 3.1568e+04,\n",
      "        5.5392e+04,        inf, 7.4560e+03, 5.3824e+04, 3.4304e+04, 3.5168e+04,\n",
      "        1.0784e+04, 5.6960e+04, 4.9088e+04,        inf, 1.1168e+04, 2.8880e+04,\n",
      "        6.5152e+04, 1.7600e+04, 5.4080e+04, 2.9440e+04, 4.9056e+04, 1.5496e+04,\n",
      "        4.6656e+04, 2.8208e+04,        inf, 6.2240e+04, 5.0080e+04, 8.0960e+03,\n",
      "        5.0000e+02, 1.8912e+04, 2.4464e+04,        inf, 3.2224e+04, 3.5040e+04,\n",
      "               inf,        inf, 2.2000e+03,        inf, 5.2680e+03, 4.5248e+04,\n",
      "        3.2864e+04, 9.6800e+03,        inf,        inf,        inf, 1.4944e+04,\n",
      "        3.8848e+04, 2.0752e+04], device='cuda:0', dtype=torch.float16), tensor([[  993.0000,   611.5000,   857.0000,  ...,  1536.0000,   911.0000,\n",
      "          1188.0000],\n",
      "        [ 1718.0000,   158.5000,    59.7812,  ...,   760.5000,  1078.0000,\n",
      "          1578.0000],\n",
      "        [ 2836.0000,  1087.0000,   552.0000,  ...,  1963.0000,  2324.0000,\n",
      "          1656.0000],\n",
      "        ...,\n",
      "        [ 9064.0000, 24576.0000,  3152.0000,  ..., 24336.0000, 32928.0000,\n",
      "         36160.0000],\n",
      "        [ 7260.0000, 14808.0000,  2518.0000,  ..., 11920.0000, 25536.0000,\n",
      "         19280.0000],\n",
      "        [11184.0000, 12400.0000,  4236.0000,  ...,  1064.0000, 42112.0000,\n",
      "         13240.0000]], device='cuda:0', dtype=torch.float16), tensor([ 1517.,  1025.,  2282.,  ..., 41632., 28064., 37632.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[ 7560., 29920.,  9888.,  ..., 22960.,  5136., 22336.],\n",
      "        [11736.,  6936., 11192.,  ..., 32992.,  8712., 18032.],\n",
      "        [10160., 20528.,  5356.,  ..., 16544.,  4832., 24032.],\n",
      "        ...,\n",
      "        [  566., 27792., 11128.,  ..., 28320.,  2652., 11568.],\n",
      "        [ 7252.,  2802.,  1102.,  ..., 10000.,  6480., 14760.],\n",
      "        [ 1184., 25472., 11520.,  ..., 42080.,  1494., 10960.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([1.3072e+04, 6.1696e+04, 2.8256e+04,        inf, 5.3152e+04, 4.5664e+04,\n",
      "        3.2112e+04, 2.3360e+03, 1.2488e+04, 6.0680e+03, 9.2800e+02, 2.7616e+04,\n",
      "        1.9248e+04, 2.9504e+04,        inf,        inf, 3.2320e+03, 1.9792e+04,\n",
      "        3.1856e+04, 3.4528e+04, 3.5080e+03, 4.1088e+04,        inf,        inf,\n",
      "        4.1248e+04, 5.5776e+04, 5.0880e+03, 1.2144e+04,        inf, 2.4620e+03,\n",
      "        2.7472e+04, 5.1800e+03, 5.9160e+03, 6.0160e+04, 5.4880e+04, 5.6320e+03,\n",
      "               inf, 4.4736e+04,        inf, 6.0480e+04, 6.0352e+04,        inf,\n",
      "               inf, 3.2336e+04, 2.7776e+04, 1.5288e+04, 3.8560e+04, 2.4816e+04,\n",
      "        1.2768e+04, 1.7000e+03, 1.1384e+04, 4.2944e+04, 1.6928e+04, 6.0608e+04,\n",
      "               inf,        inf, 2.1680e+04, 3.0160e+04, 2.4400e+04, 4.4960e+03,\n",
      "               inf,        inf, 3.0336e+04, 1.5960e+04, 5.5264e+04, 5.1712e+04,\n",
      "        3.8752e+04, 2.5760e+04, 3.7600e+04, 2.9696e+04, 2.6896e+04,        inf,\n",
      "        4.1376e+04,        inf, 3.1904e+04,        inf, 4.8192e+04, 2.9632e+04,\n",
      "        2.5936e+04, 4.8384e+04, 2.3984e+04, 1.1552e+04, 3.0720e+04, 6.4768e+04,\n",
      "        1.6152e+04, 3.0896e+04, 1.7488e+04, 5.2480e+04,        inf, 3.0816e+04,\n",
      "        1.5432e+04, 2.7200e+04, 3.5616e+04,        inf, 1.0272e+04, 1.4744e+04,\n",
      "        2.7984e+04, 3.2704e+04, 4.5696e+04, 3.5680e+04, 1.7936e+04, 1.1944e+04,\n",
      "        1.8992e+04,        inf, 6.1824e+04, 2.8320e+04, 1.1520e+03,        inf,\n",
      "        6.1632e+04, 9.6650e+02,        inf, 1.0064e+04, 2.5780e+03, 2.7776e+04,\n",
      "        2.2720e+04, 1.4824e+04, 2.2208e+04, 1.3480e+04, 3.2304e+04, 4.8640e+04,\n",
      "        8.3760e+03, 5.5776e+04,        inf,        inf, 6.2368e+04, 5.5840e+04,\n",
      "               inf, 2.7184e+04, 1.3576e+04, 4.4864e+04, 2.5940e+03, 5.2640e+04,\n",
      "               inf, 5.3720e+03, 4.2432e+04, 1.2010e+03, 7.5360e+03, 4.8672e+04,\n",
      "        6.1664e+04, 4.7392e+04, 2.4704e+04,        inf, 3.4464e+04,        inf,\n",
      "        3.9232e+04, 4.4192e+04, 1.0752e+04, 2.4752e+04, 3.9744e+04, 3.7792e+04,\n",
      "        3.2832e+04, 2.1856e+04, 1.1720e+04, 1.9696e+04, 1.4760e+04, 1.8608e+04,\n",
      "        3.6704e+04, 2.6980e+03,        inf,        inf, 2.5920e+04, 3.2608e+04,\n",
      "               inf,        inf, 3.7216e+04,        inf,        inf, 3.2896e+04,\n",
      "        6.1632e+04, 5.4432e+04, 4.9800e+03, 2.4352e+04, 1.4104e+04, 4.5472e+04,\n",
      "        1.9690e+03,        inf, 7.4200e+02, 1.9696e+04, 1.6368e+04, 5.5080e+03,\n",
      "        4.4928e+04, 3.7376e+04, 2.6128e+04, 4.7968e+04, 2.1760e+04, 5.9296e+04,\n",
      "        4.5760e+03, 1.6400e+04, 4.3456e+04, 2.2960e+04, 3.4208e+04, 5.2320e+04,\n",
      "        2.8384e+04, 4.1568e+04, 4.4608e+04, 2.2192e+04, 8.2080e+03, 3.5616e+04,\n",
      "        3.2832e+04, 6.1600e+04, 1.4912e+04, 7.1480e+03, 5.3536e+04, 4.4672e+04,\n",
      "        2.8240e+04, 5.6448e+04, 4.8576e+04,        inf, 1.8336e+04, 3.9776e+04,\n",
      "               inf,        inf, 1.6040e+04, 1.3808e+04, 5.5136e+04, 1.2576e+04,\n",
      "        4.0992e+04, 5.4848e+04, 6.7360e+03, 6.3488e+04, 2.7376e+04, 6.1216e+04,\n",
      "        4.9728e+04, 5.6864e+04,        inf, 4.9856e+04, 6.0800e+04,        inf,\n",
      "        5.8848e+04, 1.5736e+04, 4.1560e+03, 2.6544e+04, 5.5744e+04, 2.8192e+04,\n",
      "        6.5720e+03, 1.4744e+04, 4.2560e+04,        inf, 3.5552e+04, 2.9232e+04,\n",
      "        1.8544e+04, 2.6192e+04, 6.1760e+04, 2.2208e+04, 9.0960e+03,        inf,\n",
      "        2.0464e+04, 4.2624e+04, 1.1744e+04, 5.3984e+04, 2.8336e+04, 4.5152e+04,\n",
      "        2.1000e+03, 2.1776e+04, 5.4560e+04, 2.8880e+04, 6.5200e+03, 2.7408e+04,\n",
      "        1.0720e+04,        inf, 5.0272e+04, 9.4160e+03, 4.3968e+04, 4.3776e+04,\n",
      "        3.9808e+04,        inf, 5.9744e+04, 3.0432e+04,        inf, 2.1776e+04,\n",
      "        6.0280e+03,        inf, 8.1720e+03, 6.1320e+03, 9.4880e+03, 5.3500e+02,\n",
      "               inf, 3.9776e+04, 5.2512e+04,        inf, 1.4544e+04, 3.5680e+04,\n",
      "        2.9104e+04, 2.5024e+04, 5.6224e+04, 5.5680e+04, 1.7600e+04, 2.0128e+04,\n",
      "        3.2752e+04, 2.5728e+04, 8.6720e+03, 5.7100e+02, 4.4960e+04, 5.5232e+04,\n",
      "        3.3344e+04, 3.3824e+04,        inf, 2.0090e+03, 2.0820e+03, 4.8768e+04,\n",
      "        3.9080e+03, 1.5392e+04,        inf, 1.9680e+04, 2.6960e+03, 2.6144e+04,\n",
      "        6.9720e+03, 4.6848e+04, 4.4640e+04, 5.7216e+04, 2.2528e+04, 2.2820e+03,\n",
      "        2.1696e+04, 2.8496e+04, 5.1744e+04, 8.2320e+03,        inf, 2.6352e+04,\n",
      "        2.1840e+04, 6.1632e+04, 3.0064e+04, 5.1296e+04, 1.2792e+04, 3.2720e+04,\n",
      "        5.4336e+04, 2.7780e+03,        inf, 2.1020e+03, 2.9232e+04,        inf,\n",
      "        6.2400e+03, 5.6896e+04,        inf, 5.4176e+04, 1.9488e+04,        inf,\n",
      "        5.8000e+03, 2.7792e+04, 3.6704e+04,        inf,        inf, 1.8672e+04,\n",
      "               inf,        inf, 5.7888e+04, 1.4032e+04, 2.0280e+03, 3.8840e+03,\n",
      "        3.9072e+04,        inf, 6.2144e+04, 1.7584e+04, 1.2008e+04, 1.9648e+04,\n",
      "        4.6560e+03,        inf, 3.9488e+04, 1.6232e+04, 2.8976e+04, 5.9552e+04,\n",
      "        7.8920e+03, 2.4992e+04,        inf, 1.7136e+04,        inf, 8.3360e+03,\n",
      "        5.7920e+03,        inf, 5.1424e+04, 3.3120e+04, 6.1216e+04, 5.9440e+03,\n",
      "        6.6720e+03, 3.1344e+04, 5.9616e+04, 2.8112e+04, 3.7184e+04, 1.1062e+02,\n",
      "        2.1312e+04, 1.2568e+04, 1.8896e+04, 3.0016e+04, 4.1960e+03, 1.3616e+04,\n",
      "        3.6760e+03, 1.5448e+04, 1.8940e+03, 3.5296e+04, 3.5680e+04,        inf,\n",
      "        3.1872e+04, 3.2896e+04, 3.0368e+04,        inf, 2.9168e+04, 3.5648e+04,\n",
      "        1.5344e+04, 2.0160e+04, 1.8720e+04, 2.6336e+04,        inf, 1.5032e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 3.7024e+04,\n",
      "        5.7728e+04,        inf, 1.0080e+04, 3.5520e+04, 4.4448e+04, 8.0920e+03,\n",
      "        1.6976e+04, 2.8176e+04, 3.8880e+04, 1.4640e+04,        inf, 2.3660e+03,\n",
      "        4.8384e+04, 2.5424e+04,        inf, 3.1280e+04,        inf, 5.2480e+03,\n",
      "        5.9880e+03, 6.1696e+04, 5.0528e+04, 4.9280e+04, 2.6096e+04,        inf,\n",
      "        3.5968e+04,        inf, 2.5216e+04,        inf,        inf, 4.0160e+04,\n",
      "        1.4400e+04, 2.3500e+03, 4.2080e+04, 2.1216e+04, 3.1984e+04, 7.5400e+02,\n",
      "        1.5792e+04, 4.6496e+04, 6.2176e+04, 3.3184e+04, 4.1825e+02, 4.3040e+04,\n",
      "        4.1472e+04, 3.0208e+04, 2.9072e+04, 3.5456e+04, 4.1248e+04, 2.0512e+04,\n",
      "        4.6016e+04, 2.0800e+04, 1.9040e+04, 3.5808e+04, 2.8180e+03,        inf,\n",
      "        4.9080e+03, 3.1392e+04,        inf, 3.8048e+04, 1.2568e+04, 2.5680e+04,\n",
      "        4.5248e+04, 6.3104e+04, 2.7000e+03, 3.8592e+04, 1.6592e+04, 2.7680e+04,\n",
      "        2.7400e+03, 3.9296e+04, 3.9104e+04, 5.2992e+04, 7.5350e+02, 1.9104e+04,\n",
      "        4.3776e+04, 9.0080e+03, 4.0960e+04, 1.6976e+04, 3.1424e+04, 1.2224e+04,\n",
      "        1.9744e+04, 2.0608e+04, 5.1104e+04, 5.1008e+04, 2.9456e+04, 2.2719e+01,\n",
      "        2.3700e+03, 2.3008e+04, 9.8880e+03, 4.4032e+04, 2.6592e+04, 4.0320e+04,\n",
      "               inf, 6.2848e+04, 2.4020e+03,        inf, 1.1176e+04, 3.8336e+04,\n",
      "        2.0864e+04, 4.4400e+03,        inf,        inf,        inf, 1.2400e+04,\n",
      "        3.1168e+04, 2.0448e+04], device='cuda:0', dtype=torch.float16), tensor([[ 1628.,  6548.,    86.,  ...,  2868.,  7784.,  2400.],\n",
      "        [22624.,  1220.,  6904.,  ..., 20928., 16832., 11952.],\n",
      "        [36544.,  6824., 14592.,  ..., 15976., 10288.,  9904.],\n",
      "        ...,\n",
      "        [17648., 10552.,  6032.,  ...,  6552., 13792.,  8536.],\n",
      "        [10352.,  2616.,  8232.,  ...,  9032.,  1228.,  1920.],\n",
      "        [ 2716.,  4292.,  7232.,  ...,  6140.,  2990.,   668.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([ 6096., 19648., 17744.,  ..., 14448.,  7076.,  8056.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[ 6568.0000,  6756.0000,  2440.0000,  ...,  8060.0000,  7556.0000,\n",
      "          4792.0000],\n",
      "        [ 3794.0000,  7688.0000,   408.0000,  ..., 23200.0000,  2166.0000,\n",
      "          9360.0000],\n",
      "        [ 5732.0000,  5320.0000,  8384.0000,  ...,  6352.0000,  4380.0000,\n",
      "           602.0000],\n",
      "        ...,\n",
      "        [ 2510.0000,  1472.0000,   637.5000,  ...,  5680.0000,  7424.0000,\n",
      "          8528.0000],\n",
      "        [ 2716.0000,  4648.0000,  1372.0000,  ...,  2712.0000,  4260.0000,\n",
      "          3116.0000],\n",
      "        [ 2244.0000,  2276.0000, 13448.0000,  ...,  5832.0000,  4368.0000,\n",
      "          8140.0000]], device='cuda:0', dtype=torch.float16), tensor([2.2448e+04, 4.7904e+04, 2.9008e+04, 2.1680e+04, 4.1728e+04, 3.6480e+04,\n",
      "        2.9904e+04, 1.8400e+04, 3.9740e+03, 1.8352e+04, 1.4424e+04, 1.6832e+04,\n",
      "        3.2464e+04, 4.4768e+04,        inf,        inf, 5.1840e+03, 4.1760e+03,\n",
      "        2.2736e+04, 5.3248e+04, 2.3420e+03, 3.0080e+04,        inf,        inf,\n",
      "        4.5088e+04,        inf, 1.7480e+03, 2.4752e+04,        inf, 3.8460e+03,\n",
      "        7.8040e+03, 1.5048e+04, 4.2208e+04, 6.1632e+04,        inf, 2.4660e+03,\n",
      "               inf, 3.7376e+04,        inf, 2.5920e+04, 4.2656e+04, 6.0672e+04,\n",
      "        4.6624e+04, 1.7552e+04, 2.5632e+04, 3.5120e+03, 6.0608e+04, 2.2544e+04,\n",
      "        4.5960e+03, 3.7220e+03, 1.6640e+04, 3.1072e+04, 1.7140e+03, 4.2464e+04,\n",
      "               inf, 4.8832e+04, 1.5648e+04, 2.0672e+04, 1.0696e+04, 8.1200e+03,\n",
      "               inf,        inf, 4.9280e+04, 4.5080e+03, 5.9616e+04, 3.1344e+04,\n",
      "        1.9168e+04, 3.8400e+04, 2.4448e+04, 4.4128e+04, 3.4752e+04, 5.5776e+04,\n",
      "               inf, 5.8240e+04, 2.9104e+04,        inf, 4.3776e+04, 2.0048e+04,\n",
      "        6.1560e+03, 4.3584e+04, 3.9712e+04, 2.1536e+04, 1.7056e+04, 5.9328e+04,\n",
      "        9.0125e+01, 4.9504e+04, 1.0072e+04, 3.5360e+04, 5.0560e+04, 1.6816e+04,\n",
      "        1.4328e+04, 1.1928e+04, 4.2080e+04, 3.0624e+04, 1.2744e+04, 2.0048e+04,\n",
      "        2.6672e+04, 2.6800e+04, 1.2776e+04, 1.3832e+04, 1.1640e+04, 4.1760e+04,\n",
      "        7.6760e+03,        inf, 4.8640e+04, 3.5072e+04, 4.6750e+02,        inf,\n",
      "        6.5056e+04, 1.2848e+04, 6.3968e+04, 2.1088e+04, 6.4880e+03, 3.1408e+04,\n",
      "        1.0770e+03, 7.8160e+03, 2.9216e+04, 3.6256e+04, 4.3584e+04, 2.4576e+04,\n",
      "        4.4200e+03,        inf, 4.3360e+04, 5.6992e+04, 3.8752e+04, 4.2016e+04,\n",
      "        6.3680e+04, 4.1472e+04, 1.6688e+04, 5.6896e+04, 5.4280e+03, 4.0512e+04,\n",
      "        3.1184e+04, 1.8656e+04, 3.9328e+04, 1.9056e+04, 7.5320e+03, 5.2768e+04,\n",
      "               inf,        inf, 2.6848e+04, 4.9568e+04, 1.6896e+04,        inf,\n",
      "        2.7280e+04, 2.8064e+04, 2.5664e+04, 3.7536e+04, 5.6000e+04, 3.6864e+04,\n",
      "        5.8320e+03, 1.8080e+04, 2.6960e+04, 1.2280e+04, 3.7480e+03, 6.6880e+03,\n",
      "        2.2880e+04, 3.9808e+04,        inf, 4.4480e+04, 1.7424e+04, 9.8720e+03,\n",
      "        4.1280e+04, 3.8688e+04, 2.8544e+04,        inf, 5.2384e+04, 6.3616e+04,\n",
      "               inf,        inf, 2.3632e+04, 6.3232e+04, 6.5360e+03, 3.3888e+04,\n",
      "        4.5880e+03,        inf, 2.7552e+04, 3.0160e+04, 1.7888e+04, 1.1776e+04,\n",
      "               inf, 2.6848e+04, 2.6112e+04, 3.9168e+04, 5.0688e+04, 6.5280e+04,\n",
      "        2.4416e+04, 1.3704e+04, 3.1280e+04, 2.5744e+04, 3.9776e+04,        inf,\n",
      "        1.4232e+04, 1.8736e+04, 2.3584e+04, 3.0688e+04, 2.0784e+04, 3.9680e+04,\n",
      "        3.1392e+04, 4.4832e+04, 2.1872e+04, 2.3248e+04, 5.6288e+04, 2.8064e+04,\n",
      "        9.6400e+03, 4.7872e+04, 6.4352e+04,        inf, 9.9120e+03, 5.2640e+04,\n",
      "        5.5680e+04, 6.2240e+04, 2.2860e+03, 1.2032e+04, 5.0912e+04, 1.9376e+04,\n",
      "        2.9920e+04,        inf, 3.9540e+03,        inf, 3.5168e+04, 3.4944e+04,\n",
      "        5.3312e+04,        inf,        inf, 5.9264e+04, 4.3392e+04, 5.2192e+04,\n",
      "        5.0880e+04, 2.6440e+03, 4.7560e+03, 2.6224e+04, 2.9552e+04, 2.4912e+04,\n",
      "        4.2280e+03, 4.4160e+03, 6.2752e+04, 4.8704e+04, 2.4304e+04, 3.2928e+04,\n",
      "        3.4528e+04, 1.9152e+04, 5.4400e+04, 2.0416e+04, 1.1520e+03, 4.3744e+04,\n",
      "        6.1824e+04, 1.7072e+04, 7.5680e+03, 4.9824e+04, 1.3760e+04, 5.3920e+04,\n",
      "        7.5080e+03, 1.2192e+04, 4.1088e+04, 2.0304e+04, 4.7600e+03, 2.7536e+04,\n",
      "        8.6720e+03,        inf, 2.7968e+04, 3.3632e+04, 3.7824e+04, 2.3840e+04,\n",
      "        4.5952e+04,        inf, 4.5600e+04, 2.2384e+04,        inf, 2.0172e+01,\n",
      "        1.2000e+04,        inf, 1.0120e+04, 1.3490e+03, 2.2736e+04, 5.1050e+02,\n",
      "               inf, 4.2880e+04, 5.9680e+04, 4.9792e+04, 1.5192e+04, 5.2960e+04,\n",
      "        5.7920e+03, 1.9040e+04,        inf, 5.6416e+04, 2.8368e+04, 1.9488e+04,\n",
      "        1.1872e+04, 5.1424e+04, 1.0280e+04, 1.3624e+04,        inf, 5.4816e+04,\n",
      "        3.6032e+04, 3.7760e+04,        inf, 1.1990e+03, 5.0280e+03, 2.8688e+04,\n",
      "        2.5320e+03, 8.5440e+03,        inf, 1.3520e+04, 6.4920e+03, 2.6976e+04,\n",
      "        2.7300e+03, 5.8176e+04, 4.5024e+04,        inf, 4.8672e+04, 3.9264e+04,\n",
      "        1.4272e+04, 3.8848e+04, 3.6416e+04, 1.8870e+03, 4.7456e+04, 4.3360e+04,\n",
      "        2.3504e+04, 3.9968e+04, 4.1344e+04, 2.6304e+04, 2.7340e+03, 2.8384e+04,\n",
      "        4.1184e+04, 1.8528e+04, 5.8688e+04, 1.5192e+04, 1.0768e+04,        inf,\n",
      "        2.1328e+04,        inf,        inf, 5.4304e+04, 1.9104e+04,        inf,\n",
      "        1.1296e+04, 1.8656e+04, 4.2816e+04,        inf,        inf, 1.6880e+04,\n",
      "               inf,        inf, 3.9008e+04, 3.0272e+04, 8.3600e+03, 2.2320e+03,\n",
      "        6.3264e+04,        inf, 5.3888e+04, 2.2976e+04, 1.9312e+04, 8.7920e+03,\n",
      "        2.5424e+04,        inf, 4.1680e+03, 2.5320e+03, 2.5216e+04, 4.3296e+04,\n",
      "        3.2416e+04, 6.8080e+03,        inf, 4.9056e+04, 5.6992e+04, 1.1400e+04,\n",
      "        2.9072e+04, 4.2048e+04, 5.3504e+04, 1.2768e+04, 2.5936e+04, 2.4620e+03,\n",
      "        1.0576e+04, 4.0384e+04,        inf, 3.0864e+04, 3.7408e+04, 2.4576e+04,\n",
      "        2.5744e+04, 2.5216e+04, 2.2320e+03, 5.4464e+04, 2.3840e+04, 1.5120e+04,\n",
      "        2.7552e+04, 2.5488e+04, 7.5640e+03, 5.3504e+04, 2.0944e+04,        inf,\n",
      "        4.8640e+04, 5.3664e+04, 1.8096e+04,        inf, 6.0450e+02, 3.5776e+04,\n",
      "        2.6928e+04, 4.7760e+03, 3.8304e+04, 3.4368e+04, 5.5200e+04, 9.8100e+02,\n",
      "               inf,        inf, 6.4608e+04,        inf, 4.9184e+04, 1.0088e+04,\n",
      "        5.5424e+04,        inf, 8.2800e+03, 3.7408e+04, 2.6112e+04, 7.3880e+03,\n",
      "        2.7232e+04, 1.2400e+04, 3.4560e+04, 1.1064e+04, 6.2912e+04, 7.8960e+03,\n",
      "        3.5328e+04, 2.6336e+04,        inf, 4.2752e+04,        inf, 5.1640e+03,\n",
      "        2.2624e+04, 4.0768e+04, 3.1472e+04, 5.9424e+04, 2.1296e+04, 6.1408e+04,\n",
      "        2.9408e+04,        inf, 3.7504e+04,        inf,        inf, 4.3296e+04,\n",
      "        4.7328e+04, 9.9920e+03, 5.4592e+04, 2.7152e+04, 2.5616e+04, 6.2640e+03,\n",
      "        1.7520e+04, 4.6272e+04,        inf, 8.6240e+03, 1.0576e+04, 7.1720e+03,\n",
      "        3.0096e+04, 2.2064e+04, 3.5968e+04, 2.8608e+04, 2.4192e+04, 2.1300e+03,\n",
      "        4.8224e+04, 2.2304e+04, 8.7120e+03, 2.4976e+04, 1.9344e+04, 4.9440e+04,\n",
      "        1.4824e+04, 3.5552e+04,        inf, 2.1360e+04, 8.0000e+03, 1.6144e+04,\n",
      "        2.6160e+04, 2.3696e+04, 3.2528e+04, 2.1568e+04, 2.6736e+04, 1.6688e+04,\n",
      "        5.3040e+03, 5.0272e+04, 3.3184e+04, 5.4112e+04, 1.7712e+04, 9.0320e+03,\n",
      "        2.2016e+04, 2.7712e+04, 2.9824e+04, 2.0688e+04, 2.7152e+04, 9.6880e+03,\n",
      "        5.0688e+04, 1.7440e+04, 3.6992e+04, 4.7808e+04, 3.9552e+04, 4.3296e+04,\n",
      "        1.5704e+04, 5.7640e+03, 1.3824e+04, 3.7952e+04, 3.0784e+04, 3.0032e+04,\n",
      "               inf, 4.1024e+04, 8.9050e+02, 3.3376e+04, 2.0320e+04, 1.7888e+04,\n",
      "        1.4904e+04, 1.4480e+04,        inf, 4.0160e+04, 4.9312e+04, 2.0800e+04,\n",
      "        3.0368e+04, 4.8000e+04], device='cuda:0', dtype=torch.float16), tensor([40960.0000,  8184.0000,  9872.0000,  6368.0000, 29952.0000, 30960.0000,\n",
      "         3344.0000, 10200.0000, 17424.0000, 16688.0000,  4032.0000, 13552.0000,\n",
      "        32896.0000, 28224.0000, 26112.0000, 47488.0000,  8240.0000,   698.0000,\n",
      "         4264.0000, 22768.0000, 21312.0000, 37120.0000, 21040.0000, 38912.0000,\n",
      "        32768.0000,  3444.0000, 27504.0000, 19920.0000, 18608.0000, 28816.0000,\n",
      "        14336.0000,  6744.0000, 12976.0000, 28352.0000,  8264.0000,  6540.0000,\n",
      "        60704.0000,  7124.0000, 26176.0000,   649.0000, 20224.0000, 23232.0000,\n",
      "        10368.0000, 27312.0000,  3332.0000, 37440.0000, 22496.0000, 17856.0000,\n",
      "        14344.0000,  6720.0000,  8200.0000,  7516.0000,  4256.0000,  9584.0000,\n",
      "        48896.0000, 63360.0000, 38752.0000, 11376.0000, 46112.0000, 10320.0000,\n",
      "        19936.0000, 36512.0000, 12152.0000,   602.0000, 24544.0000,  6260.0000,\n",
      "        24448.0000, 11488.0000, 16496.0000,  5064.0000,  4228.0000,  5396.0000,\n",
      "         5880.0000, 14544.0000,  5608.0000, 48768.0000, 30608.0000,   220.5000,\n",
      "        11872.0000, 32672.0000,  7948.0000, 10816.0000, 22960.0000,  5544.0000,\n",
      "         2684.0000,  8720.0000,  7888.0000, 19840.0000, 12720.0000, 32752.0000,\n",
      "         5640.0000,  7264.0000, 11944.0000, 35712.0000, 19584.0000,  2700.0000,\n",
      "        25344.0000,  6836.0000,  9904.0000, 16088.0000,  2534.0000, 10448.0000,\n",
      "         1410.0000, 50688.0000, 19360.0000,  4620.0000,  2874.0000, 62080.0000,\n",
      "        14304.0000,  2056.0000,        inf, 13960.0000,  6904.0000,  7484.0000,\n",
      "         8056.0000,  5168.0000,  8160.0000,  7060.0000,  5904.0000, 23792.0000,\n",
      "        27328.0000, 57600.0000, 14832.0000, 27712.0000, 49344.0000,  3266.0000,\n",
      "        37408.0000, 11776.0000, 13864.0000, 37504.0000, 25440.0000, 29376.0000,\n",
      "        48032.0000, 21184.0000,  2524.0000,   764.0000,  3682.0000, 20624.0000,\n",
      "        10992.0000, 14688.0000,  6724.0000,        inf, 52032.0000, 51872.0000,\n",
      "         6800.0000,  5936.0000,  9744.0000, 16352.0000,  9144.0000, 12976.0000,\n",
      "        28112.0000,  2388.0000,  2650.0000,  6912.0000,  6976.0000,  2422.0000,\n",
      "        19920.0000,   367.2500, 12120.0000, 17504.0000,  3478.0000, 17952.0000,\n",
      "        20704.0000, 48992.0000, 21728.0000, 15920.0000, 60384.0000, 17296.0000,\n",
      "         3822.0000, 56288.0000,  3734.0000, 17344.0000,  2630.0000, 29440.0000,\n",
      "        15840.0000, 42464.0000,  2800.0000, 10000.0000, 10272.0000,  2612.0000,\n",
      "        34656.0000, 19520.0000, 21952.0000, 24160.0000,  4452.0000, 14216.0000,\n",
      "         2310.0000,  3232.0000, 28960.0000,  4920.0000,  4752.0000, 19808.0000,\n",
      "        11152.0000, 32736.0000, 32128.0000, 20176.0000,  9536.0000, 27600.0000,\n",
      "        26064.0000, 60832.0000,  8504.0000,  8816.0000, 26192.0000, 44192.0000,\n",
      "        10768.0000, 44032.0000, 29568.0000, 48736.0000, 27056.0000, 24512.0000,\n",
      "        27520.0000, 53312.0000,   195.2500, 23264.0000, 44032.0000, 24272.0000,\n",
      "        33152.0000,        inf, 18560.0000, 40864.0000,  8784.0000, 31536.0000,\n",
      "        27184.0000, 57056.0000, 30992.0000, 15040.0000, 38208.0000, 50976.0000,\n",
      "        31856.0000,  5280.0000,  6732.0000, 27696.0000, 37376.0000, 12480.0000,\n",
      "          920.0000,  1468.0000, 31536.0000, 12656.0000, 43232.0000,  1038.0000,\n",
      "         5504.0000, 24848.0000, 29152.0000,   797.0000,  2902.0000, 21824.0000,\n",
      "         6936.0000,   847.0000,  8296.0000, 30688.0000, 16896.0000, 16208.0000,\n",
      "        10168.0000, 25584.0000, 13680.0000, 11816.0000,  2284.0000,  7080.0000,\n",
      "        43296.0000, 21904.0000, 17040.0000, 18992.0000, 29104.0000, 53632.0000,\n",
      "        27840.0000, 19248.0000,  5336.0000, 26416.0000, 12880.0000, 42656.0000,\n",
      "         2560.0000, 17680.0000, 19808.0000, 12264.0000,  9152.0000, 11992.0000,\n",
      "        33376.0000, 44992.0000, 43392.0000, 22816.0000, 30256.0000, 33664.0000,\n",
      "        15536.0000,  1027.0000, 14776.0000, 64352.0000, 21216.0000,  9256.0000,\n",
      "        37216.0000, 10240.0000, 33312.0000, 20576.0000, 33984.0000, 56128.0000,\n",
      "        18224.0000, 10720.0000, 24464.0000, 14688.0000, 29072.0000,  4712.0000,\n",
      "         9744.0000, 22144.0000, 47456.0000, 23488.0000, 22864.0000, 10896.0000,\n",
      "         6220.0000,        inf, 28448.0000, 39584.0000,  8280.0000,  1960.0000,\n",
      "        12320.0000, 42272.0000, 52512.0000,  9216.0000,  7512.0000, 17056.0000,\n",
      "        20112.0000,  3476.0000, 11472.0000, 21856.0000, 35232.0000, 23472.0000,\n",
      "         6512.0000, 29008.0000, 24656.0000, 16832.0000,  5292.0000,  7088.0000,\n",
      "        31024.0000, 63616.0000, 21296.0000, 26192.0000, 10400.0000, 43936.0000,\n",
      "        24000.0000, 10488.0000, 12856.0000,  8060.0000,        inf,   991.0000,\n",
      "         9488.0000, 30240.0000, 25600.0000, 40224.0000, 13696.0000, 21264.0000,\n",
      "        36800.0000, 61600.0000, 60864.0000,  5584.0000,  4924.0000, 25408.0000,\n",
      "         5204.0000, 52000.0000, 24272.0000, 20112.0000, 50400.0000, 34752.0000,\n",
      "         7572.0000, 23616.0000,        inf,  3596.0000, 19168.0000,  2758.0000,\n",
      "        31296.0000, 45792.0000, 39616.0000, 24368.0000,  6104.0000,  9640.0000,\n",
      "        11752.0000, 13920.0000,        inf, 36352.0000, 26288.0000,  9848.0000,\n",
      "        19120.0000, 23360.0000, 12856.0000, 55168.0000,   339.5000,  4400.0000,\n",
      "        10552.0000, 14032.0000, 10368.0000,  8384.0000,  6412.0000,        inf,\n",
      "        16168.0000, 34112.0000, 15424.0000,        inf, 14136.0000, 19168.0000,\n",
      "         5096.0000, 15456.0000, 11088.0000,  8000.0000,        inf, 10744.0000,\n",
      "               inf, 61984.0000,        inf,        inf,        inf, 20224.0000,\n",
      "        40896.0000, 44608.0000,  8392.0000, 57888.0000, 60256.0000,  9192.0000,\n",
      "        25744.0000, 18960.0000, 29536.0000, 15432.0000,        inf,  2686.0000,\n",
      "        41216.0000,   986.0000,        inf,  5044.0000, 46336.0000,  6528.0000,\n",
      "        23264.0000,  8360.0000,  5872.0000, 25088.0000, 11536.0000, 19840.0000,\n",
      "        28912.0000, 46048.0000, 26848.0000,        inf, 11960.0000, 25760.0000,\n",
      "         9520.0000, 13784.0000, 14104.0000, 20000.0000, 15912.0000, 11720.0000,\n",
      "        14272.0000, 39872.0000, 32704.0000, 10552.0000,  7700.0000, 15176.0000,\n",
      "        38688.0000, 30448.0000, 16848.0000, 51392.0000, 26816.0000, 17408.0000,\n",
      "        50784.0000, 50240.0000,  2572.0000, 54400.0000,  6368.0000,  7288.0000,\n",
      "         5196.0000,  3826.0000, 11816.0000, 26352.0000, 20112.0000, 23264.0000,\n",
      "        28528.0000, 49536.0000,  9728.0000, 32240.0000,  5884.0000,  1126.0000,\n",
      "        10976.0000, 33152.0000, 51296.0000, 44800.0000, 11320.0000, 12432.0000,\n",
      "        45248.0000,  3720.0000, 34944.0000, 27840.0000, 18064.0000,   192.7500,\n",
      "        14864.0000, 18752.0000, 58368.0000, 33728.0000, 18000.0000,  8664.0000,\n",
      "          751.5000,  1600.0000,  3178.0000, 56224.0000,  9568.0000, 37568.0000,\n",
      "        49504.0000,        inf,    76.6250, 54432.0000, 11632.0000, 38784.0000,\n",
      "        19552.0000,  9432.0000, 39392.0000,        inf, 45024.0000, 21936.0000,\n",
      "        32832.0000, 26944.0000], device='cuda:0', dtype=torch.float16), tensor([9.6160e+03, 6.2016e+04, 2.8432e+04,        inf, 5.4656e+04, 4.6336e+04,\n",
      "        3.5424e+04, 6.3240e+03, 1.5344e+04, 1.1010e+03, 4.2960e+03, 2.9648e+04,\n",
      "        2.1696e+04, 3.1216e+04,        inf,        inf, 4.7400e+03, 2.1344e+04,\n",
      "        3.1936e+04, 3.8240e+04, 5.2440e+03, 4.1600e+04,        inf,        inf,\n",
      "        3.6512e+04, 5.2928e+04, 3.4860e+03, 1.9424e+04,        inf, 5.8240e+03,\n",
      "        2.6224e+04, 2.2880e+03, 8.2080e+03, 5.7120e+04, 5.2864e+04, 7.4600e+03,\n",
      "               inf, 4.5952e+04,        inf,        inf,        inf,        inf,\n",
      "               inf, 3.3600e+04, 2.1824e+04, 9.8080e+03, 3.8784e+04, 2.3424e+04,\n",
      "        1.5408e+04, 7.6200e+02, 1.7888e+04, 4.8128e+04, 1.4648e+04, 6.3712e+04,\n",
      "               inf,        inf, 2.6192e+04, 2.7728e+04, 2.1008e+04, 5.6850e+02,\n",
      "               inf, 6.3360e+04, 3.4784e+04, 1.5080e+04, 5.8976e+04, 4.9920e+04,\n",
      "        4.2240e+04, 3.0432e+04, 3.6736e+04, 2.6640e+04, 2.8176e+04,        inf,\n",
      "        4.5536e+04,        inf, 3.1232e+04,        inf, 4.4384e+04, 3.3152e+04,\n",
      "        2.7600e+04, 4.6912e+04, 1.8016e+04, 1.2968e+04, 2.8192e+04, 6.3584e+04,\n",
      "        1.1296e+04, 3.4912e+04, 1.9120e+04, 5.9264e+04,        inf, 3.2256e+04,\n",
      "        1.4056e+04, 2.8608e+04, 3.7376e+04,        inf, 8.4640e+03, 1.6416e+04,\n",
      "        2.7792e+04, 2.9840e+04, 4.4864e+04, 3.9136e+04, 1.9552e+04, 1.5120e+04,\n",
      "        1.0480e+04,        inf, 6.3328e+04, 2.9248e+04, 2.4620e+03,        inf,\n",
      "        6.0032e+04, 5.3400e+03,        inf, 1.1632e+04, 1.6570e+03, 2.6816e+04,\n",
      "        2.1088e+04, 1.5336e+04, 2.4192e+04, 1.4680e+04, 3.6320e+04, 4.1312e+04,\n",
      "        9.4640e+03, 5.8016e+04,        inf,        inf, 6.2368e+04, 4.9024e+04,\n",
      "               inf, 2.6016e+04, 1.7664e+04, 4.7968e+04, 1.9800e+03, 5.7376e+04,\n",
      "               inf, 5.3960e+03, 3.7120e+04, 4.3680e+03, 4.0740e+03, 4.2752e+04,\n",
      "        6.5152e+04, 4.6432e+04, 2.6064e+04,        inf, 3.5232e+04,        inf,\n",
      "        4.2080e+04, 4.7616e+04, 7.6520e+03, 2.3776e+04, 3.1760e+04, 3.5872e+04,\n",
      "        4.0640e+04, 2.4640e+04, 1.2224e+04, 1.1304e+04, 1.5288e+04, 1.8352e+04,\n",
      "        3.6160e+04, 5.7920e+03,        inf,        inf, 3.0608e+04, 2.5264e+04,\n",
      "               inf,        inf, 3.4912e+04,        inf,        inf, 3.7920e+04,\n",
      "        6.1376e+04, 5.8336e+04, 4.2760e+03, 2.6800e+04, 6.1640e+03, 4.4576e+04,\n",
      "        6.0750e+02,        inf, 1.0640e+03, 2.2288e+04, 1.3456e+04, 1.0024e+04,\n",
      "        4.8992e+04, 4.4608e+04, 2.9200e+04, 4.4032e+04, 2.1120e+04, 5.6992e+04,\n",
      "        5.9500e+02, 1.5368e+04, 4.6208e+04, 2.1856e+04, 2.9248e+04, 5.2384e+04,\n",
      "        2.5696e+04, 4.0064e+04, 4.4288e+04, 1.9952e+04, 9.3840e+03, 3.0880e+04,\n",
      "        2.9680e+04, 6.0768e+04, 1.4224e+04, 9.2880e+03, 6.3136e+04, 4.9120e+04,\n",
      "        3.2080e+04, 5.7760e+04, 5.4176e+04,        inf, 1.7840e+04, 3.4816e+04,\n",
      "               inf,        inf, 1.4240e+04, 1.1440e+04, 5.5808e+04, 8.9760e+03,\n",
      "        4.3968e+04, 5.8688e+04, 4.0120e+03, 6.3392e+04, 2.7024e+04, 6.1344e+04,\n",
      "        4.9088e+04, 5.8336e+04,        inf, 4.4992e+04, 5.7280e+04,        inf,\n",
      "        6.1792e+04, 1.5264e+04, 9.3400e+02, 2.1728e+04, 5.3760e+04, 2.9072e+04,\n",
      "        5.3360e+03, 9.5120e+03, 4.7808e+04,        inf, 3.4464e+04, 3.2768e+04,\n",
      "        1.9472e+04, 2.7936e+04, 5.8592e+04, 1.8576e+04, 3.8880e+03,        inf,\n",
      "        2.5360e+04, 4.1312e+04, 9.1200e+03, 5.6704e+04, 2.7792e+04, 4.1216e+04,\n",
      "        2.1440e+03, 2.1488e+04, 5.7024e+04, 2.9696e+04, 8.4080e+03, 3.0672e+04,\n",
      "        1.9872e+04,        inf, 5.3824e+04, 1.2088e+04, 4.5184e+04, 3.8656e+04,\n",
      "        4.2208e+04,        inf, 6.0576e+04, 3.2560e+04,        inf, 1.9104e+04,\n",
      "        1.5736e+04,        inf, 1.1976e+04, 6.9440e+03, 6.6440e+03, 7.0480e+03,\n",
      "               inf, 4.4288e+04, 5.4720e+04, 6.3808e+04, 1.5048e+04, 2.9648e+04,\n",
      "        2.5792e+04, 2.5344e+04, 5.9744e+04, 5.7408e+04, 1.8352e+04, 1.4664e+04,\n",
      "        3.5936e+04, 2.5840e+04, 9.0480e+03, 6.6040e+03, 4.8288e+04, 5.5520e+04,\n",
      "        3.1296e+04, 3.1312e+04,        inf, 4.8160e+03, 4.7920e+03, 5.1168e+04,\n",
      "        3.7240e+03, 1.9104e+04,        inf, 2.5008e+04, 4.1920e+03, 2.4544e+04,\n",
      "        5.9880e+03, 4.6880e+04, 4.5568e+04, 5.2864e+04, 2.0448e+04, 5.1520e+03,\n",
      "        2.0976e+04, 3.2048e+04, 4.7392e+04, 1.4224e+04, 6.4864e+04, 2.4080e+04,\n",
      "        2.1456e+04, 5.4592e+04, 2.2544e+04, 5.1744e+04, 6.2960e+03, 3.0304e+04,\n",
      "        5.4112e+04, 4.7680e+03, 5.9584e+04, 8.4960e+03, 3.1856e+04,        inf,\n",
      "        4.5920e+03, 5.5392e+04,        inf, 5.2096e+04, 2.4608e+04,        inf,\n",
      "        5.1120e+03, 3.6032e+04, 3.6864e+04,        inf,        inf, 1.2400e+04,\n",
      "               inf,        inf, 6.1280e+04, 2.3088e+04, 4.4360e+03, 5.1040e+03,\n",
      "        3.6864e+04,        inf, 6.1056e+04, 1.4672e+04, 1.0288e+04, 1.7088e+04,\n",
      "        4.9200e+03,        inf, 3.4336e+04, 1.4608e+04, 3.3760e+04,        inf,\n",
      "        9.1920e+03, 2.3168e+04,        inf, 2.4240e+04,        inf, 8.9040e+03,\n",
      "        4.2480e+03,        inf, 5.5744e+04, 3.0112e+04, 5.6992e+04, 5.7360e+03,\n",
      "        1.2848e+04, 3.6320e+04, 5.8272e+04, 2.7792e+04, 3.5616e+04, 1.3980e+03,\n",
      "        2.5456e+04, 1.3528e+04, 1.7280e+04, 2.7584e+04, 5.5320e+03, 8.8240e+03,\n",
      "        1.2784e+04, 1.2040e+04, 8.5760e+03, 4.1952e+04, 3.8144e+04,        inf,\n",
      "        3.4080e+04, 2.8160e+04, 3.2256e+04,        inf, 2.3712e+04, 3.2608e+04,\n",
      "        8.8880e+03, 1.4720e+04, 1.4912e+04, 2.7392e+04,        inf, 1.4472e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 3.7856e+04,\n",
      "        6.0576e+04,        inf, 1.0824e+04, 3.9808e+04, 4.3616e+04, 1.2432e+04,\n",
      "        2.0240e+04, 2.8240e+04, 4.2400e+04, 1.6672e+04,        inf, 4.4240e+03,\n",
      "        5.1232e+04, 2.8496e+04,        inf, 3.2608e+04,        inf, 5.9120e+03,\n",
      "        6.0480e+03, 5.6896e+04, 4.7328e+04, 5.1040e+04, 2.7600e+04,        inf,\n",
      "        3.3952e+04,        inf, 2.3936e+04,        inf,        inf, 4.1056e+04,\n",
      "        1.1744e+04, 1.9562e+02, 4.1408e+04, 1.5896e+04, 3.1472e+04, 2.3260e+03,\n",
      "        1.2008e+04, 4.6624e+04, 6.4704e+04, 2.4576e+04, 3.9312e+01, 4.2464e+04,\n",
      "        3.8944e+04, 3.4912e+04, 2.9376e+04, 3.8784e+04, 4.2976e+04, 1.6072e+04,\n",
      "        5.0176e+04, 2.3984e+04, 2.4608e+04, 3.9104e+04, 3.0000e+03, 6.4864e+04,\n",
      "        5.8680e+03, 2.8624e+04,        inf, 3.4048e+04, 1.1000e+04, 2.4960e+04,\n",
      "        4.4992e+04, 6.4992e+04, 3.0400e+03, 3.9712e+04, 1.9600e+04, 2.7136e+04,\n",
      "        6.1520e+03, 4.6048e+04, 3.7024e+04, 5.4624e+04, 5.9280e+03, 1.8528e+04,\n",
      "        4.5568e+04, 1.5184e+04, 3.9072e+04, 1.5992e+04, 3.3504e+04, 1.1304e+04,\n",
      "        2.6544e+04, 1.7440e+04, 4.9920e+04, 4.5248e+04, 3.3344e+04, 9.5450e+02,\n",
      "        2.3820e+03, 1.9648e+04, 1.1072e+04, 4.8448e+04, 2.5840e+04, 3.2256e+04,\n",
      "               inf,        inf, 2.4020e+03,        inf, 9.0880e+03, 3.7472e+04,\n",
      "        2.0336e+04, 7.6080e+03,        inf,        inf,        inf, 1.3184e+04,\n",
      "        2.4672e+04, 2.0096e+04], device='cuda:0', dtype=torch.float16), tensor([41504.0000, 22272.0000, 12960.0000, 12032.0000, 14968.0000, 42272.0000,\n",
      "          568.0000,  8296.0000, 19360.0000, 18640.0000,  2626.0000, 12712.0000,\n",
      "        32896.0000, 19152.0000, 16008.0000, 47872.0000,  8432.0000,  2190.0000,\n",
      "         7724.0000, 24896.0000, 20416.0000, 34464.0000,  8712.0000,        inf,\n",
      "        31776.0000,  5728.0000, 25808.0000, 26352.0000,  8544.0000, 28672.0000,\n",
      "        11976.0000,  2944.0000, 15192.0000, 19312.0000,  3806.0000,  6216.0000,\n",
      "               inf,  7988.0000, 47328.0000,  6856.0000,  8328.0000, 28496.0000,\n",
      "        22480.0000, 20368.0000,  1802.0000, 43808.0000, 34496.0000, 16576.0000,\n",
      "        11808.0000,  8240.0000,  6504.0000,  2852.0000,  3584.0000,  9496.0000,\n",
      "               inf, 57792.0000, 31440.0000,  2966.0000, 43776.0000,  8488.0000,\n",
      "        26320.0000, 41632.0000, 13488.0000,  5748.0000,  7516.0000,  4784.0000,\n",
      "        19984.0000, 22800.0000, 22704.0000, 11704.0000,  2678.0000,   357.2500,\n",
      "         4932.0000,  9624.0000,  3746.0000, 53280.0000, 33888.0000,  5880.0000,\n",
      "        10488.0000, 38016.0000, 11576.0000,  6996.0000, 10504.0000, 25840.0000,\n",
      "         6116.0000,  6428.0000,   284.5000, 29200.0000, 17952.0000, 29136.0000,\n",
      "         1390.0000,   601.0000, 10016.0000, 35040.0000, 17600.0000,  1415.0000,\n",
      "        24528.0000,  3698.0000,  7400.0000, 10360.0000,  2412.0000,  6528.0000,\n",
      "         3540.0000, 48768.0000, 11024.0000,  9632.0000,  1542.0000,        inf,\n",
      "        23024.0000,   554.5000,        inf, 13040.0000,  6596.0000,  7884.0000,\n",
      "         2050.0000,  2878.0000,  7200.0000,  7072.0000,  3940.0000, 26560.0000,\n",
      "        30384.0000, 51136.0000, 29168.0000, 31952.0000, 40320.0000,  5404.0000,\n",
      "        23712.0000,  9824.0000, 12536.0000, 41152.0000, 25216.0000, 32224.0000,\n",
      "        54688.0000, 21280.0000,  1738.0000,  2380.0000,   743.0000,  4260.0000,\n",
      "        12376.0000,  1470.0000,  3420.0000,        inf, 51744.0000, 27232.0000,\n",
      "        19040.0000, 10384.0000,  7320.0000, 19040.0000, 12016.0000, 27536.0000,\n",
      "        28672.0000,  1001.0000,  3156.0000,  3506.0000,  5492.0000,  3792.0000,\n",
      "        25040.0000,  1338.0000, 15792.0000,  1967.0000, 10376.0000, 30208.0000,\n",
      "        47264.0000, 39360.0000, 22736.0000, 18528.0000,        inf, 26864.0000,\n",
      "         9144.0000,        inf,  4300.0000, 15792.0000,  3536.0000, 26928.0000,\n",
      "        17536.0000, 58464.0000,  1282.0000,  7028.0000, 14928.0000,  3034.0000,\n",
      "        32144.0000, 13184.0000, 20672.0000, 31776.0000,  8224.0000, 22784.0000,\n",
      "         3824.0000,  3910.0000, 20816.0000,  1037.0000, 11736.0000, 27120.0000,\n",
      "        18400.0000, 20944.0000, 35136.0000, 17696.0000, 10208.0000, 37952.0000,\n",
      "        39008.0000, 52512.0000,  4280.0000,  7008.0000, 26704.0000, 62368.0000,\n",
      "        17776.0000, 52416.0000, 43200.0000, 47680.0000, 24976.0000, 22224.0000,\n",
      "        32864.0000, 33600.0000,  2010.0000, 26320.0000, 35808.0000, 25488.0000,\n",
      "        43808.0000,        inf, 15640.0000, 30240.0000,  1487.0000, 35296.0000,\n",
      "        36640.0000, 53888.0000, 38272.0000, 10376.0000, 46752.0000, 64000.0000,\n",
      "        22272.0000,  2760.0000,  6332.0000, 32896.0000, 23712.0000,  7004.0000,\n",
      "          241.2500,  3084.0000, 21248.0000, 18768.0000, 45632.0000,  6300.0000,\n",
      "         2468.0000, 22608.0000, 14376.0000,  1550.0000,   729.0000, 26960.0000,\n",
      "         3436.0000,   936.5000,  8288.0000, 23696.0000, 13896.0000,  8784.0000,\n",
      "        10920.0000, 32896.0000, 13816.0000, 24096.0000,  4428.0000,   302.5000,\n",
      "        39488.0000,   908.5000, 16560.0000, 22400.0000, 31856.0000, 52320.0000,\n",
      "        32928.0000,   903.0000,  6024.0000, 21840.0000, 10288.0000, 41184.0000,\n",
      "         1431.0000,  2396.0000, 23456.0000, 13480.0000,  8432.0000,  8600.0000,\n",
      "        37984.0000, 44320.0000, 56832.0000, 18560.0000, 35968.0000, 41888.0000,\n",
      "        21152.0000,  2264.0000,  2518.0000, 57408.0000, 24832.0000, 12408.0000,\n",
      "        37312.0000,  6056.0000, 28592.0000, 22288.0000, 32064.0000, 46368.0000,\n",
      "        25456.0000,  8600.0000, 33120.0000, 13912.0000, 30160.0000,  8092.0000,\n",
      "        10064.0000, 27616.0000, 54048.0000, 25824.0000, 23264.0000, 16224.0000,\n",
      "         6256.0000,        inf, 39072.0000, 42048.0000, 11200.0000,   965.5000,\n",
      "        12992.0000, 45056.0000,        inf,  6140.0000,  6620.0000,  9576.0000,\n",
      "        22144.0000, 14192.0000,  9280.0000,  4984.0000, 28368.0000, 22944.0000,\n",
      "        18368.0000, 29616.0000, 57696.0000, 14448.0000,  7584.0000, 16032.0000,\n",
      "        32480.0000,        inf, 36352.0000, 15880.0000, 10184.0000, 23232.0000,\n",
      "        21696.0000,  2322.0000,  9056.0000, 46080.0000,        inf,  3334.0000,\n",
      "         8736.0000, 34176.0000, 10504.0000, 38432.0000, 12384.0000, 19408.0000,\n",
      "        40000.0000, 38976.0000, 61856.0000,  8888.0000,  2776.0000, 30944.0000,\n",
      "         6828.0000, 64896.0000, 17056.0000, 15480.0000, 49344.0000, 20736.0000,\n",
      "         5504.0000, 18832.0000,        inf,  2194.0000,  1614.0000,  3092.0000,\n",
      "        32368.0000, 61184.0000, 41248.0000, 25600.0000, 23744.0000,  6624.0000,\n",
      "        11992.0000, 20704.0000,        inf, 43072.0000, 29312.0000, 11720.0000,\n",
      "        11408.0000, 24656.0000, 21344.0000, 56960.0000,   510.0000,   899.0000,\n",
      "         7676.0000, 12920.0000, 12592.0000,  4496.0000,  9264.0000,        inf,\n",
      "          857.5000, 41312.0000, 25760.0000,        inf,  8952.0000, 21984.0000,\n",
      "          293.0000, 18688.0000, 14048.0000, 13712.0000, 55104.0000,  8568.0000,\n",
      "               inf,        inf,        inf,        inf,        inf, 19120.0000,\n",
      "        50656.0000, 53472.0000,  8552.0000, 65216.0000, 62016.0000,  8368.0000,\n",
      "        25904.0000, 24704.0000, 36800.0000, 13776.0000,        inf,  1710.0000,\n",
      "        27728.0000,  4600.0000, 61792.0000, 12608.0000, 53696.0000,  5516.0000,\n",
      "        21488.0000, 31472.0000,  7856.0000, 31024.0000,  1288.0000, 16000.0000,\n",
      "        26256.0000, 24496.0000, 21152.0000, 53024.0000, 43232.0000, 25760.0000,\n",
      "         8984.0000, 12640.0000, 15992.0000, 15304.0000, 17824.0000, 10656.0000,\n",
      "        15016.0000, 46432.0000, 33280.0000, 13816.0000,  6772.0000, 12120.0000,\n",
      "        41088.0000, 40256.0000, 15560.0000, 46400.0000, 22080.0000, 16688.0000,\n",
      "        48992.0000, 51168.0000,   753.0000, 57984.0000,  6216.0000, 19344.0000,\n",
      "         5460.0000,   711.0000, 21632.0000, 31696.0000, 21968.0000, 25584.0000,\n",
      "        24800.0000, 50080.0000,  8744.0000, 23888.0000,  9336.0000,  4344.0000,\n",
      "        11760.0000, 32144.0000, 58464.0000, 36352.0000, 12232.0000, 11448.0000,\n",
      "        52128.0000,  3204.0000, 47936.0000, 25712.0000, 12920.0000,   443.2500,\n",
      "        15032.0000, 20272.0000, 52960.0000, 27056.0000, 19376.0000,  9520.0000,\n",
      "         3550.0000,  2102.0000,  6428.0000, 48960.0000, 14024.0000, 29584.0000,\n",
      "        48224.0000, 59328.0000,   892.0000, 52160.0000,  7244.0000, 51776.0000,\n",
      "        17520.0000,  6060.0000, 15944.0000,        inf, 59744.0000, 23104.0000,\n",
      "        30720.0000, 19280.0000], device='cuda:0', dtype=torch.float16), tensor([ 9152.0000, 64192.0000, 28576.0000,        inf, 56672.0000, 47328.0000,\n",
      "        36960.0000,  2432.0000, 15496.0000,  1508.0000,  4248.0000, 31136.0000,\n",
      "        22608.0000, 29136.0000,        inf,        inf,  4392.0000, 20416.0000,\n",
      "        32480.0000, 37984.0000,  3152.0000, 42880.0000,        inf,        inf,\n",
      "        33760.0000, 52672.0000,  3452.0000, 24256.0000,        inf,  5492.0000,\n",
      "        25952.0000,   915.5000,  7468.0000, 59488.0000, 55072.0000,  9968.0000,\n",
      "               inf, 47488.0000,        inf,        inf, 64096.0000,        inf,\n",
      "               inf, 34176.0000, 25888.0000, 11552.0000, 38624.0000, 21904.0000,\n",
      "        17232.0000,  3700.0000, 16864.0000, 46400.0000, 14024.0000, 62912.0000,\n",
      "               inf,        inf, 28000.0000, 31712.0000, 19792.0000,  3886.0000,\n",
      "               inf,        inf, 31504.0000, 17344.0000, 60128.0000, 50496.0000,\n",
      "        41312.0000, 30336.0000, 36544.0000, 29424.0000, 28640.0000,        inf,\n",
      "        47584.0000,        inf, 30592.0000,        inf, 44800.0000, 33408.0000,\n",
      "        24960.0000, 50816.0000, 21824.0000, 10856.0000, 27952.0000,        inf,\n",
      "         9328.0000, 35808.0000, 22544.0000, 61024.0000,        inf, 31376.0000,\n",
      "        10456.0000, 28768.0000, 37088.0000, 64832.0000,  6664.0000, 15184.0000,\n",
      "        30304.0000, 29792.0000, 46784.0000, 42528.0000, 17920.0000, 14880.0000,\n",
      "        12896.0000,        inf,        inf, 30720.0000,   890.0000,        inf,\n",
      "        59296.0000,  4228.0000,        inf, 10864.0000,  4232.0000, 28352.0000,\n",
      "        21088.0000, 12776.0000, 23088.0000, 17760.0000, 35744.0000, 45184.0000,\n",
      "        13800.0000, 60352.0000,        inf,        inf, 63744.0000, 52864.0000,\n",
      "               inf, 26592.0000, 20928.0000, 49440.0000,   203.0000, 56512.0000,\n",
      "               inf,  5876.0000, 39200.0000,  4900.0000,  4904.0000, 43584.0000,\n",
      "               inf, 48480.0000, 24016.0000,        inf, 37792.0000,        inf,\n",
      "        42336.0000, 44800.0000,  8616.0000, 23568.0000, 32864.0000, 35904.0000,\n",
      "        40384.0000, 24752.0000, 11608.0000, 12760.0000, 16080.0000, 19056.0000,\n",
      "        36416.0000,  6004.0000,        inf,        inf, 29552.0000, 27024.0000,\n",
      "               inf,        inf, 34528.0000,        inf,        inf, 38240.0000,\n",
      "        62016.0000, 58400.0000,  5988.0000, 29616.0000,  9592.0000, 44576.0000,\n",
      "         1410.0000,        inf,   304.5000, 19424.0000, 11640.0000, 11080.0000,\n",
      "        54624.0000, 44416.0000, 30336.0000, 45600.0000, 20864.0000, 58496.0000,\n",
      "         1700.0000, 15512.0000, 48320.0000, 22416.0000, 33088.0000, 53248.0000,\n",
      "        27504.0000, 39200.0000, 46272.0000, 20512.0000, 10312.0000, 31120.0000,\n",
      "        31904.0000, 63104.0000, 14360.0000,  7976.0000, 61792.0000, 49856.0000,\n",
      "        33120.0000, 58688.0000, 54080.0000,        inf, 17216.0000, 38368.0000,\n",
      "               inf,        inf, 12928.0000,  9480.0000, 55648.0000, 10888.0000,\n",
      "        41952.0000, 58688.0000,  1853.0000, 62144.0000, 26656.0000, 64096.0000,\n",
      "        49696.0000, 56448.0000,        inf, 45824.0000, 55392.0000,        inf,\n",
      "        62048.0000, 16320.0000,   961.5000, 21008.0000, 53792.0000, 30320.0000,\n",
      "         4968.0000, 10472.0000, 49152.0000,        inf, 32832.0000, 34624.0000,\n",
      "        15888.0000, 26064.0000, 59936.0000, 22112.0000,  6444.0000,        inf,\n",
      "        25920.0000, 42976.0000,  7400.0000, 56608.0000, 28880.0000, 42944.0000,\n",
      "         3084.0000, 23360.0000, 57184.0000, 29520.0000,  8456.0000, 28848.0000,\n",
      "        20000.0000,        inf, 52768.0000, 12256.0000, 45024.0000, 42080.0000,\n",
      "        44096.0000,        inf, 64032.0000, 33056.0000,        inf, 19040.0000,\n",
      "        16256.0000,        inf, 13392.0000,  6376.0000,  5192.0000,  6152.0000,\n",
      "               inf, 45344.0000, 55744.0000,        inf, 13768.0000, 35168.0000,\n",
      "        28064.0000, 27168.0000, 60224.0000, 58048.0000, 19840.0000, 17424.0000,\n",
      "        35584.0000, 29616.0000,  9768.0000,  7656.0000, 47456.0000, 56832.0000,\n",
      "        33504.0000, 34176.0000,        inf,  3268.0000,  3168.0000, 52480.0000,\n",
      "          303.2500, 20016.0000,        inf, 25264.0000,  3004.0000, 26560.0000,\n",
      "         2102.0000, 45376.0000, 44800.0000, 57280.0000, 20304.0000,  7060.0000,\n",
      "        22640.0000, 31168.0000, 51712.0000, 15864.0000,        inf, 22400.0000,\n",
      "        20288.0000, 57472.0000, 26240.0000, 51520.0000,  5876.0000, 29936.0000,\n",
      "        56160.0000,  4576.0000, 63328.0000,  6852.0000, 34464.0000,        inf,\n",
      "         4812.0000, 56000.0000,        inf, 50944.0000, 23440.0000,        inf,\n",
      "         3970.0000, 33280.0000, 36096.0000,        inf,        inf, 12432.0000,\n",
      "               inf,        inf, 64000.0000, 23712.0000,  2818.0000,  5828.0000,\n",
      "        38016.0000,        inf, 61824.0000, 15472.0000, 11224.0000, 20192.0000,\n",
      "         3694.0000,        inf, 34592.0000, 14448.0000, 34336.0000,        inf,\n",
      "        11136.0000, 23840.0000,        inf, 22464.0000,        inf,  8640.0000,\n",
      "         4936.0000,        inf, 55936.0000, 30816.0000, 59680.0000,  8696.0000,\n",
      "        12064.0000, 34752.0000, 60480.0000, 30896.0000, 37760.0000,  3864.0000,\n",
      "        24240.0000, 14360.0000, 19936.0000, 31536.0000,  6300.0000, 10872.0000,\n",
      "        12184.0000, 13600.0000,  6840.0000, 41120.0000, 40672.0000,        inf,\n",
      "        36128.0000, 28720.0000, 31568.0000,        inf, 27616.0000, 33888.0000,\n",
      "         7172.0000, 15008.0000, 14304.0000, 29888.0000,        inf, 13584.0000,\n",
      "               inf,        inf,        inf,        inf,        inf, 36224.0000,\n",
      "        64480.0000,        inf, 11936.0000, 41152.0000, 43488.0000,  8720.0000,\n",
      "        21456.0000, 28720.0000, 43744.0000, 17136.0000,        inf,  4568.0000,\n",
      "        54240.0000, 26176.0000,        inf, 35232.0000,        inf,  4856.0000,\n",
      "         5608.0000, 59168.0000, 47936.0000, 51968.0000, 27360.0000,        inf,\n",
      "        33856.0000,        inf, 23488.0000,        inf,        inf, 41472.0000,\n",
      "        14024.0000,  1734.0000, 42848.0000, 13200.0000, 31792.0000,  1502.0000,\n",
      "        11928.0000, 48128.0000,        inf, 27184.0000,   406.0000, 39008.0000,\n",
      "        39296.0000, 37056.0000, 31824.0000, 37728.0000, 44512.0000, 16016.0000,\n",
      "        51040.0000, 24112.0000, 23648.0000, 41920.0000,  3130.0000, 64000.0000,\n",
      "         2450.0000, 28672.0000,        inf, 35712.0000, 13016.0000, 25808.0000,\n",
      "        46688.0000, 63072.0000,  2458.0000, 37664.0000, 20256.0000, 25216.0000,\n",
      "         7588.0000, 44896.0000, 36096.0000, 55776.0000,  6848.0000, 18944.0000,\n",
      "        48640.0000, 13576.0000, 39680.0000, 14536.0000, 35328.0000,  7584.0000,\n",
      "        25760.0000, 18544.0000, 49984.0000, 50976.0000, 33504.0000,   576.0000,\n",
      "          320.7500, 20576.0000, 14640.0000, 46880.0000, 23264.0000, 34432.0000,\n",
      "               inf,        inf,   691.5000,        inf, 11376.0000, 35424.0000,\n",
      "        20384.0000,  6168.0000,        inf,        inf,        inf, 11968.0000,\n",
      "        25824.0000, 18656.0000], device='cuda:0', dtype=torch.float16), tensor([4.3648e+04, 5.7360e+03, 2.3696e+04, 1.7900e+03, 9.3440e+03, 2.1440e+04,\n",
      "        7.9150e+02, 8.7680e+03, 1.3632e+04, 1.9936e+04, 2.9616e+04, 9.1760e+03,\n",
      "        2.6048e+04, 3.8048e+04, 1.6608e+04, 4.8576e+04, 1.2320e+04, 2.3640e+03,\n",
      "        1.9792e+04, 2.5104e+04, 1.7520e+04, 1.9632e+04, 3.2736e+04,        inf,\n",
      "        3.4976e+04, 1.6376e+04, 2.2080e+04, 1.7360e+04, 1.1176e+04, 2.5840e+04,\n",
      "        9.5440e+03, 1.6008e+04, 2.3024e+04, 2.0176e+04, 7.1720e+03, 4.6680e+03,\n",
      "               inf, 3.7280e+03, 5.0784e+04, 8.3040e+03, 9.0640e+03, 6.1240e+03,\n",
      "        2.1664e+04, 9.7200e+03, 1.2184e+04, 3.2640e+04, 5.4048e+04, 2.1296e+04,\n",
      "        2.8340e+03, 7.9280e+03, 6.0720e+03, 2.3680e+03, 4.6080e+03, 2.2060e+03,\n",
      "               inf, 2.6080e+04, 2.0896e+04, 2.0860e+03, 3.1936e+04, 3.7960e+03,\n",
      "        2.3552e+04, 6.4416e+04, 7.3000e+03, 1.0440e+03, 2.4016e+04, 2.7200e+03,\n",
      "        1.2376e+04, 4.0096e+04, 1.8736e+04, 1.4096e+04, 1.1648e+04, 7.0280e+03,\n",
      "        1.7408e+04, 1.5784e+04, 2.5420e+03,        inf, 3.0544e+04, 2.2920e+03,\n",
      "        4.1160e+03, 2.8512e+04, 2.0560e+04, 1.0168e+04, 1.8720e+04, 3.1584e+04,\n",
      "        8.8600e+02, 2.1440e+03, 8.0650e+02, 5.7760e+03, 1.8976e+04, 2.6736e+04,\n",
      "        2.3040e+03, 1.2728e+04, 7.0900e+02, 2.5800e+03, 2.4064e+04, 1.3820e+03,\n",
      "        2.5888e+04, 4.2520e+03, 1.1840e+04, 2.1488e+04, 6.6800e+03, 1.9824e+04,\n",
      "        8.4800e+03,        inf, 1.1984e+04, 2.0480e+04, 7.3360e+03,        inf,\n",
      "        3.3376e+04, 7.8320e+03, 6.0736e+04, 2.4200e+03, 8.6880e+03, 7.8320e+03,\n",
      "        5.4760e+03, 1.9150e+03, 7.0160e+03, 4.6000e+03, 4.1720e+03, 1.4392e+04,\n",
      "        2.4192e+04, 6.3808e+04, 1.1000e+04, 2.0576e+04, 3.9648e+04, 1.2256e+04,\n",
      "        1.5510e+03, 8.3440e+03, 6.1600e+02, 3.3952e+04, 5.4640e+03, 1.9152e+04,\n",
      "        2.8208e+04, 2.9344e+04, 8.5760e+03, 4.7320e+03, 1.2096e+04, 3.9800e+03,\n",
      "        1.4440e+03, 1.4176e+04, 1.9020e+03,        inf, 3.2608e+04, 2.9616e+04,\n",
      "        1.9840e+04, 2.3728e+04, 7.0600e+03, 4.6624e+04, 4.8448e+04, 1.8912e+04,\n",
      "        9.0200e+02, 9.0800e+02, 2.6848e+04, 1.1330e+03, 9.8960e+03, 4.5240e+03,\n",
      "        1.5152e+04, 1.9712e+04, 1.6360e+04, 1.8272e+04, 1.0944e+04, 1.0416e+04,\n",
      "        1.1112e+04, 4.7160e+03, 1.8592e+04, 4.1888e+04, 5.2672e+04,        inf,\n",
      "        2.0560e+04,        inf, 5.7720e+03, 4.1024e+04, 6.8920e+03, 1.9456e+04,\n",
      "        2.9856e+04, 4.9504e+04, 1.2704e+04, 7.5800e+03, 1.8320e+04, 3.6800e+03,\n",
      "        4.8992e+04, 1.2600e+03, 1.2312e+04, 2.2096e+04, 4.0480e+04, 4.1568e+04,\n",
      "        1.3400e+04, 3.2040e+03, 2.5020e+03, 1.5776e+04, 2.0112e+04, 3.8784e+04,\n",
      "        1.9408e+04, 1.7952e+04, 2.2416e+04, 2.3856e+04, 3.5744e+04, 4.1664e+04,\n",
      "        1.9810e+03, 4.1152e+04, 1.2664e+04, 8.4750e+02, 2.0240e+04, 3.6320e+04,\n",
      "        1.3720e+04, 2.1600e+04, 5.2736e+04, 2.8144e+04, 1.9664e+04, 8.2480e+03,\n",
      "        1.2880e+04, 2.3168e+04, 1.1648e+04, 1.6048e+04, 3.8272e+04, 2.0256e+04,\n",
      "        4.4960e+04,        inf, 1.4672e+04, 1.9664e+04, 5.5080e+03, 2.0912e+04,\n",
      "        3.3760e+04, 5.2512e+04, 4.6112e+04, 1.3520e+04, 4.2048e+04, 4.1920e+04,\n",
      "        1.4568e+04, 1.2480e+03, 6.3240e+03, 5.2512e+04, 2.9712e+04, 7.8000e+02,\n",
      "        8.1120e+03, 4.3000e+03, 2.6768e+04, 7.6120e+03, 5.0496e+04, 3.2820e+03,\n",
      "        1.4184e+04, 2.2048e+04, 1.7600e+04, 2.1200e+03, 8.0440e+03, 6.9360e+03,\n",
      "        3.4720e+04, 1.0480e+03, 4.9120e+03, 5.5488e+04, 1.3472e+04, 2.3660e+03,\n",
      "        2.3328e+04, 2.2944e+04, 1.0064e+04, 1.1216e+04, 5.0500e+01, 3.7100e+03,\n",
      "        2.9968e+04, 4.9520e+03, 1.2664e+04, 3.0864e+04, 2.5488e+04, 2.3904e+04,\n",
      "        4.5984e+04, 9.2720e+03, 1.4096e+04, 2.8640e+04, 3.9808e+04, 2.8256e+04,\n",
      "        1.1816e+04, 3.0096e+04, 2.9744e+04, 2.2528e+04, 4.9800e+03, 1.1872e+04,\n",
      "        3.1280e+04, 4.3584e+04,        inf, 9.6320e+03, 1.4120e+04, 5.9136e+04,\n",
      "        1.6688e+04, 4.5440e+03, 3.3952e+04, 3.6736e+04, 3.5968e+04, 1.4888e+04,\n",
      "        3.2176e+04, 1.0424e+04, 3.1088e+04, 1.0264e+04, 3.3568e+04, 4.8224e+04,\n",
      "        2.2320e+04, 1.8224e+04,        inf, 1.0488e+04, 2.9616e+04, 8.7050e+02,\n",
      "        7.6000e+03, 7.4160e+03, 4.9856e+04, 1.7648e+04, 1.6224e+04, 2.8128e+04,\n",
      "        2.2288e+04,        inf, 5.0144e+04, 4.4352e+04, 1.1888e+04, 3.4976e+04,\n",
      "        7.5400e+03, 4.2976e+04, 4.5024e+04, 2.9200e+04, 2.2200e+03, 2.1984e+04,\n",
      "        8.3800e+02, 4.7640e+03, 1.0568e+04, 8.2100e+02, 3.5040e+04, 2.0704e+04,\n",
      "        1.9520e+04, 1.5096e+04, 3.2032e+04, 1.6248e+04, 9.8400e+03, 1.8112e+04,\n",
      "        3.1248e+04,        inf, 4.5600e+04, 6.8000e+01, 1.3088e+04, 3.1760e+04,\n",
      "        1.3216e+04, 8.8960e+03, 3.5872e+04, 2.4464e+04,        inf, 7.8480e+03,\n",
      "        3.9808e+04, 3.3184e+04, 2.1680e+03, 4.7328e+04, 1.2192e+04, 1.0648e+04,\n",
      "               inf,        inf, 2.4208e+04, 2.4464e+04, 1.3816e+04, 7.5440e+03,\n",
      "        1.7280e+04,        inf, 2.9120e+03, 9.2960e+03, 4.0928e+04, 7.4440e+03,\n",
      "        1.9616e+04, 1.8544e+04,        inf, 8.5920e+03, 1.4208e+04, 5.6640e+03,\n",
      "        6.1600e+04, 1.4176e+04,        inf, 1.7632e+04, 5.3240e+03, 1.0608e+04,\n",
      "        9.2400e+02, 6.7360e+03,        inf, 5.3856e+04, 2.3072e+04, 1.4248e+04,\n",
      "        1.2200e+04, 2.7568e+04, 4.0840e+03,        inf, 6.8640e+03, 7.7250e+02,\n",
      "        2.4368e+04, 1.1680e+04, 2.1776e+04, 1.0248e+04, 6.0360e+03,        inf,\n",
      "        2.2256e+04, 6.2016e+04, 2.2380e+03,        inf, 3.7360e+03, 3.8976e+04,\n",
      "        1.6144e+04, 2.7200e+03, 1.6448e+04, 6.9560e+03, 2.0960e+04, 1.6368e+04,\n",
      "               inf,        inf,        inf,        inf,        inf, 1.5088e+04,\n",
      "        3.2656e+04, 4.3808e+04, 7.3640e+03, 6.0448e+04, 2.3808e+04, 6.0920e+03,\n",
      "        2.3264e+04, 1.9010e+03, 2.0752e+04, 1.4032e+04, 4.2944e+04, 2.7740e+03,\n",
      "        2.6112e+04, 6.2160e+03,        inf, 2.1000e+01,        inf, 3.6200e+03,\n",
      "        2.8736e+04, 1.4168e+04, 1.0216e+04, 2.8768e+04, 8.2080e+03, 1.8400e+04,\n",
      "        2.8512e+04, 4.6592e+04, 3.7920e+04, 4.1856e+04, 5.0208e+04, 2.4608e+04,\n",
      "        3.4080e+04, 1.5240e+04, 1.9872e+04, 3.2928e+04, 1.2800e+04, 1.2210e+03,\n",
      "        2.6832e+04, 4.9888e+04,        inf, 2.6896e+04, 1.8310e+03, 1.2200e+04,\n",
      "        2.3392e+04, 2.6912e+04, 2.1376e+04, 2.4528e+04, 2.3024e+04, 1.2168e+04,\n",
      "        4.7904e+04, 3.0912e+04, 2.4448e+04, 3.4720e+04, 2.8064e+04, 4.7040e+03,\n",
      "        3.7800e+03, 2.8992e+04, 6.7120e+03, 1.9824e+04, 8.8320e+03, 1.4368e+04,\n",
      "        1.0376e+04, 1.7616e+04, 3.7152e+04, 1.5840e+04, 1.6608e+04, 1.0600e+04,\n",
      "        6.1960e+03, 1.8976e+04, 4.9408e+04, 2.9056e+04, 3.2688e+04, 2.6768e+04,\n",
      "        1.9808e+04, 1.2528e+04, 3.4880e+04, 2.8576e+04, 8.5040e+03, 6.0520e+03,\n",
      "        5.3248e+04, 2.4928e+04, 3.2992e+04, 2.5104e+04, 1.0912e+04, 2.5488e+04,\n",
      "        2.0912e+04, 1.4936e+04, 8.1600e+03, 5.7312e+04, 2.6160e+04, 3.1184e+04,\n",
      "        3.0800e+04, 4.5408e+04, 2.1860e+03, 1.4384e+04, 2.5760e+04, 2.6656e+04,\n",
      "        7.7680e+03, 1.5480e+04, 2.6448e+04, 3.8272e+04, 2.6000e+04, 3.4880e+04,\n",
      "        4.3616e+04, 1.7632e+04], device='cuda:0', dtype=torch.float16), tensor([2.6288e+04, 5.4400e+04, 3.2112e+04, 1.8064e+04, 4.4544e+04, 3.5200e+04,\n",
      "        3.5392e+04, 1.7520e+04, 6.2600e+03, 2.2656e+04, 1.8816e+04, 1.3840e+04,\n",
      "        3.6512e+04, 4.8448e+04,        inf,        inf, 5.9800e+03, 1.5700e+03,\n",
      "        2.2000e+04, 5.1744e+04, 2.4325e+02, 3.0032e+04,        inf,        inf,\n",
      "        5.0240e+04,        inf, 1.6120e+03, 2.7408e+04,        inf, 1.6240e+03,\n",
      "        2.9620e+03, 1.6592e+04, 4.8736e+04, 6.5152e+04,        inf, 3.1344e+01,\n",
      "               inf, 4.1248e+04,        inf, 3.7664e+04, 5.0176e+04,        inf,\n",
      "        5.2928e+04, 1.8352e+04, 1.8704e+04, 4.3200e+03, 6.4512e+04, 2.4224e+04,\n",
      "        4.5720e+03, 2.0640e+03, 1.6120e+04, 3.7888e+04, 8.0720e+03, 4.1312e+04,\n",
      "               inf, 4.9472e+04, 1.7248e+04, 1.8448e+04, 1.3552e+04, 9.6320e+03,\n",
      "               inf,        inf, 5.4912e+04, 1.4910e+03, 6.3168e+04, 3.3824e+04,\n",
      "        2.6320e+04, 3.9520e+04, 2.8464e+04, 4.5888e+04, 3.1184e+04, 5.9648e+04,\n",
      "               inf, 5.7408e+04, 3.2672e+04,        inf, 3.7888e+04, 2.0368e+04,\n",
      "        5.6400e+03, 4.1408e+04, 3.9936e+04, 1.7792e+04, 2.0400e+04, 6.0128e+04,\n",
      "        1.8120e+03, 5.1392e+04, 6.5160e+03, 3.8464e+04, 5.6928e+04, 1.6960e+04,\n",
      "        1.1232e+04, 1.4736e+04, 4.1376e+04, 3.5328e+04, 1.8032e+04, 2.2176e+04,\n",
      "        2.4832e+04, 3.1680e+04, 1.1064e+04, 1.0288e+04, 1.2744e+04, 4.6624e+04,\n",
      "        8.7440e+03,        inf, 4.5696e+04, 3.4176e+04, 4.7760e+03,        inf,\n",
      "        6.3968e+04, 1.4272e+04,        inf, 2.2336e+04, 6.0520e+03, 3.0784e+04,\n",
      "        9.9040e+03, 5.8920e+03, 3.0624e+04, 4.0000e+04, 4.2464e+04, 2.7184e+04,\n",
      "        1.3240e+03,        inf, 4.8256e+04, 5.3824e+04, 3.9776e+04, 3.8048e+04,\n",
      "        6.5440e+04, 4.4480e+04, 1.2624e+04, 5.7760e+04, 4.4880e+03, 4.5344e+04,\n",
      "        3.3312e+04, 2.4400e+04, 3.8144e+04, 1.8864e+04, 9.0080e+03, 5.2160e+04,\n",
      "               inf,        inf, 2.5120e+04, 4.8704e+04, 2.2784e+04,        inf,\n",
      "        2.9488e+04, 2.8960e+04, 3.0912e+04, 4.3776e+04, 5.7056e+04, 3.4240e+04,\n",
      "        3.9360e+03, 1.1976e+04, 2.4784e+04, 1.1168e+04, 5.8360e+03, 7.5350e+02,\n",
      "        2.2992e+04, 4.4544e+04,        inf, 4.7072e+04, 1.6280e+04, 9.1600e+03,\n",
      "        4.7008e+04, 3.7728e+04, 2.9616e+04,        inf, 5.3632e+04,        inf,\n",
      "               inf,        inf, 2.1952e+04, 6.2752e+04, 9.0720e+03, 3.7056e+04,\n",
      "        1.2728e+04,        inf, 3.2928e+04, 3.0736e+04, 2.3024e+04, 2.1552e+04,\n",
      "               inf, 1.9456e+04, 2.9360e+04, 4.9248e+04, 4.9696e+04,        inf,\n",
      "        2.3920e+04, 1.8384e+04, 2.7296e+04, 2.7184e+04, 3.9296e+04, 6.3104e+04,\n",
      "        1.6448e+04, 2.4304e+04, 2.1824e+04, 3.2240e+04, 2.8112e+04, 4.2496e+04,\n",
      "        3.1712e+04, 4.2368e+04, 2.1872e+04, 2.3504e+04, 6.4896e+04, 2.8688e+04,\n",
      "        1.3520e+04, 5.6352e+04,        inf,        inf, 8.6560e+03, 5.9136e+04,\n",
      "        5.6928e+04, 6.5120e+04, 4.9120e+03, 1.5296e+04, 5.0784e+04, 1.7264e+04,\n",
      "        3.6800e+04,        inf, 7.7160e+03,        inf, 3.6352e+04, 3.4496e+04,\n",
      "        5.9232e+04,        inf,        inf, 6.1408e+04, 3.9520e+04, 5.4720e+04,\n",
      "        5.6032e+04, 1.6250e+03, 2.1200e+03, 2.9472e+04, 3.4048e+04, 2.7808e+04,\n",
      "        2.5580e+03, 8.5680e+03,        inf, 5.3280e+04, 2.3472e+04, 3.6160e+04,\n",
      "        3.9872e+04, 1.6352e+04, 5.8976e+04, 1.8096e+04, 5.8600e+03, 4.2336e+04,\n",
      "        6.0032e+04, 1.8784e+04, 9.3120e+03, 6.2400e+04, 1.4664e+04, 5.5968e+04,\n",
      "        8.9760e+03, 1.5216e+04, 4.6400e+04, 2.0192e+04, 7.5200e+03, 3.2464e+04,\n",
      "        8.6560e+03,        inf, 3.0304e+04, 3.3504e+04, 3.8944e+04, 2.7616e+04,\n",
      "        5.4304e+04,        inf, 4.9536e+04, 1.9360e+04,        inf, 1.7340e+03,\n",
      "        4.1760e+03,        inf, 1.4376e+04, 9.5350e+02, 2.5568e+04, 1.8362e+02,\n",
      "               inf, 4.6688e+04,        inf, 5.7888e+04, 9.1760e+03, 5.6416e+04,\n",
      "        6.7960e+03, 2.0432e+04,        inf, 6.2880e+04, 2.4752e+04, 2.4160e+04,\n",
      "        1.4520e+04, 4.6976e+04, 4.1800e+03, 8.8960e+03,        inf, 5.5232e+04,\n",
      "        4.2240e+04, 3.5488e+04,        inf, 1.4300e+03, 9.3920e+03, 3.1472e+04,\n",
      "        5.4720e+03, 7.4800e+03,        inf, 1.3336e+04, 3.2940e+03, 2.3072e+04,\n",
      "        6.4160e+03,        inf, 4.3744e+04,        inf, 4.9984e+04, 3.7344e+04,\n",
      "        1.3904e+04, 3.6000e+04, 3.5904e+04, 4.3025e+02, 5.6512e+04, 4.4832e+04,\n",
      "        2.4656e+04, 3.3664e+04, 4.0384e+04, 2.6736e+04, 8.7120e+03, 2.9760e+04,\n",
      "        4.8224e+04, 2.2544e+04, 5.7504e+04, 1.3616e+04, 1.1224e+04,        inf,\n",
      "        1.9808e+04,        inf,        inf, 5.7376e+04, 3.0576e+04,        inf,\n",
      "        6.8360e+03, 1.7984e+04, 4.8608e+04,        inf,        inf, 2.1184e+04,\n",
      "               inf,        inf, 4.4224e+04, 3.2208e+04, 1.3240e+04, 3.1594e+01,\n",
      "               inf,        inf, 5.9552e+04, 2.0112e+04, 2.3904e+04, 1.2936e+04,\n",
      "        2.6816e+04,        inf, 9.9280e+03, 1.5050e+03, 2.9056e+04, 5.3056e+04,\n",
      "        3.4688e+04, 6.9600e+03,        inf, 5.0880e+04, 5.8816e+04, 1.2680e+04,\n",
      "        2.7648e+04, 4.3392e+04, 6.3232e+04, 1.5808e+04, 2.8032e+04, 6.3350e+02,\n",
      "        1.0688e+04, 4.4096e+04,        inf, 3.5808e+04, 3.9936e+04, 2.2192e+04,\n",
      "        2.3536e+04, 2.3312e+04, 8.1062e+01, 5.7472e+04, 1.8304e+04, 1.4224e+04,\n",
      "        3.3184e+04, 2.6160e+04, 1.4600e+04, 5.0560e+04, 1.4432e+04,        inf,\n",
      "        4.5568e+04, 5.2288e+04, 1.1576e+04,        inf, 1.2590e+03, 3.9136e+04,\n",
      "        2.7904e+04, 9.0240e+03, 4.0640e+04, 3.2992e+04, 5.3984e+04, 5.1200e+03,\n",
      "               inf,        inf,        inf,        inf, 5.3632e+04, 1.0840e+04,\n",
      "        5.4560e+04,        inf, 5.5520e+03, 4.4288e+04, 2.5648e+04, 7.7000e+03,\n",
      "        2.5824e+04, 1.7584e+04, 3.7824e+04, 1.2080e+04, 6.4992e+04, 7.9120e+03,\n",
      "        3.8720e+04, 2.1232e+04,        inf, 3.6640e+04,        inf, 2.3040e+03,\n",
      "        2.8448e+04, 4.2720e+04, 3.5840e+04,        inf, 2.1168e+04,        inf,\n",
      "        3.2048e+04,        inf, 4.0416e+04,        inf,        inf, 4.3168e+04,\n",
      "        4.8096e+04, 4.8800e+03, 5.4656e+04, 3.3888e+04, 2.5552e+04, 8.1520e+03,\n",
      "        2.4048e+04, 4.8256e+04,        inf, 8.0440e+03, 1.2608e+04, 8.0360e+03,\n",
      "        2.7584e+04, 2.8192e+04, 4.3776e+04, 2.2096e+04, 2.7840e+04, 4.1160e+03,\n",
      "        4.8640e+04, 1.9376e+04, 4.7280e+03, 2.6912e+04, 2.5200e+04, 5.1008e+04,\n",
      "        8.5120e+03, 4.1280e+04,        inf, 2.7008e+04, 2.2700e+03, 1.4928e+04,\n",
      "        2.3152e+04, 2.2720e+04, 3.3248e+04, 1.4992e+04, 2.4336e+04, 1.6304e+04,\n",
      "        8.0320e+03, 5.2768e+04, 3.5264e+04, 5.4656e+04, 1.6448e+04, 4.8480e+03,\n",
      "        1.6400e+04, 3.0352e+04, 2.7616e+04, 1.4952e+04, 3.1424e+04, 6.5400e+03,\n",
      "        5.8880e+04, 2.2768e+04, 3.4784e+04, 4.8256e+04, 3.5648e+04, 4.4992e+04,\n",
      "        1.6736e+04, 4.6760e+03, 1.1624e+04, 4.5728e+04, 3.2832e+04, 3.2320e+04,\n",
      "               inf, 4.1568e+04, 1.3000e+03, 3.3152e+04, 2.0048e+04, 1.7616e+04,\n",
      "        1.4128e+04, 1.7872e+04,        inf, 4.4672e+04, 4.8608e+04, 2.5696e+04,\n",
      "        3.4336e+04, 4.7808e+04], device='cuda:0', dtype=torch.float16), tensor([[ 1895.0000,  1410.0000,   936.0000,  ...,   839.0000,  3006.0000,\n",
      "           939.0000],\n",
      "        [  830.0000,   546.0000,   915.0000,  ...,  1884.0000,  2696.0000,\n",
      "           509.0000],\n",
      "        [ 2270.0000,   485.7500,   554.5000,  ...,   847.5000,   820.0000,\n",
      "          1622.0000],\n",
      "        ...,\n",
      "        [26384.0000,  1988.0000, 22720.0000,  ...,  2604.0000,  6456.0000,\n",
      "          8304.0000],\n",
      "        [ 1144.0000,  4496.0000, 17744.0000,  ..., 28912.0000, 20800.0000,\n",
      "          1508.0000],\n",
      "        [39360.0000, 11112.0000,  4800.0000,  ..., 37344.0000, 27424.0000,\n",
      "         25968.0000]], device='cuda:0', dtype=torch.float16), tensor([ 1808.,  1086.,   845.,  ..., 10888., 24400., 29664.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[10664., 12688., 52416.,  ..., 15136., 16912., 27584.],\n",
      "        [ 6444., 28352., 48704.,  ...,  9024., 55008.,   788.],\n",
      "        [17312.,   478., 38144.,  ..., 20864.,  1728.,  8256.],\n",
      "        ...,\n",
      "        [12912., 21664.,  7840.,  ..., 36448., 60896., 17488.],\n",
      "        [ 9456., 20560., 52320.,  ...,  2236., 49056., 12384.],\n",
      "        [ 6796., 28224., 10016.,  ..., 25248., 64928.,  9512.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([2.9104e+04, 5.8816e+04, 1.2456e+04, 3.9456e+04, 5.0720e+04, 4.8928e+04,\n",
      "        3.5936e+04, 1.7648e+04, 2.6660e+03, 3.9860e+03, 2.7696e+04, 3.1056e+04,\n",
      "        1.4336e+04, 3.5488e+04,        inf,        inf, 2.1184e+04, 1.8064e+04,\n",
      "        1.0736e+04, 4.7392e+04, 1.2480e+04, 1.6152e+04,        inf,        inf,\n",
      "        5.6640e+03, 5.1424e+04, 2.8272e+04, 2.1560e+03,        inf, 3.9072e+04,\n",
      "        6.3650e+02, 5.5560e+03, 2.2672e+04, 3.4976e+04,        inf, 1.4536e+04,\n",
      "               inf, 2.4224e+04,        inf, 4.2016e+04, 6.5280e+04, 4.5792e+04,\n",
      "        2.5760e+04, 1.6328e+04, 1.7840e+04, 1.7824e+04,        inf, 2.1824e+04,\n",
      "        1.6240e+04, 9.4000e+03, 2.4096e+04, 3.9680e+04, 1.1944e+04, 1.7472e+04,\n",
      "        5.5488e+04, 2.8816e+04, 2.0528e+04, 5.0720e+04, 8.2800e+03, 1.4512e+04,\n",
      "        3.8432e+04,        inf, 6.1696e+04, 7.6120e+03, 2.7648e+04, 3.1712e+04,\n",
      "        1.1784e+04, 4.2080e+04, 2.0320e+04, 3.6864e+04, 8.4320e+03, 5.7344e+04,\n",
      "        3.9264e+04, 5.9904e+04, 4.2112e+04,        inf, 8.2000e+03, 1.3032e+04,\n",
      "        1.4080e+04, 4.3168e+04, 3.0016e+04, 2.3136e+04, 4.8520e+03, 6.4960e+04,\n",
      "        4.5906e+01, 5.3824e+04, 2.8800e+04, 4.1504e+04, 2.3568e+04, 3.4560e+04,\n",
      "        2.1104e+04, 4.9875e+02, 2.7504e+04, 3.2512e+04, 9.5200e+03, 3.0992e+04,\n",
      "        4.5216e+04, 4.6496e+04, 1.9280e+04, 1.7248e+04, 1.9536e+04, 1.8480e+04,\n",
      "        1.5330e+03,        inf, 5.6448e+04, 2.2624e+04, 8.9600e+03, 5.2928e+04,\n",
      "        5.9040e+04, 2.0496e+04, 3.9328e+04, 3.4656e+04, 1.9280e+04, 4.4064e+04,\n",
      "        4.1376e+04, 4.4800e+03, 1.7850e+03, 3.3920e+04, 3.4048e+04, 3.2528e+04,\n",
      "        1.3616e+04, 3.0384e+04, 4.2144e+04, 3.7440e+04, 1.1656e+04, 3.9904e+04,\n",
      "               inf, 2.9664e+04, 9.7440e+03, 4.8480e+04, 2.9152e+04, 4.1696e+04,\n",
      "        3.2720e+04, 3.3216e+04, 5.5872e+04, 2.8464e+04, 1.8768e+04, 6.1216e+04,\n",
      "        5.7088e+04, 6.3456e+04, 1.0488e+04, 1.0920e+04, 5.0480e+03,        inf,\n",
      "        3.0144e+04, 3.5360e+04, 3.4144e+04, 2.8704e+04,        inf, 2.2336e+04,\n",
      "        2.8448e+04, 5.4160e+03, 1.5760e+04, 4.1632e+04, 6.9840e+03, 5.1480e+03,\n",
      "        4.1408e+04, 5.5712e+04, 6.4608e+04, 4.3424e+04, 8.0080e+03, 1.7248e+04,\n",
      "               inf, 1.5504e+04, 1.1160e+04,        inf, 3.8240e+04,        inf,\n",
      "        5.1648e+04,        inf, 1.8496e+04, 6.0192e+04, 4.0928e+04, 1.5648e+04,\n",
      "        5.9520e+03,        inf, 3.9008e+04, 9.8240e+03, 2.0528e+04, 1.6800e+04,\n",
      "               inf, 5.2640e+03, 9.2240e+03, 2.6960e+04, 2.7056e+04, 4.2176e+04,\n",
      "        4.0128e+04, 2.4480e+03, 2.1712e+04, 2.0048e+04, 4.3136e+04,        inf,\n",
      "        7.8360e+03, 1.8336e+04, 2.6112e+04, 1.2680e+04, 9.0240e+03, 4.1984e+04,\n",
      "        6.1920e+04, 1.5952e+04, 2.9312e+04, 4.4800e+03, 4.4608e+04, 4.7488e+04,\n",
      "        1.2496e+04, 1.7456e+04,        inf,        inf, 1.3032e+04, 4.2976e+04,\n",
      "        7.3720e+03, 4.4544e+04, 4.0608e+04, 1.1240e+04, 5.3376e+04, 2.9744e+04,\n",
      "        2.2736e+04,        inf, 7.3720e+03, 5.3312e+04, 1.4992e+04, 2.2432e+04,\n",
      "        2.9952e+04,        inf,        inf, 4.5024e+04, 2.5312e+04, 4.7424e+04,\n",
      "        3.9072e+04, 1.6576e+04, 4.4425e+02, 1.6030e+03, 4.6368e+04, 2.8976e+04,\n",
      "        3.9520e+04, 6.6080e+03, 6.1344e+04, 3.7408e+04, 2.1584e+04, 2.1056e+04,\n",
      "        1.2872e+04, 2.4288e+04, 3.7344e+04, 2.7900e+03, 5.1720e+03, 2.4544e+04,\n",
      "        5.1456e+04, 1.7264e+04, 3.2608e+04, 3.2896e+04, 2.0752e+04, 3.1584e+04,\n",
      "        3.7504e+04, 1.6192e+04, 4.4768e+04, 8.2320e+03, 2.6000e+04, 2.7072e+04,\n",
      "        1.1768e+04,        inf, 5.9104e+04, 2.7056e+04, 2.3152e+04, 5.9960e+03,\n",
      "               inf,        inf, 4.4800e+04, 5.0725e+02,        inf, 2.2912e+04,\n",
      "        1.3464e+04,        inf, 3.6768e+04, 3.3568e+04, 3.3120e+04, 1.3576e+04,\n",
      "               inf, 3.9264e+04,        inf, 2.7328e+04, 2.4608e+04, 4.6528e+04,\n",
      "        5.2032e+04, 1.9968e+04, 6.2688e+04, 5.6864e+04, 7.1680e+03, 1.6544e+04,\n",
      "        2.1280e+04, 4.5632e+04, 2.7632e+04, 8.2960e+03,        inf, 4.4192e+04,\n",
      "               inf, 1.4600e+04,        inf, 2.5760e+04, 1.8176e+04, 2.8432e+04,\n",
      "        2.7376e+04, 1.6416e+04,        inf, 2.0940e+03, 9.9680e+03, 1.3584e+04,\n",
      "        5.0400e+03,        inf, 5.5904e+04,        inf, 5.0528e+04, 2.9712e+04,\n",
      "        6.7000e+03, 1.1280e+04, 2.2944e+04, 2.0896e+04, 5.8752e+04, 3.0880e+04,\n",
      "        3.0640e+03, 3.9200e+04, 3.7984e+04, 2.3808e+04, 3.9136e+04, 4.2048e+04,\n",
      "        5.3568e+04, 5.5520e+03, 3.5328e+04, 1.9008e+04, 2.3248e+04,        inf,\n",
      "        1.3816e+04, 5.2480e+04,        inf, 1.6896e+04, 4.4288e+04,        inf,\n",
      "        1.9840e+04, 9.3760e+03, 2.4480e+04, 6.3072e+04,        inf, 4.0256e+04,\n",
      "               inf,        inf, 6.4192e+04, 1.7824e+04, 2.2704e+04, 4.8480e+04,\n",
      "        6.1120e+04,        inf, 5.3056e+04, 1.4232e+04, 3.5552e+04, 8.3600e+03,\n",
      "        3.6512e+04,        inf, 1.9040e+04, 3.8100e+03, 1.1480e+04, 4.3392e+04,\n",
      "        4.5760e+04, 6.5640e+03,        inf, 1.6224e+04, 4.9312e+04, 2.1072e+04,\n",
      "        2.6992e+04, 2.6496e+04, 2.8592e+04, 2.9472e+04, 1.9968e+04, 1.7300e+03,\n",
      "        2.2688e+04, 3.8560e+04, 6.5088e+04, 5.3504e+04, 4.6272e+04, 4.2112e+04,\n",
      "        2.1120e+04, 1.5672e+04, 2.3296e+04, 3.1104e+04, 1.6144e+04, 1.6592e+04,\n",
      "        5.9840e+04, 2.3456e+04, 4.8040e+03, 2.2688e+04, 2.8800e+04, 5.3056e+04,\n",
      "        3.6256e+04, 2.2960e+04, 1.0328e+04,        inf, 6.0680e+03, 5.4560e+04,\n",
      "        2.4992e+04, 4.7560e+03, 1.8176e+04, 4.2752e+04,        inf, 1.8304e+04,\n",
      "               inf, 5.8336e+04, 4.1280e+04,        inf,        inf, 3.3504e+04,\n",
      "        4.1344e+04,        inf, 7.0720e+03, 2.3328e+04, 1.5008e+04, 6.8320e+03,\n",
      "        1.9168e+04, 1.0176e+04, 1.0464e+04, 1.8160e+04, 4.9248e+04, 6.7840e+03,\n",
      "        2.9040e+04, 1.3856e+04,        inf, 5.0496e+04,        inf, 6.6040e+03,\n",
      "        1.9450e+03, 5.1232e+04, 1.3512e+04, 5.1328e+04, 2.8176e+04, 5.0720e+04,\n",
      "        1.4856e+04,        inf, 4.5632e+04,        inf,        inf, 3.5616e+04,\n",
      "        3.6736e+04, 7.7240e+03,        inf, 5.1040e+04, 1.6864e+04, 2.2060e+03,\n",
      "        9.6720e+03, 7.6480e+03, 6.3584e+04, 2.3620e+03, 2.0512e+02, 1.2776e+04,\n",
      "        2.2080e+04, 4.7328e+04, 4.0640e+04, 2.9536e+04, 2.8032e+04, 4.0220e+03,\n",
      "        8.7760e+03, 2.8000e+04, 2.4752e+04, 1.5536e+04, 2.4016e+04,        inf,\n",
      "        1.2288e+04, 3.7408e+04,        inf, 3.2528e+04, 1.2872e+04, 2.9360e+04,\n",
      "        2.6416e+04, 2.1760e+04, 1.6544e+04, 4.8160e+03, 1.5744e+04, 1.2096e+04,\n",
      "        3.3952e+04, 4.8320e+04, 3.3376e+04,        inf, 3.9800e+03, 3.2064e+04,\n",
      "        5.1008e+04, 1.8832e+04, 3.9616e+04, 3.9392e+04, 9.3680e+03, 1.2248e+04,\n",
      "        6.0960e+04, 1.4584e+04, 5.5936e+04, 4.7232e+04, 1.7680e+04, 2.9616e+04,\n",
      "        3.5520e+04, 4.8320e+03, 1.5152e+04, 4.0352e+04, 4.3232e+04, 1.1912e+04,\n",
      "               inf, 4.0800e+04, 7.7900e+02, 2.4304e+04, 8.8160e+03, 1.1168e+04,\n",
      "        6.4000e+03, 1.0672e+04,        inf, 4.9664e+04, 4.8896e+04, 3.5648e+04,\n",
      "        5.2800e+04, 4.3200e+04], device='cuda:0', dtype=torch.float16), tensor([[ 2240.0000,   785.0000,  1302.0000,  ...,  1516.0000,  1344.0000,\n",
      "           975.5000],\n",
      "        [  676.0000,  3786.0000,  5784.0000,  ...,  7508.0000,  3976.0000,\n",
      "           857.5000],\n",
      "        [ 3568.0000,   485.0000,   173.0000,  ...,   839.0000,   993.0000,\n",
      "          1588.0000],\n",
      "        ...,\n",
      "        [ 8984.0000, 18784.0000,  4176.0000,  ..., 27712.0000, 12096.0000,\n",
      "         35264.0000],\n",
      "        [  253.7500,  3540.0000,   316.0000,  ...,  9072.0000,    54.0000,\n",
      "          9328.0000],\n",
      "        [  610.0000,   201.8750,   653.0000,  ...,  1253.0000,  1445.0000,\n",
      "          2550.0000]], device='cuda:0', dtype=torch.float16), tensor([ 2290.0000,  4440.0000,   229.6250,  ..., 27872.0000,  4268.0000,\n",
      "          452.7500], device='cuda:0', dtype=torch.float16), tensor([[13056.,  3852., 12840.,  ..., 26928., 10744., 35936.],\n",
      "        [43200.,  6668.,   752.,  ..., 35520., 18944., 46880.],\n",
      "        [ 6356.,   751.,  7076.,  ..., 17104., 14688., 23936.],\n",
      "        ...,\n",
      "        [43424.,  7940., 16184.,  ..., 15432., 38496., 19568.],\n",
      "        [47264.,  6044.,  5020.,  ..., 53344.,  3612.,    inf],\n",
      "        [45760.,  2864.,  9344.,  ..., 34304., 23200., 42272.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([24448.0000, 49536.0000,  8704.0000, 30816.0000, 50976.0000, 63616.0000,\n",
      "        35488.0000, 19472.0000, 11400.0000, 11256.0000, 31408.0000, 30208.0000,\n",
      "        23840.0000, 37952.0000,        inf,        inf, 19632.0000, 23344.0000,\n",
      "         6708.0000, 58912.0000, 18128.0000, 18432.0000,        inf,        inf,\n",
      "        12256.0000, 56768.0000, 27216.0000,   736.0000,        inf, 38720.0000,\n",
      "         1581.0000,  3610.0000, 16688.0000, 34784.0000,        inf, 10256.0000,\n",
      "               inf, 24880.0000,        inf, 47648.0000,        inf, 53184.0000,\n",
      "        26880.0000, 13576.0000, 19488.0000, 18048.0000,        inf, 23856.0000,\n",
      "        13080.0000,  2654.0000, 21184.0000, 48608.0000, 16080.0000, 20080.0000,\n",
      "               inf, 30496.0000, 23936.0000, 51200.0000, 10624.0000, 18480.0000,\n",
      "        38176.0000,        inf,        inf,  6928.0000, 28000.0000, 35872.0000,\n",
      "        10952.0000, 40256.0000, 20880.0000, 36992.0000,  9272.0000, 57504.0000,\n",
      "        41728.0000, 47552.0000, 45632.0000,        inf,  5280.0000, 16608.0000,\n",
      "         9112.0000, 47296.0000, 36576.0000, 20880.0000, 12352.0000,        inf,\n",
      "         6260.0000, 58560.0000, 33760.0000, 40288.0000, 27568.0000, 36800.0000,\n",
      "        25376.0000,  3798.0000, 32080.0000, 37312.0000, 11336.0000, 29440.0000,\n",
      "        47456.0000, 54112.0000, 22560.0000, 13720.0000, 22944.0000, 30992.0000,\n",
      "         2864.0000,        inf,        inf, 15896.0000,  8768.0000, 54720.0000,\n",
      "               inf, 20608.0000, 39872.0000, 32832.0000, 19152.0000, 51264.0000,\n",
      "        51072.0000,  6116.0000,  6928.0000, 34304.0000, 34112.0000, 34272.0000,\n",
      "         9768.0000, 27248.0000, 46688.0000, 34112.0000, 11632.0000, 37184.0000,\n",
      "               inf, 29936.0000, 14720.0000, 54304.0000, 30928.0000, 36832.0000,\n",
      "        29184.0000, 37536.0000, 63872.0000, 24032.0000, 16800.0000,        inf,\n",
      "        60192.0000,        inf, 13368.0000, 27104.0000,  4188.0000,        inf,\n",
      "        36160.0000, 39168.0000, 30336.0000, 23632.0000,        inf, 23072.0000,\n",
      "        33440.0000,  3536.0000, 18560.0000, 44256.0000,  8808.0000,  7384.0000,\n",
      "        42752.0000, 52352.0000, 62976.0000, 40448.0000,  6616.0000, 10928.0000,\n",
      "               inf, 18400.0000, 20672.0000,        inf, 40512.0000, 63520.0000,\n",
      "        57504.0000,        inf, 11192.0000, 58880.0000, 50528.0000,  7560.0000,\n",
      "         3308.0000,        inf, 37472.0000, 10368.0000, 22944.0000, 15792.0000,\n",
      "        64384.0000,  4828.0000,  8624.0000, 28384.0000, 22944.0000, 47424.0000,\n",
      "        38208.0000, 10536.0000, 21376.0000,  8872.0000, 33664.0000,        inf,\n",
      "        16264.0000, 18384.0000, 23952.0000, 11560.0000, 10000.0000, 45600.0000,\n",
      "               inf, 16000.0000, 37344.0000,  9288.0000, 42528.0000, 49472.0000,\n",
      "         4070.0000, 19392.0000,        inf,        inf, 12640.0000, 42208.0000,\n",
      "          189.1250, 53472.0000, 36416.0000, 12224.0000, 60768.0000, 36672.0000,\n",
      "        20448.0000,        inf,  8560.0000, 48064.0000, 15752.0000, 18704.0000,\n",
      "        27392.0000,        inf,        inf, 44160.0000, 30896.0000, 43040.0000,\n",
      "        38656.0000, 10080.0000,  3830.0000,  7780.0000, 49696.0000, 30848.0000,\n",
      "        42240.0000, 20784.0000, 60768.0000, 37664.0000, 19808.0000, 29856.0000,\n",
      "         9152.0000, 30944.0000, 45344.0000,  1129.0000,   777.5000, 22096.0000,\n",
      "        58496.0000, 20368.0000, 37312.0000, 35904.0000, 31776.0000, 27600.0000,\n",
      "        31824.0000, 29456.0000, 51040.0000,  2410.0000, 17680.0000, 30272.0000,\n",
      "        13464.0000,        inf, 53152.0000, 26928.0000, 35488.0000,  6356.0000,\n",
      "               inf,        inf, 41984.0000,  8328.0000,        inf, 23600.0000,\n",
      "         7068.0000,        inf, 36512.0000, 31488.0000, 34560.0000, 11488.0000,\n",
      "               inf, 41120.0000,        inf, 26688.0000, 24272.0000, 50336.0000,\n",
      "        48000.0000, 29248.0000, 62656.0000, 55008.0000,  5644.0000, 11704.0000,\n",
      "        32128.0000, 47584.0000, 26960.0000,  4380.0000,        inf, 47968.0000,\n",
      "        64608.0000,  3070.0000,        inf, 29696.0000, 10208.0000, 31264.0000,\n",
      "        29776.0000, 20784.0000, 61472.0000,  3300.0000, 11072.0000, 12208.0000,\n",
      "         5140.0000,        inf, 62464.0000,        inf, 48480.0000, 25648.0000,\n",
      "         4824.0000,  9440.0000, 23632.0000, 20960.0000, 58720.0000, 37408.0000,\n",
      "         8208.0000, 38048.0000, 27248.0000, 29408.0000, 50944.0000, 38912.0000,\n",
      "        51872.0000, 13280.0000, 40576.0000, 16608.0000, 33152.0000,        inf,\n",
      "         3502.0000, 60480.0000,        inf, 25232.0000, 50336.0000, 56128.0000,\n",
      "        20064.0000, 11456.0000, 28736.0000,        inf,        inf, 46176.0000,\n",
      "               inf,        inf,        inf, 21472.0000, 19936.0000, 50208.0000,\n",
      "        60736.0000,        inf, 52480.0000, 16016.0000, 38240.0000, 11384.0000,\n",
      "        28896.0000,        inf, 16576.0000,  5236.0000, 14504.0000, 48096.0000,\n",
      "        38912.0000, 14168.0000,        inf, 17392.0000, 53280.0000, 25232.0000,\n",
      "        18224.0000, 36864.0000, 35648.0000, 37952.0000, 26960.0000,   407.2500,\n",
      "        13656.0000, 42336.0000,        inf, 58784.0000, 43168.0000, 54944.0000,\n",
      "        23744.0000, 17504.0000, 27680.0000, 36608.0000, 11896.0000, 12344.0000,\n",
      "        65376.0000, 17536.0000,  4376.0000, 25232.0000, 25680.0000, 59072.0000,\n",
      "        37344.0000, 25440.0000, 13632.0000,        inf,  1224.0000, 56416.0000,\n",
      "        24272.0000,  3954.0000, 17616.0000, 47392.0000,        inf, 16064.0000,\n",
      "               inf, 55712.0000, 37440.0000,        inf,        inf, 30448.0000,\n",
      "        35968.0000,        inf,  3576.0000, 17888.0000, 17216.0000, 14168.0000,\n",
      "        20800.0000,  5976.0000, 10736.0000, 23824.0000, 55648.0000, 14320.0000,\n",
      "        28432.0000, 20224.0000,        inf, 52864.0000,        inf,  4668.0000,\n",
      "         3024.0000, 55424.0000, 14024.0000, 51712.0000, 31344.0000, 48960.0000,\n",
      "        15896.0000,        inf, 47936.0000,        inf,        inf, 39264.0000,\n",
      "        41312.0000,  6748.0000,        inf, 57760.0000, 14320.0000,  3724.0000,\n",
      "        16896.0000,  7672.0000,        inf,  3830.0000,  3836.0000, 14008.0000,\n",
      "        23152.0000, 48928.0000, 36096.0000, 37536.0000, 39872.0000,  8152.0000,\n",
      "         7164.0000, 26944.0000, 25104.0000, 16288.0000, 21584.0000,        inf,\n",
      "        16816.0000, 38208.0000,        inf, 23696.0000,  7980.0000, 32448.0000,\n",
      "        25792.0000, 20416.0000, 15952.0000,  1419.0000, 17168.0000, 19440.0000,\n",
      "        31104.0000, 55328.0000, 36832.0000,        inf,  6320.0000, 31904.0000,\n",
      "        55840.0000, 24992.0000, 50624.0000, 43744.0000, 11504.0000,  5012.0000,\n",
      "        65440.0000, 10272.0000, 56928.0000, 60320.0000, 16720.0000, 33120.0000,\n",
      "        30752.0000,  7492.0000,  4420.0000, 34368.0000, 48704.0000,  1499.0000,\n",
      "               inf, 34208.0000,  2560.0000, 29648.0000, 18128.0000,  5000.0000,\n",
      "         9664.0000, 11472.0000,        inf, 51360.0000, 52640.0000, 41024.0000,\n",
      "        58848.0000, 48864.0000], device='cuda:0', dtype=torch.float16), tensor([[10904.,  1914.,  3992.,  ...,  6504.,  4608.,  6944.],\n",
      "        [14000.,  1713.,  8944.,  ...,  7400.,  2944., 14656.],\n",
      "        [21504.,  7352.,   128.,  ...,  5272., 10328.,  9712.],\n",
      "        ...,\n",
      "        [21344., 25680., 23280.,  ...,  8800.,   404., 15888.],\n",
      "        [26896.,  7892., 21008.,  ...,  3480., 27696., 16296.],\n",
      "        [ 1396.,  5904.,  3142.,  ...,  2512.,  1493.,  3500.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([ 6168.,  2956.,  9976.,  ..., 10560.,  8392.,  3782.], device='cuda:0',\n",
      "       dtype=torch.float16), tensor([[ 2112.,  6992., 21104.,  ..., 18000.,  2600.,  4688.],\n",
      "        [ 3414., 16176., 10304.,  ...,  6828., 20160.,  8880.],\n",
      "        [ 3762.,  5952., 13328.,  ...,  4488., 18880.,  9840.],\n",
      "        ...,\n",
      "        [ 1285., 32736., 21120.,  ...,  4600.,  1736.,  8968.],\n",
      "        [ 8864., 35456.,  3568.,  ..., 10800., 12624.,  8084.],\n",
      "        [  586., 44224., 13792.,  ...,  2944.,   248.,  1960.]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([3.1888e+04, 4.0768e+04, 1.7824e+04, 1.6520e+03, 5.2704e+04, 6.3232e+04,\n",
      "        4.2336e+04, 1.9408e+04, 3.1968e+04, 1.4528e+04, 3.9520e+04, 1.4424e+04,\n",
      "        1.7232e+04, 3.4656e+04, 3.1744e+04,        inf, 2.5856e+04, 3.0848e+04,\n",
      "        3.1440e+04,        inf, 9.4000e+03, 5.1160e+03, 6.5024e+04,        inf,\n",
      "        6.7720e+03, 4.5568e+04, 5.2100e+02, 5.6240e+03, 5.8816e+04, 1.8390e+03,\n",
      "        4.3280e+03, 1.2420e+03, 6.6640e+03, 4.4000e+04,        inf, 2.4860e+03,\n",
      "        6.1600e+04, 5.5616e+04,        inf, 3.5616e+04, 5.7248e+04, 5.8656e+04,\n",
      "        4.0672e+04, 2.0944e+04, 6.5200e+03, 1.8832e+04, 5.8432e+04, 2.0272e+04,\n",
      "        1.9968e+04, 7.6200e+03, 9.4160e+03, 3.4112e+04, 1.5896e+04, 2.3456e+04,\n",
      "               inf, 1.9728e+04, 3.1184e+04, 4.6784e+04, 3.2480e+03, 3.6448e+04,\n",
      "        1.4744e+04,        inf, 6.2720e+04, 7.3000e+03, 5.3216e+04, 7.9600e+03,\n",
      "        2.7328e+04, 4.0672e+04, 1.3512e+04, 1.7696e+04, 4.0000e+04,        inf,\n",
      "        5.8400e+04, 4.9504e+04, 5.9360e+04, 6.0384e+04, 3.5980e+03, 5.6000e+02,\n",
      "        1.6304e+04, 2.9776e+04, 1.0024e+04, 5.9712e+04, 4.7080e+03, 5.1328e+04,\n",
      "        9.8560e+03, 1.7824e+04, 1.5568e+04, 1.9376e+04, 3.1008e+04, 2.4624e+04,\n",
      "        1.8960e+04, 3.8940e+03, 4.4800e+04, 5.3760e+04, 2.0928e+04, 4.8768e+04,\n",
      "        5.4528e+04,        inf, 2.1200e+04, 1.4408e+04, 4.3872e+04, 2.5920e+04,\n",
      "        5.3920e+03,        inf, 6.3968e+04, 1.5272e+04, 8.4960e+03, 5.6512e+04,\n",
      "        3.3504e+04, 2.3168e+04, 1.4744e+04, 5.6000e+04, 8.7360e+03, 1.0016e+04,\n",
      "               inf, 2.6112e+04, 2.0544e+04, 1.5456e+04, 4.4032e+04, 6.5280e+03,\n",
      "        2.1072e+04, 2.4448e+04, 6.3232e+04, 3.0352e+04, 2.0176e+04, 1.8400e+04,\n",
      "        3.9200e+04, 1.6752e+04, 3.9296e+04, 5.8464e+04, 4.6880e+04, 3.4400e+04,\n",
      "        5.8848e+04, 2.7760e+04, 5.9168e+04, 1.7552e+04, 1.6080e+04, 5.3824e+04,\n",
      "        5.7728e+04, 5.8464e+04, 2.2496e+04, 2.7536e+04, 9.2880e+03, 6.1120e+04,\n",
      "        3.2432e+04, 4.4864e+04, 5.0144e+04, 4.5696e+04, 3.3952e+04, 2.3616e+04,\n",
      "        3.3216e+04, 1.9424e+04, 2.7216e+04, 6.4928e+04, 2.4848e+04, 1.0032e+04,\n",
      "        5.2704e+04, 5.0976e+04,        inf, 4.5216e+04, 1.6704e+04, 1.9872e+04,\n",
      "        5.0080e+04, 1.7712e+04, 1.6912e+04,        inf, 6.3840e+04, 5.5168e+04,\n",
      "        5.1616e+04, 5.7632e+04, 2.2368e+04, 5.4592e+04, 3.2800e+04, 1.0912e+04,\n",
      "        3.3632e+04,        inf, 5.7720e+03, 1.6608e+04, 1.4960e+03, 1.3352e+04,\n",
      "        3.8944e+04, 2.3344e+04, 2.2208e+04, 2.7280e+04, 2.4096e+04, 4.3968e+04,\n",
      "        3.0496e+04, 2.6544e+04, 1.8912e+04, 4.5840e+03, 5.6736e+04, 5.5392e+04,\n",
      "        1.8448e+04, 5.5808e+04, 1.1048e+04, 1.0712e+04, 1.6224e+04, 4.3072e+04,\n",
      "               inf, 8.7040e+03, 3.2992e+04, 1.7744e+04, 4.7200e+04, 3.7472e+04,\n",
      "        1.4360e+04, 7.7760e+03,        inf,        inf, 1.6528e+04, 4.9088e+04,\n",
      "        9.3750e+02, 4.4512e+04, 5.7824e+04, 2.3664e+04, 6.1792e+04, 3.5968e+04,\n",
      "        2.0336e+04, 4.2304e+04, 2.3392e+04, 2.9024e+04, 2.2928e+04, 5.1744e+04,\n",
      "        2.5984e+04,        inf,        inf, 5.4208e+04, 3.0832e+04, 2.8096e+04,\n",
      "        1.9570e+03, 1.7776e+04, 2.5104e+04, 6.7320e+03, 4.9344e+04, 2.7232e+04,\n",
      "        2.2240e+04, 4.1120e+04,        inf, 6.3264e+04, 1.7312e+04, 4.4832e+04,\n",
      "        9.9600e+03, 2.8048e+04, 4.8672e+04, 2.9584e+04, 1.4240e+04, 3.6280e+03,\n",
      "        3.4944e+04, 1.2310e+03, 3.6160e+04, 4.9024e+04, 2.5952e+04, 4.5824e+04,\n",
      "        1.6160e+04, 2.0624e+04, 5.1392e+04, 1.1136e+04, 2.0256e+04, 2.4816e+04,\n",
      "        2.4688e+04, 5.9456e+04, 5.5232e+04, 1.5432e+04, 3.8400e+04, 2.3696e+04,\n",
      "               inf,        inf, 5.0976e+04, 7.3280e+03,        inf, 2.5584e+04,\n",
      "        2.1712e+04,        inf, 3.4016e+04, 6.4128e+04, 1.9680e+03, 5.1360e+03,\n",
      "               inf, 5.8816e+04, 5.5232e+04, 5.1680e+03, 2.2144e+04, 4.9152e+04,\n",
      "        3.6160e+04, 4.0256e+04, 5.5616e+04, 3.8496e+04, 1.6320e+04, 3.4880e+04,\n",
      "        2.7936e+04,        inf, 5.0840e+03, 1.2720e+04,        inf, 2.1200e+04,\n",
      "               inf, 3.3408e+04,        inf, 5.6736e+04, 1.1976e+04, 3.3632e+04,\n",
      "        1.7184e+04, 3.3632e+04, 4.5376e+04, 4.7880e+03, 8.6000e+03, 1.9616e+04,\n",
      "        1.7296e+04,        inf, 4.0480e+04,        inf, 6.2432e+04, 6.5120e+03,\n",
      "        2.9152e+04, 1.6160e+04, 8.8800e+03, 3.1800e+03, 3.9488e+04, 1.1168e+04,\n",
      "        2.8512e+04, 3.0800e+04, 1.8784e+04, 1.3120e+04, 3.3152e+04, 3.2752e+04,\n",
      "        3.6224e+04, 3.9360e+04, 2.3872e+04, 5.6288e+04, 1.9424e+04,        inf,\n",
      "        1.3264e+04, 5.4656e+04,        inf, 1.5624e+04, 3.9616e+04, 5.0496e+04,\n",
      "        1.3424e+04, 3.0784e+04, 1.1888e+04, 5.7344e+04,        inf, 1.9232e+04,\n",
      "               inf, 5.8848e+04, 4.0608e+04, 2.4448e+04, 2.1328e+04, 3.9456e+04,\n",
      "        5.2224e+04,        inf, 3.5840e+04, 2.2112e+04, 5.5520e+04, 2.8960e+04,\n",
      "        2.7808e+04, 5.5456e+04, 4.4992e+04, 8.9280e+03, 2.0960e+04, 6.3776e+04,\n",
      "        3.2576e+04, 1.8688e+04,        inf, 1.1856e+04, 1.9728e+04, 2.3456e+04,\n",
      "        6.4160e+03, 2.2160e+04, 4.4384e+04, 3.2352e+04, 2.3792e+04, 2.2096e+04,\n",
      "        1.5400e+04, 6.5376e+04,        inf, 6.2496e+04, 6.0128e+04, 3.6576e+04,\n",
      "        1.4968e+04, 9.0480e+03, 1.6152e+04, 3.3824e+04, 1.2592e+04, 4.3424e+04,\n",
      "               inf, 2.2704e+04, 9.9920e+03, 3.1072e+04, 1.2928e+04, 6.4288e+04,\n",
      "        4.1200e+03, 3.0768e+04, 4.0736e+04,        inf, 1.0824e+04, 4.3072e+04,\n",
      "        3.3344e+04, 5.4680e+03, 4.5000e+03, 2.9712e+04,        inf, 3.0300e+03,\n",
      "               inf, 4.6944e+04, 3.2896e+04, 5.6128e+04, 5.5776e+04, 3.3024e+04,\n",
      "        4.4672e+04,        inf, 6.3000e+03, 3.2048e+04, 2.6240e+04, 1.1512e+04,\n",
      "        2.3104e+04, 6.8240e+03, 6.7080e+03, 3.7568e+04, 6.0416e+04, 1.6120e+04,\n",
      "        1.7632e+04, 2.1880e+03,        inf, 3.1824e+04,        inf, 1.2088e+04,\n",
      "        2.1056e+04, 5.4752e+04, 3.4528e+04, 3.0832e+04, 2.9984e+04, 4.2816e+04,\n",
      "        3.5320e+03,        inf, 2.8736e+04,        inf,        inf, 3.5296e+04,\n",
      "        2.4224e+04, 5.4440e+03, 3.2000e+04, 4.3648e+04, 1.1760e+04, 2.3056e+04,\n",
      "        1.9125e+01, 2.1328e+04, 5.7952e+04, 5.7440e+03, 2.3232e+04, 8.8960e+03,\n",
      "        2.1264e+04, 3.9872e+04, 2.6832e+04, 3.7952e+04, 4.2848e+04, 3.3856e+04,\n",
      "        9.7920e+03, 2.4592e+04, 2.7376e+04, 1.8400e+04, 9.7360e+03,        inf,\n",
      "        2.4240e+04, 2.6064e+04,        inf, 5.9200e+03, 2.2496e+04, 4.3104e+04,\n",
      "        8.0875e+01, 1.2328e+04, 1.2696e+04, 1.3952e+04, 1.9440e+04, 2.1296e+04,\n",
      "        4.0420e+03, 3.0816e+04, 4.2320e+03,        inf, 4.0288e+04, 2.8112e+04,\n",
      "        6.3648e+04, 4.2144e+04, 3.8944e+04,        inf, 2.8544e+04, 8.4720e+03,\n",
      "        5.1648e+04, 2.9504e+04, 4.2144e+04,        inf, 3.8592e+04, 4.5240e+03,\n",
      "        2.3872e+04, 3.5420e+03, 1.3728e+04, 1.8400e+04, 5.5200e+04, 1.7520e+04,\n",
      "        5.4752e+04, 4.3584e+04, 1.6944e+04, 3.0848e+04, 1.1128e+04, 1.7520e+04,\n",
      "        1.6710e+03, 3.8176e+04, 5.3120e+04, 5.0240e+04, 6.5440e+04, 2.2672e+04,\n",
      "        5.9840e+04, 4.2784e+04], device='cuda:0', dtype=torch.float16), tensor([31680.0000, 14320.0000, 11328.0000, 18688.0000,  7504.0000, 32384.0000,\n",
      "         5668.0000,  1516.0000, 14448.0000, 10200.0000, 43008.0000,  8560.0000,\n",
      "        21584.0000, 25472.0000, 26480.0000, 34720.0000,  1732.0000,  3082.0000,\n",
      "         3392.0000, 14952.0000, 14720.0000,  4336.0000, 11504.0000, 53120.0000,\n",
      "         8552.0000, 18656.0000, 37920.0000, 11424.0000, 35136.0000, 35680.0000,\n",
      "         8320.0000,  9672.0000,  6524.0000, 23568.0000,  6668.0000, 12880.0000,\n",
      "               inf,  5392.0000, 63776.0000,  3832.0000,  3022.0000,  2610.0000,\n",
      "         9384.0000, 10080.0000, 12280.0000, 20352.0000, 49376.0000, 20048.0000,\n",
      "         3800.0000,  9296.0000,  5660.0000, 21840.0000,  9504.0000,  4094.0000,\n",
      "        54080.0000, 21904.0000, 19360.0000, 19664.0000, 41504.0000,  1502.0000,\n",
      "         2438.0000, 30112.0000,  5608.0000,   341.0000,  8736.0000, 15624.0000,\n",
      "         5376.0000, 56000.0000, 30576.0000, 32864.0000,  8520.0000, 22384.0000,\n",
      "          807.0000, 21280.0000,  9424.0000,        inf, 17920.0000,  4896.0000,\n",
      "         6400.0000, 16296.0000, 25344.0000, 29920.0000, 14328.0000, 19680.0000,\n",
      "          411.7500, 18992.0000,  8376.0000,  4816.0000,  1320.0000, 24720.0000,\n",
      "         6228.0000, 10920.0000,  4356.0000, 12128.0000, 14848.0000, 12912.0000,\n",
      "         7572.0000, 21408.0000, 14976.0000, 24480.0000,  9032.0000,  8968.0000,\n",
      "         2200.0000, 64736.0000, 44544.0000, 16768.0000, 11192.0000, 45088.0000,\n",
      "        35136.0000, 14416.0000, 47264.0000, 11032.0000, 12056.0000, 30768.0000,\n",
      "        15616.0000,  2900.0000, 26768.0000,  7440.0000, 10032.0000, 16944.0000,\n",
      "        11632.0000, 31344.0000, 20288.0000, 11456.0000, 13912.0000, 12928.0000,\n",
      "        33856.0000, 14296.0000,  1623.0000, 32496.0000,   813.0000, 21632.0000,\n",
      "        16912.0000, 39296.0000, 16496.0000, 25248.0000,  6320.0000, 20256.0000,\n",
      "         3494.0000, 24992.0000,  8488.0000, 27616.0000, 27232.0000, 30032.0000,\n",
      "         9752.0000, 23760.0000,  1479.0000, 31136.0000,        inf, 16184.0000,\n",
      "        16480.0000,  5180.0000, 21808.0000, 14064.0000,   157.7500,  1872.0000,\n",
      "        16320.0000, 24480.0000,  8744.0000, 21936.0000,  7540.0000,  6668.0000,\n",
      "        10416.0000,  6216.0000,  2580.0000, 10832.0000, 36512.0000, 44032.0000,\n",
      "        19024.0000,        inf,  4480.0000, 31072.0000, 18352.0000,  4026.0000,\n",
      "        37280.0000, 46080.0000, 13936.0000,   602.0000, 20912.0000,  7324.0000,\n",
      "        43232.0000,  2812.0000, 27040.0000, 18080.0000, 14080.0000, 17792.0000,\n",
      "        22944.0000,  3464.0000,  1361.0000,  3316.0000, 28336.0000, 48832.0000,\n",
      "        15824.0000,  6828.0000, 28992.0000,  7144.0000, 10784.0000, 58144.0000,\n",
      "         1156.0000, 27392.0000,  3532.0000,  5232.0000,  2164.0000, 42336.0000,\n",
      "         1340.0000, 15976.0000, 30752.0000, 29936.0000, 26080.0000,   337.7500,\n",
      "         3170.0000, 23664.0000,  6844.0000,  5788.0000, 53376.0000,  6352.0000,\n",
      "        21984.0000,        inf,  1980.0000, 15432.0000,  8952.0000,  3794.0000,\n",
      "        25760.0000, 50432.0000, 56832.0000,  2688.0000, 28816.0000, 46784.0000,\n",
      "         9032.0000,  6724.0000, 10304.0000, 19232.0000, 28896.0000,  5176.0000,\n",
      "        40256.0000,  8408.0000, 22784.0000,  3408.0000, 51776.0000,  8608.0000,\n",
      "          905.5000, 36608.0000,  7540.0000, 13448.0000,  7828.0000, 13576.0000,\n",
      "        21824.0000,  3258.0000, 10928.0000, 30352.0000, 21056.0000,  8112.0000,\n",
      "        37984.0000, 10200.0000, 17632.0000, 13032.0000, 19152.0000,   668.5000,\n",
      "        29520.0000, 27168.0000,  3840.0000, 25968.0000,  3212.0000, 13624.0000,\n",
      "        46496.0000, 12672.0000, 11712.0000, 11128.0000, 19760.0000,  6932.0000,\n",
      "        24416.0000, 35488.0000, 53440.0000, 25408.0000,  8928.0000, 12288.0000,\n",
      "        37760.0000, 49856.0000,        inf,  3056.0000,   603.5000, 60736.0000,\n",
      "        47872.0000,  2148.0000, 37440.0000, 41280.0000, 26720.0000,  8140.0000,\n",
      "        35328.0000,  7680.0000, 34560.0000,  2460.0000, 47168.0000, 33152.0000,\n",
      "        50016.0000, 19424.0000,        inf, 32464.0000, 38400.0000,  5584.0000,\n",
      "         6964.0000, 22704.0000, 56000.0000, 17600.0000, 18672.0000, 32064.0000,\n",
      "        23520.0000,        inf, 38368.0000,        inf,  7132.0000, 22960.0000,\n",
      "        13520.0000, 28992.0000, 34304.0000, 49792.0000, 11816.0000, 14504.0000,\n",
      "         1755.0000,   761.5000, 12136.0000,  2348.0000, 12080.0000, 21280.0000,\n",
      "         5220.0000,  9032.0000, 29616.0000,  6368.0000, 35712.0000,  2440.0000,\n",
      "        27200.0000, 39680.0000, 55424.0000, 10528.0000,  2846.0000, 36896.0000,\n",
      "         2144.0000, 11424.0000, 23424.0000, 41120.0000, 51968.0000, 44288.0000,\n",
      "        33504.0000, 25824.0000, 14184.0000, 43200.0000, 11192.0000, 26704.0000,\n",
      "               inf, 44544.0000,  6804.0000, 19520.0000, 13984.0000,  1375.0000,\n",
      "        33792.0000, 51232.0000,  2910.0000, 11128.0000,  2430.0000,  1951.0000,\n",
      "        38880.0000,  6132.0000,        inf,  1992.0000, 21824.0000,  5780.0000,\n",
      "        43808.0000, 10952.0000, 23712.0000, 33536.0000,   138.7500, 12176.0000,\n",
      "         3850.0000, 10552.0000, 51232.0000,        inf, 28800.0000, 23920.0000,\n",
      "        11248.0000, 22080.0000, 26560.0000, 61920.0000,  7436.0000,  6740.0000,\n",
      "        38176.0000, 11752.0000,  9336.0000,  1216.0000,  8108.0000,        inf,\n",
      "        11496.0000, 17216.0000,  6920.0000,        inf,  6584.0000, 40224.0000,\n",
      "        18528.0000,  7080.0000,  8712.0000, 13088.0000,   804.0000, 23392.0000,\n",
      "               inf, 58240.0000,        inf,        inf,        inf, 33952.0000,\n",
      "        11792.0000, 38784.0000,  3456.0000, 23504.0000, 18640.0000, 10632.0000,\n",
      "        10896.0000,  1959.0000,  4400.0000, 15552.0000, 29360.0000,  9344.0000,\n",
      "        28592.0000,  3340.0000,        inf, 20528.0000, 39104.0000,  4024.0000,\n",
      "        13336.0000, 20640.0000, 12032.0000, 15264.0000,  2772.0000,  1951.0000,\n",
      "        15064.0000, 43328.0000, 51616.0000, 27776.0000, 19504.0000, 13680.0000,\n",
      "        38464.0000, 21344.0000, 21440.0000, 50464.0000, 12024.0000,  1616.0000,\n",
      "          990.5000, 21504.0000, 56992.0000, 22320.0000,  3940.0000, 10112.0000,\n",
      "        17472.0000, 60352.0000, 17312.0000, 42336.0000, 38656.0000,  4952.0000,\n",
      "        16704.0000, 32000.0000, 23200.0000, 12256.0000, 33600.0000, 46656.0000,\n",
      "          115.5000, 22096.0000,  7244.0000, 18304.0000,  7320.0000, 30224.0000,\n",
      "         5960.0000,  8432.0000, 26656.0000,  1364.0000,  2264.0000, 15424.0000,\n",
      "        11024.0000, 27168.0000, 64096.0000,  3362.0000,  7112.0000, 38112.0000,\n",
      "               inf,  4216.0000, 62528.0000, 50912.0000,  1470.0000,  8992.0000,\n",
      "               inf,  2614.0000, 57760.0000, 47616.0000,  5092.0000, 10344.0000,\n",
      "        47648.0000, 15472.0000, 14256.0000, 31776.0000, 41888.0000, 29712.0000,\n",
      "        21328.0000, 39072.0000,   621.5000, 16704.0000,  1108.0000, 17408.0000,\n",
      "         5716.0000, 14416.0000, 31184.0000, 37984.0000, 47008.0000, 32368.0000,\n",
      "               inf, 10656.0000], device='cuda:0', dtype=torch.float16), tensor([29488.0000, 55584.0000, 11376.0000, 33728.0000, 52128.0000, 60320.0000,\n",
      "        36384.0000, 18608.0000,  6968.0000, 11592.0000, 30944.0000, 30016.0000,\n",
      "        21056.0000, 37728.0000,        inf,        inf, 23904.0000, 17328.0000,\n",
      "         7700.0000, 53472.0000, 17296.0000, 17504.0000,        inf,        inf,\n",
      "        14752.0000, 52224.0000, 30928.0000,  2472.0000,        inf, 37216.0000,\n",
      "         3124.0000,  6088.0000, 20560.0000, 38144.0000,        inf, 15952.0000,\n",
      "               inf, 26832.0000,        inf, 46080.0000,        inf, 51456.0000,\n",
      "        25648.0000, 18144.0000, 17296.0000, 17088.0000,        inf, 22416.0000,\n",
      "        16104.0000,  6772.0000, 21872.0000, 47456.0000, 11136.0000, 21232.0000,\n",
      "        60608.0000, 33952.0000, 20832.0000, 53600.0000, 11488.0000, 17104.0000,\n",
      "        33664.0000,        inf, 64256.0000,  9568.0000, 32368.0000, 33856.0000,\n",
      "        12872.0000, 42784.0000, 22224.0000, 37728.0000,  7724.0000, 56416.0000,\n",
      "        43040.0000, 56864.0000, 42176.0000,        inf,  4772.0000, 18704.0000,\n",
      "        12144.0000, 45856.0000, 36768.0000, 23552.0000, 10736.0000,        inf,\n",
      "         1078.0000, 58880.0000, 30768.0000, 45952.0000, 25904.0000, 34144.0000,\n",
      "        26944.0000,  1566.0000, 31984.0000, 34880.0000,  8776.0000, 29856.0000,\n",
      "        51360.0000, 50144.0000, 19728.0000, 11168.0000, 24192.0000, 24528.0000,\n",
      "          819.5000,        inf, 61184.0000, 19344.0000,  9720.0000, 52288.0000,\n",
      "        63584.0000, 19216.0000, 46912.0000, 33184.0000, 17424.0000, 44736.0000,\n",
      "        43648.0000,  6528.0000,  3366.0000, 37344.0000, 35488.0000, 34464.0000,\n",
      "        13192.0000, 30896.0000, 47072.0000, 37280.0000, 13824.0000, 40640.0000,\n",
      "               inf, 29504.0000, 13872.0000, 54848.0000, 28960.0000, 42144.0000,\n",
      "        31680.0000, 35008.0000, 55008.0000, 27680.0000, 17712.0000, 65312.0000,\n",
      "        59200.0000,        inf, 12936.0000, 16944.0000,  3396.0000,        inf,\n",
      "        30912.0000, 38240.0000, 38528.0000, 29440.0000,        inf, 23584.0000,\n",
      "        30880.0000,  3696.0000, 16024.0000, 43936.0000,  8624.0000,  7328.0000,\n",
      "        42144.0000, 57248.0000,        inf, 42496.0000,  2470.0000, 12904.0000,\n",
      "               inf, 19984.0000, 11368.0000,        inf, 38208.0000,        inf,\n",
      "        52032.0000,        inf, 15488.0000, 63712.0000, 46208.0000, 13680.0000,\n",
      "        10784.0000,        inf, 40448.0000, 10472.0000, 23696.0000, 17760.0000,\n",
      "               inf,  6452.0000, 11008.0000, 28896.0000, 23824.0000, 44224.0000,\n",
      "        43808.0000,  5532.0000, 18512.0000, 19616.0000, 38336.0000,        inf,\n",
      "        10992.0000, 17056.0000, 23888.0000, 12184.0000,  7156.0000, 43072.0000,\n",
      "        64768.0000, 14904.0000, 32080.0000,  7732.0000, 48320.0000, 45504.0000,\n",
      "        12160.0000, 21792.0000,        inf,        inf, 10368.0000, 46656.0000,\n",
      "         5544.0000, 46880.0000, 36896.0000, 14672.0000, 54656.0000, 33504.0000,\n",
      "        22944.0000,        inf,  4284.0000, 51680.0000, 14800.0000, 18848.0000,\n",
      "        29600.0000,        inf,        inf, 46496.0000, 28960.0000, 44736.0000,\n",
      "        40544.0000, 14544.0000,  1042.0000,  3402.0000, 52064.0000, 32384.0000,\n",
      "        40992.0000, 13200.0000, 59552.0000, 41088.0000, 22896.0000, 24000.0000,\n",
      "         8172.0000, 26704.0000, 44384.0000,   788.0000,  6256.0000, 23456.0000,\n",
      "        56224.0000, 17760.0000, 37312.0000, 36352.0000, 25920.0000, 30016.0000,\n",
      "        36256.0000, 22256.0000, 49056.0000,  3824.0000, 24000.0000, 29328.0000,\n",
      "        12832.0000,        inf, 56800.0000, 23504.0000, 25024.0000,  4172.0000,\n",
      "               inf,        inf, 44576.0000,  4180.0000,        inf, 22864.0000,\n",
      "         9224.0000,        inf, 35008.0000, 34528.0000, 31152.0000, 11392.0000,\n",
      "               inf, 43200.0000,        inf, 28832.0000, 25712.0000, 46944.0000,\n",
      "        50752.0000, 21264.0000, 61344.0000, 56160.0000,  5872.0000, 14920.0000,\n",
      "        26048.0000, 48064.0000, 21120.0000,  8760.0000,        inf, 43808.0000,\n",
      "               inf,  8944.0000,        inf, 31200.0000, 16232.0000, 30384.0000,\n",
      "        29024.0000, 20752.0000,        inf,   841.0000,  9032.0000, 16512.0000,\n",
      "         5064.0000,        inf, 57984.0000,        inf, 44224.0000, 32416.0000,\n",
      "         6736.0000, 13048.0000, 21632.0000, 21568.0000, 62944.0000, 33568.0000,\n",
      "         7864.0000, 39168.0000, 35936.0000, 28592.0000, 40512.0000, 40672.0000,\n",
      "        54080.0000,  8944.0000, 41568.0000, 20640.0000, 32480.0000,        inf,\n",
      "         9880.0000, 56864.0000,        inf, 20432.0000, 50368.0000, 64864.0000,\n",
      "        19488.0000, 10736.0000, 25664.0000, 64768.0000,        inf, 43424.0000,\n",
      "               inf,        inf, 61536.0000, 19360.0000, 21984.0000, 51040.0000,\n",
      "        62848.0000,        inf, 57472.0000, 18000.0000, 38592.0000,  9976.0000,\n",
      "        35552.0000,        inf, 20320.0000,  5012.0000, 12200.0000, 48512.0000,\n",
      "        43456.0000,  8240.0000,        inf, 13592.0000, 51456.0000, 16512.0000,\n",
      "        20208.0000, 29520.0000, 29600.0000, 34528.0000, 29440.0000,  1241.0000,\n",
      "        17248.0000, 41536.0000,        inf, 56992.0000, 45248.0000, 52000.0000,\n",
      "        21840.0000, 15576.0000, 27600.0000, 36832.0000, 14024.0000, 13760.0000,\n",
      "        62720.0000, 15776.0000,    89.8750, 25792.0000, 27392.0000, 57088.0000,\n",
      "        42176.0000, 24096.0000, 10888.0000,        inf,  3498.0000, 54080.0000,\n",
      "        24736.0000,  5216.0000, 26080.0000, 46272.0000,        inf, 16480.0000,\n",
      "               inf, 58880.0000, 47424.0000,        inf,        inf, 31280.0000,\n",
      "        39424.0000,        inf,  7780.0000, 22368.0000, 17168.0000, 11152.0000,\n",
      "        15272.0000,  6424.0000,  9104.0000, 18112.0000, 54944.0000,  8328.0000,\n",
      "        30640.0000, 15920.0000,        inf, 51872.0000,        inf,  5136.0000,\n",
      "         3482.0000, 50368.0000, 12464.0000, 52032.0000, 28800.0000, 49760.0000,\n",
      "        17088.0000,        inf, 47328.0000,        inf,        inf, 39776.0000,\n",
      "        41536.0000, 10272.0000,        inf, 52992.0000, 12024.0000,  4364.0000,\n",
      "         9616.0000,  9784.0000, 65376.0000,  2554.0000,  1380.0000,  9304.0000,\n",
      "        22624.0000, 54464.0000, 44256.0000, 36928.0000, 33824.0000,  8416.0000,\n",
      "         4436.0000, 31184.0000, 24864.0000, 15440.0000, 22816.0000,        inf,\n",
      "        13216.0000, 32864.0000,        inf, 25456.0000, 12128.0000, 31984.0000,\n",
      "        22048.0000, 19296.0000, 15984.0000,  3586.0000, 18288.0000, 14336.0000,\n",
      "        28640.0000, 52960.0000, 39264.0000,        inf,  6460.0000, 28288.0000,\n",
      "        57888.0000, 20352.0000, 46208.0000, 42912.0000,  9744.0000,  9608.0000,\n",
      "               inf, 11424.0000, 59296.0000, 57024.0000, 17360.0000, 34688.0000,\n",
      "        31392.0000,  6724.0000,  5296.0000, 39776.0000, 45664.0000,  6568.0000,\n",
      "               inf, 41120.0000,   237.8750, 30400.0000, 11520.0000,  7152.0000,\n",
      "         8088.0000, 11176.0000,        inf, 51904.0000, 52736.0000, 36128.0000,\n",
      "        54400.0000, 47360.0000], device='cuda:0', dtype=torch.float16), tensor([3.6960e+04, 1.1430e+03, 9.9280e+03, 1.6544e+04, 3.2180e+03, 6.0928e+04,\n",
      "        2.8240e+03, 6.5760e+03, 1.5808e+04, 7.8320e+03, 4.8864e+04, 8.2000e+03,\n",
      "        2.6352e+04, 2.6592e+04, 3.5840e+04, 1.9632e+04, 1.8730e+03, 2.9720e+03,\n",
      "        1.1820e+03, 5.0200e+03, 1.5088e+04, 2.0320e+03, 3.9104e+04,        inf,\n",
      "        1.0560e+04, 2.9184e+04, 4.3232e+04, 1.1368e+04, 9.0480e+03, 3.6288e+04,\n",
      "        7.2720e+03, 1.0528e+04, 4.2640e+03, 2.3120e+04, 7.4520e+03, 1.1568e+04,\n",
      "               inf, 3.7840e+03, 3.0064e+04, 1.3784e+04, 1.3888e+04, 7.0440e+03,\n",
      "        1.4744e+04, 1.4696e+04, 9.9920e+03, 1.8160e+04, 5.9488e+04, 2.4256e+04,\n",
      "        5.5600e+03, 8.7600e+03, 8.5120e+03, 3.4752e+04, 4.3500e+01, 8.6720e+03,\n",
      "        6.3552e+04, 5.7080e+03, 2.3600e+04, 1.6176e+04, 3.6032e+04, 3.4960e+03,\n",
      "        9.5520e+03, 2.0272e+04, 1.3320e+04, 4.3200e+02, 1.1104e+04, 1.4176e+04,\n",
      "        3.9360e+03, 4.8608e+04, 2.8144e+04, 3.1008e+04, 6.5280e+03, 3.7408e+04,\n",
      "        5.3200e+03, 3.7152e+04, 6.5560e+03,        inf, 1.6864e+04, 1.0920e+03,\n",
      "        6.7560e+03, 1.7744e+04, 3.0432e+04, 3.3344e+04, 1.4968e+04, 1.5592e+04,\n",
      "        5.2250e+02, 2.8672e+04, 1.8800e+04, 5.0360e+03, 3.6160e+03, 3.0288e+04,\n",
      "        1.0192e+04, 1.0472e+04, 5.0920e+03, 1.3440e+03, 1.0712e+04, 2.4880e+04,\n",
      "        1.4648e+04, 2.2384e+04, 1.8864e+04, 2.4672e+04, 8.8800e+03, 8.8080e+03,\n",
      "        1.7780e+03, 5.7536e+04, 2.7472e+04, 1.6240e+04, 1.0432e+04, 2.6208e+04,\n",
      "        4.3168e+04, 1.7136e+04, 3.6864e+04, 1.5408e+04, 7.0920e+03, 2.1168e+04,\n",
      "        8.9280e+03, 9.5450e+02, 2.5312e+04, 1.3920e+04, 3.4600e+03, 6.4880e+03,\n",
      "        1.3008e+04, 3.7536e+04, 9.7120e+03, 2.8580e+03, 1.8288e+04, 5.5200e+03,\n",
      "        5.1200e+04, 7.9360e+03, 3.4660e+03, 2.4992e+04, 3.8380e+03, 2.4896e+04,\n",
      "        2.3168e+04, 3.9200e+04, 9.5040e+03, 3.5872e+04, 7.5125e+01, 3.6096e+04,\n",
      "        7.3880e+03, 3.6800e+04, 9.0160e+03, 2.8608e+04, 2.7920e+04, 3.6064e+04,\n",
      "        2.3664e+04, 1.7984e+04, 8.9040e+03, 3.1872e+04, 6.2848e+04, 1.8544e+04,\n",
      "        1.6048e+04, 5.6720e+03, 2.6336e+04, 1.2730e+03, 3.0700e+03, 2.7960e+03,\n",
      "        1.6896e+04, 2.6240e+04, 8.4560e+03, 3.2544e+04, 7.3480e+03, 9.5100e+02,\n",
      "        1.4920e+04, 5.1640e+03, 1.5848e+04, 6.8320e+03, 5.5840e+04, 2.5216e+04,\n",
      "        1.0400e+02,        inf, 2.8560e+03, 2.5056e+04, 9.7120e+03, 2.3380e+03,\n",
      "        3.8208e+04,        inf, 1.9968e+04, 2.1760e+03, 2.0352e+04, 5.6480e+03,\n",
      "        4.7360e+04, 2.4500e+03, 3.4400e+04, 2.0256e+04, 1.5960e+04, 1.2584e+04,\n",
      "        2.4752e+04, 7.2760e+03, 2.3240e+03, 5.4800e+02, 4.1472e+04, 5.0240e+04,\n",
      "        1.5664e+04, 3.4280e+03, 1.5192e+04, 4.5360e+03, 1.2176e+04, 6.2432e+04,\n",
      "        3.7620e+03, 3.3664e+04, 9.4000e+03, 1.4960e+03, 8.8800e+02, 4.7872e+04,\n",
      "        1.4330e+03, 1.7536e+04, 2.6912e+04, 1.9808e+04, 2.6608e+04, 3.3560e+03,\n",
      "        2.5100e+03, 4.8256e+04, 1.7620e+03, 1.0912e+04, 5.2672e+04, 2.4575e+02,\n",
      "        1.9424e+04,        inf, 2.9180e+03, 8.9360e+03, 1.0424e+04, 3.7180e+03,\n",
      "        2.4368e+04, 3.7728e+04, 6.1312e+04, 8.2480e+03, 1.3456e+04, 4.9088e+04,\n",
      "        3.0736e+04, 7.8880e+03, 8.0840e+03, 2.4464e+04, 2.9616e+04, 9.4400e+03,\n",
      "        6.2400e+04, 1.6176e+04, 1.8464e+04, 2.0940e+03, 4.4800e+04, 1.3080e+04,\n",
      "        4.1280e+03, 4.0320e+04, 2.7660e+03, 1.3384e+04, 6.7440e+03, 1.7184e+04,\n",
      "        2.1744e+04, 8.5000e+02, 1.2840e+04, 3.5712e+04, 2.5984e+04, 7.6280e+03,\n",
      "        2.5552e+04, 1.0656e+04, 4.8280e+03, 1.2168e+04, 1.4888e+04, 4.2080e+03,\n",
      "        3.2144e+04, 1.9776e+04, 1.1480e+04, 2.0560e+04, 3.0640e+03, 1.2816e+04,\n",
      "        5.6448e+04, 3.7568e+04, 3.8840e+03, 1.1752e+04, 2.2560e+04, 1.5152e+04,\n",
      "        2.3680e+04, 2.1616e+04, 5.8816e+04, 2.9280e+04, 2.8880e+03, 9.3200e+03,\n",
      "        4.9888e+04, 5.2096e+04, 5.8720e+04, 2.4900e+03, 1.3296e+04, 4.6720e+04,\n",
      "        4.7264e+04, 1.2040e+04, 5.5008e+04, 5.8976e+04, 2.7072e+04, 8.2880e+03,\n",
      "        2.5280e+04, 1.6928e+04, 3.9296e+04, 4.4200e+02, 6.1472e+04, 2.7104e+04,\n",
      "        5.4592e+04, 1.6832e+04, 4.5888e+04, 3.3568e+04, 3.5616e+04, 1.7690e+03,\n",
      "        2.0224e+04, 1.6008e+04, 3.8720e+04, 1.9328e+04, 1.7584e+04, 3.2288e+04,\n",
      "        2.1408e+04, 6.3872e+04,        inf, 6.2976e+04, 4.2440e+03, 2.4144e+04,\n",
      "        1.5496e+04, 2.6608e+04, 3.3472e+04, 4.4896e+04, 1.2856e+04, 3.3184e+04,\n",
      "        2.3150e+02, 1.1784e+04, 1.1264e+04, 4.5360e+03, 6.8680e+03, 4.6600e+03,\n",
      "        1.8432e+04, 7.9880e+03, 2.6288e+04, 5.7200e+03, 3.2464e+04, 4.4520e+03,\n",
      "        2.6512e+04, 2.8176e+04, 5.5264e+04, 1.5410e+03, 8.3280e+03, 2.4160e+04,\n",
      "        3.3600e+03, 8.2400e+03, 1.6264e+04, 2.7984e+04, 4.6144e+04, 5.1168e+04,\n",
      "        4.4256e+04, 3.8752e+04, 1.1480e+04, 4.0928e+04, 5.5960e+03, 3.3020e+03,\n",
      "               inf, 4.1952e+04, 2.3520e+04, 1.9776e+04, 1.7568e+04, 5.1550e+02,\n",
      "        2.2016e+04,        inf, 3.2120e+03, 1.2000e+04, 5.1560e+03, 8.1562e+00,\n",
      "        3.0224e+04, 5.4360e+03,        inf, 4.0920e+03, 2.2800e+04, 8.6320e+03,\n",
      "        4.5248e+04, 1.4880e+04, 2.0080e+04, 4.2048e+04, 1.4290e+03, 9.9120e+03,\n",
      "        3.5640e+03, 2.7248e+04,        inf,        inf, 2.8640e+04, 8.4400e+03,\n",
      "        1.8976e+04, 1.8016e+04, 2.4416e+04, 5.7984e+04, 7.3480e+03, 7.4240e+03,\n",
      "        3.3088e+04, 7.4040e+03, 7.1920e+03, 7.5760e+03, 6.0040e+03,        inf,\n",
      "        1.4176e+04, 1.0368e+04, 1.1536e+04,        inf, 5.4800e+03,        inf,\n",
      "        1.7632e+04, 7.7080e+03, 1.0960e+04, 5.7200e+03, 3.3180e+03, 2.9456e+04,\n",
      "               inf, 3.2608e+04,        inf, 6.1120e+04, 5.2832e+04, 4.3392e+04,\n",
      "        4.5520e+03,        inf, 4.1840e+03, 2.6112e+04, 1.7584e+04, 8.0720e+03,\n",
      "        9.2000e+03, 1.5790e+03, 2.4920e+03, 2.0560e+04, 3.0976e+04, 1.0112e+04,\n",
      "        2.1008e+04, 8.7040e+03,        inf, 6.5920e+03, 4.0576e+04, 1.5310e+03,\n",
      "        1.4312e+04, 3.0176e+04, 9.2320e+03, 8.5875e+01, 6.8280e+03, 1.0384e+04,\n",
      "        1.2728e+04,        inf, 4.8352e+04, 2.8640e+04, 4.8512e+04, 1.6152e+04,\n",
      "        4.7392e+04, 1.5960e+04, 1.2664e+04, 5.7632e+04, 1.0288e+04, 4.2720e+03,\n",
      "        7.8450e+02, 2.1104e+04, 2.6944e+04, 2.1152e+04, 2.6040e+03, 7.9200e+03,\n",
      "        1.5448e+04, 6.4064e+04, 1.0912e+04, 5.0144e+04, 4.4512e+04, 3.3600e+03,\n",
      "        1.5400e+04, 3.0048e+04, 1.8448e+04, 1.3200e+04, 4.0256e+04, 6.4864e+04,\n",
      "        5.7600e+03, 2.4384e+04, 4.5840e+03, 1.2368e+04, 7.1680e+03, 3.5520e+04,\n",
      "        1.5536e+04, 1.0384e+04, 2.8064e+04, 1.4380e+03, 1.2240e+03, 1.2096e+04,\n",
      "        1.7632e+04, 4.3744e+04, 5.3376e+04, 1.8848e+04, 5.8320e+03, 4.4672e+04,\n",
      "               inf, 8.9600e+03, 4.9792e+04, 5.3792e+04, 4.0740e+03, 7.7040e+03,\n",
      "        4.8224e+04, 8.6720e+03, 5.7376e+04, 5.0848e+04, 2.9760e+03, 1.1064e+04,\n",
      "        4.0672e+04, 1.2872e+04, 1.5888e+04, 3.1632e+04, 2.9616e+04, 3.0656e+04,\n",
      "        2.0128e+04, 3.9360e+04, 6.6150e+02, 1.4776e+04, 2.4328e+01, 1.7072e+04,\n",
      "        4.9560e+03, 8.4640e+03, 4.4768e+04, 3.2464e+04, 4.4960e+04, 3.4816e+04,\n",
      "               inf, 1.6816e+04], device='cuda:0', dtype=torch.float16), tensor([28880., 54496., 10152., 34944., 54848., 61184., 36960., 19024.,  9560.,\n",
      "        11632., 33088., 29008., 18704., 43040.,    inf,    inf, 23792., 15304.,\n",
      "         8248., 57056., 20272., 19216.,    inf,    inf, 14640., 53888., 28880.,\n",
      "         3004.,    inf, 38272.,  6848.,  6156., 21424., 37280.,    inf, 12096.,\n",
      "           inf, 28864.,    inf, 47136.,    inf, 52384., 28544., 19072., 16816.,\n",
      "        16328.,    inf, 26608., 17312.,  5992., 21568., 49696., 14104., 20352.,\n",
      "        61792., 34720., 23952., 55264., 12280., 19088., 35328.,    inf, 63808.,\n",
      "         6232., 32544., 35360., 12664., 44800., 22192., 38400.,  5840., 59040.,\n",
      "        44480., 55488., 44320.,    inf,  5260., 18880., 10064., 49632., 38240.,\n",
      "        24480., 11920.,    inf,  3912., 59328., 40384., 44640., 25600., 34528.,\n",
      "        30192.,  2060., 31408., 38272.,  9544., 33248., 47232., 52128., 21264.,\n",
      "        12640., 26352., 24864.,  1749.,    inf, 59680., 19760., 10080., 55872.,\n",
      "        65024., 19616., 45344., 33280., 18400., 44064., 50656.,  5596.,  2330.,\n",
      "        35680., 34432., 36320., 12080., 32288., 48448., 40352., 14312., 41280.,\n",
      "           inf, 28576., 18144., 56192., 29712., 43328., 30768., 34944., 57408.,\n",
      "        29824., 17840.,    inf, 59744.,    inf, 15536., 19136.,  4094.,    inf,\n",
      "        31872., 35200., 37984., 26480.,    inf, 21920., 32704.,  4384., 15440.,\n",
      "        42592.,  9264.,  7508., 44864., 57152.,    inf, 43008.,  1725., 11248.,\n",
      "           inf, 21856., 14768.,    inf, 38720.,    inf, 55360.,    inf, 13240.,\n",
      "        62048., 47296., 11040., 12528.,    inf, 42400., 11264., 20480., 18912.,\n",
      "           inf,  5644., 16296., 29392., 27504., 43712., 44320.,  7340., 19264.,\n",
      "        16048., 37824.,    inf, 13240., 16832., 23680., 11496.,  9208., 42560.,\n",
      "           inf, 21152., 33248., 11768., 50080., 45280.,  9648., 20032.,    inf,\n",
      "           inf,  9528., 48448.,  3290., 50944., 39296., 17440., 58432., 36032.,\n",
      "        23616.,    inf,  5292., 50688., 14112., 16640., 30176.,    inf,    inf,\n",
      "        48928., 28496., 46304., 39520., 15304.,  3734.,  5928., 54016., 32752.,\n",
      "        46912., 14296., 62240., 41440., 19616., 24560.,  6576., 26688., 46144.,\n",
      "          592.,  5600., 18960., 58880., 17584., 40288., 38656., 28048., 30784.,\n",
      "        35808., 22416., 51424.,  3216., 19920., 29056., 10064.,    inf, 56160.,\n",
      "        21088., 28944.,  4264.,    inf,    inf, 47328.,  4264.,    inf, 21008.,\n",
      "         9120.,    inf, 33952., 33248., 33024.,  9880.,    inf, 45728.,    inf,\n",
      "        25920., 25904., 47456., 51808., 22352., 64000., 58240.,  5936., 13296.,\n",
      "        26528., 47168., 23824.,  6700.,    inf, 44320.,    inf,  7688.,    inf,\n",
      "        32208., 14456., 32176., 30224., 20016.,    inf,  1323.,  7512., 17904.,\n",
      "         1730.,    inf, 60096.,    inf, 44352., 31712.,  4812., 10816., 20656.,\n",
      "        20080., 65088., 37344.,  4336., 41344., 33344., 27584., 41632., 42496.,\n",
      "        54144., 11048., 42400., 20672., 32768.,    inf,  8960., 61632.,    inf,\n",
      "        23744., 53376., 63808., 18208., 11288., 24816.,    inf,    inf, 41952.,\n",
      "           inf,    inf, 61984., 18624., 21360., 52928., 64704.,    inf, 60384.,\n",
      "        14056., 35904., 10320., 31952.,    inf, 17712.,  4340., 14784., 51168.,\n",
      "        42112., 10568.,    inf, 14464., 51168., 19840., 25264., 31056., 30368.,\n",
      "        35840., 26640.,   829., 16672., 42336.,    inf, 57088., 44864., 51712.,\n",
      "        22912., 13936., 28912., 37824., 14400., 14824., 63776., 14920.,  1461.,\n",
      "        25952., 29488., 58464., 40672., 23200., 15968.,    inf,  5576., 56608.,\n",
      "        27552.,  4472., 22576., 48096.,    inf, 16592.,    inf, 58656., 46048.,\n",
      "           inf,    inf, 32192., 40992.,    inf,  8832., 26496., 15512.,  7376.,\n",
      "        14296.,  7364., 10520., 17952., 58944.,  9320., 35552., 17360.,    inf,\n",
      "        51264.,    inf,  3442.,  6184., 51136., 12632., 53728., 28688., 50144.,\n",
      "        15504.,    inf, 49120.,    inf,    inf, 41472., 41888.,  7116.,    inf,\n",
      "        54496., 12096.,  1062., 12424., 10408.,    inf,  4848.,  1043., 12600.,\n",
      "        23520., 56576., 43840., 38432., 36448.,  9080.,  3238., 29760., 25632.,\n",
      "        14776., 27152.,    inf, 15952., 35264.,    inf, 25552., 11464., 36064.,\n",
      "        23168., 23824., 15800.,  3680., 14592., 14816., 30720., 54752., 38400.,\n",
      "           inf,  7192., 28512., 60832., 22912., 46592., 45344.,  8352.,  8616.,\n",
      "           inf, 15696., 63648., 56352., 18928., 32928., 30224.,  7432.,  5460.,\n",
      "        41536., 44544.,  7168.,    inf, 39808.,  1193., 32000., 14056.,  6312.,\n",
      "         7336., 11928.,    inf, 51680., 55488., 39776., 57984., 48192.],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([43776.0000,  7176.0000,  5756.0000, 21008.0000, 10280.0000, 63840.0000,\n",
      "         8648.0000,   550.5000, 18848.0000,  8848.0000,        inf,  8744.0000,\n",
      "        29632.0000, 15312.0000, 14304.0000, 21184.0000,  2086.0000, 20832.0000,\n",
      "          473.7500,  2428.0000,  4104.0000,   156.2500, 45952.0000,        inf,\n",
      "         2704.0000, 39392.0000, 27664.0000, 12472.0000,  2710.0000, 11536.0000,\n",
      "        12600.0000,  7020.0000,  9896.0000, 31760.0000,   285.0000,  9896.0000,\n",
      "               inf,  3132.0000, 25344.0000,   139.3750,  6484.0000,  7268.0000,\n",
      "        12384.0000, 27712.0000,  7456.0000,  7900.0000,        inf, 20176.0000,\n",
      "         1984.0000, 15104.0000,  6816.0000, 28800.0000, 16464.0000,  7284.0000,\n",
      "               inf,  1648.0000, 25056.0000,  2314.0000, 33888.0000, 26896.0000,\n",
      "         3102.0000, 42112.0000, 20688.0000,  5088.0000, 16480.0000,   648.0000,\n",
      "         8376.0000, 44672.0000, 36384.0000, 21664.0000, 13248.0000, 62720.0000,\n",
      "         2508.0000, 19632.0000,  8544.0000, 32592.0000,  5832.0000,  6632.0000,\n",
      "          309.0000,  5764.0000, 20080.0000, 38944.0000,  4288.0000, 10672.0000,\n",
      "        17472.0000, 20800.0000, 12440.0000,  7192.0000,  9056.0000, 32800.0000,\n",
      "        11984.0000,  8688.0000,  2840.0000, 26592.0000, 18352.0000, 44544.0000,\n",
      "          148.2500, 29648.0000, 33600.0000, 22368.0000,  7628.0000,  2950.0000,\n",
      "         5472.0000, 39872.0000, 58528.0000,  7568.0000,  4176.0000, 35168.0000,\n",
      "        28496.0000, 19344.0000, 12888.0000, 19136.0000,  3746.0000, 17152.0000,\n",
      "        16360.0000,  1534.0000, 16512.0000,  5868.0000, 18112.0000,  8728.0000,\n",
      "        12736.0000, 12800.0000,   821.5000,  2007.0000, 12544.0000,  7460.0000,\n",
      "        32144.0000,   264.7500,  3776.0000, 28256.0000, 16752.0000,  2490.0000,\n",
      "        30000.0000, 21136.0000, 11976.0000, 20720.0000,  1172.0000, 27584.0000,\n",
      "        11272.0000, 32144.0000, 12320.0000, 29440.0000, 15728.0000, 27872.0000,\n",
      "        26688.0000,  3166.0000,  1957.0000, 45984.0000, 19776.0000, 17744.0000,\n",
      "        16880.0000,  5380.0000, 23264.0000, 11720.0000, 11200.0000,  3086.0000,\n",
      "        17472.0000, 13488.0000, 12504.0000, 20848.0000,  5204.0000,  6580.0000,\n",
      "        18640.0000,  5388.0000,  5516.0000, 27584.0000,        inf, 15528.0000,\n",
      "        15128.0000,        inf, 12096.0000, 33344.0000, 17136.0000,  2074.0000,\n",
      "        18048.0000,        inf,  5480.0000,  3158.0000, 25120.0000, 12640.0000,\n",
      "        43776.0000,   791.0000, 36960.0000, 11704.0000,   669.0000, 14432.0000,\n",
      "        14648.0000, 36832.0000, 11376.0000,  7528.0000, 53600.0000, 20480.0000,\n",
      "         5904.0000, 13648.0000,  3488.0000,  1825.0000, 17584.0000, 62816.0000,\n",
      "          690.5000, 22144.0000,  1847.0000,  1800.0000, 20784.0000, 46336.0000,\n",
      "          279.0000,  1289.0000, 25744.0000, 12312.0000, 42016.0000,  4796.0000,\n",
      "         5724.0000, 40640.0000,  1000.0000,  3592.0000, 49600.0000, 17152.0000,\n",
      "        21680.0000, 61856.0000, 15440.0000, 21488.0000, 12088.0000,  9128.0000,\n",
      "        23312.0000, 19696.0000, 17472.0000,  1027.0000, 23776.0000, 29888.0000,\n",
      "         3640.0000,  1436.0000,   225.1250,  2168.0000, 47104.0000, 10824.0000,\n",
      "        38784.0000, 54208.0000, 28208.0000, 21200.0000, 35744.0000, 28528.0000,\n",
      "          499.0000, 31312.0000,  4144.0000, 13616.0000,  2348.0000,  2448.0000,\n",
      "         3192.0000,  4280.0000,  6880.0000, 43360.0000, 21216.0000, 16072.0000,\n",
      "        17968.0000, 11832.0000,  2706.0000, 13232.0000, 18000.0000, 14016.0000,\n",
      "        35552.0000, 22784.0000, 22800.0000,  9360.0000, 15320.0000, 13808.0000,\n",
      "               inf, 45440.0000,  4612.0000,  1347.0000,  9576.0000, 28672.0000,\n",
      "        13112.0000, 31776.0000, 52768.0000,        inf, 10152.0000, 12112.0000,\n",
      "        48640.0000, 42112.0000, 51072.0000, 12352.0000, 18432.0000, 30944.0000,\n",
      "        49056.0000, 34496.0000, 56768.0000, 53216.0000, 16512.0000,  9072.0000,\n",
      "        35104.0000, 27440.0000,  9408.0000, 12944.0000,        inf,  7648.0000,\n",
      "               inf, 16832.0000, 58464.0000, 46592.0000, 26592.0000,  4320.0000,\n",
      "        21904.0000, 21408.0000, 17248.0000, 11672.0000, 31936.0000, 38176.0000,\n",
      "        23056.0000,        inf, 44768.0000, 65088.0000,  1376.0000, 13696.0000,\n",
      "        24224.0000, 13400.0000,  9064.0000, 23104.0000,  7500.0000, 12296.0000,\n",
      "        11360.0000, 14240.0000, 18080.0000,   886.0000, 23888.0000, 22800.0000,\n",
      "        18208.0000, 26960.0000, 19984.0000, 15920.0000, 21824.0000, 24000.0000,\n",
      "        17920.0000, 31456.0000,        inf,  2588.0000, 10416.0000,  2594.0000,\n",
      "        11272.0000,   115.0000,  3536.0000,  7056.0000, 53760.0000, 25888.0000,\n",
      "        32160.0000, 22112.0000,  3738.0000, 50560.0000,  2624.0000, 10648.0000,\n",
      "        62240.0000, 62784.0000, 15040.0000, 46304.0000, 25744.0000, 33824.0000,\n",
      "        27200.0000,        inf, 12064.0000, 14648.0000,  4160.0000, 16352.0000,\n",
      "        27376.0000,  5844.0000,        inf,  2946.0000,  8184.0000, 11808.0000,\n",
      "        22752.0000,  1796.0000, 47488.0000, 30464.0000,   596.0000, 12848.0000,\n",
      "         2832.0000, 60704.0000,        inf,        inf, 29360.0000, 15944.0000,\n",
      "        11616.0000, 10536.0000,  6968.0000, 57760.0000,  3226.0000, 60192.0000,\n",
      "        43488.0000,  1612.0000,  3608.0000, 28416.0000,  1637.0000,        inf,\n",
      "         4224.0000, 35904.0000, 41376.0000,        inf, 15104.0000, 42656.0000,\n",
      "        27232.0000,  3404.0000,  4616.0000, 15632.0000,  7736.0000,  4038.0000,\n",
      "               inf, 28112.0000, 48864.0000, 35776.0000, 40160.0000, 31808.0000,\n",
      "        23184.0000,        inf,  6520.0000, 30784.0000, 23120.0000, 17072.0000,\n",
      "         6464.0000, 11512.0000,  1287.0000, 20144.0000, 17616.0000,  2396.0000,\n",
      "        15176.0000,  8296.0000, 53632.0000,  1674.0000, 26496.0000, 14488.0000,\n",
      "        11544.0000, 16496.0000,  4200.0000,   566.0000,  9200.0000, 22400.0000,\n",
      "         2622.0000, 51488.0000, 31312.0000,   358.0000, 61440.0000,  9872.0000,\n",
      "         3212.0000,  3964.0000, 18544.0000, 21344.0000,  9080.0000, 20480.0000,\n",
      "        13536.0000, 35168.0000, 35328.0000, 25488.0000, 19104.0000,  7024.0000,\n",
      "        18368.0000, 39072.0000, 11784.0000, 59296.0000, 54560.0000, 13968.0000,\n",
      "        28096.0000, 29008.0000, 11504.0000, 22240.0000, 12328.0000, 38176.0000,\n",
      "        14024.0000, 19008.0000, 28336.0000,  2986.0000, 12104.0000, 58688.0000,\n",
      "         8848.0000,  6692.0000, 24960.0000, 18208.0000,  9896.0000, 26544.0000,\n",
      "         6608.0000, 23472.0000,  8944.0000, 13000.0000, 56800.0000, 38784.0000,\n",
      "               inf,   737.0000, 35808.0000, 59168.0000,  5912.0000,  8872.0000,\n",
      "        47648.0000, 12152.0000, 56864.0000, 35392.0000,  7624.0000,  6400.0000,\n",
      "        32960.0000,  6788.0000, 20768.0000, 17984.0000, 33280.0000, 29696.0000,\n",
      "         9584.0000, 34944.0000, 20768.0000, 11848.0000,  2184.0000,  2514.0000,\n",
      "         1193.0000,  4740.0000, 18272.0000, 42848.0000, 56768.0000, 28576.0000,\n",
      "        28208.0000,  6740.0000], device='cuda:0', dtype=torch.float16), tensor([32640.0000, 45312.0000, 12800.0000,  4644.0000, 52640.0000,        inf,\n",
      "        44544.0000, 26032.0000, 28960.0000, 13440.0000, 43648.0000, 12760.0000,\n",
      "        14672.0000, 36800.0000, 33440.0000,        inf, 20608.0000, 33856.0000,\n",
      "        29856.0000,        inf,  8496.0000,  6364.0000, 64160.0000,        inf,\n",
      "         5352.0000, 47488.0000,  1923.0000,  5448.0000, 65088.0000,  2334.0000,\n",
      "         1888.0000,  1018.5000,  7224.0000, 42624.0000,        inf,  5156.0000,\n",
      "        64480.0000, 59520.0000,        inf, 43008.0000, 61376.0000, 62208.0000,\n",
      "        39872.0000, 19088.0000,  7632.0000, 18592.0000, 59072.0000, 26432.0000,\n",
      "        17280.0000,  7996.0000,  9552.0000, 36448.0000, 19088.0000, 23568.0000,\n",
      "               inf, 15216.0000, 31808.0000, 45728.0000,  1528.0000, 36352.0000,\n",
      "        23952.0000,        inf,        inf,  7956.0000, 53344.0000, 11272.0000,\n",
      "        25792.0000, 46592.0000, 17168.0000, 18480.0000, 36256.0000,        inf,\n",
      "        59200.0000, 52992.0000,        inf, 59616.0000,  5856.0000,   902.0000,\n",
      "        18560.0000, 31376.0000,  7728.0000, 63200.0000,  3912.0000, 56192.0000,\n",
      "        15280.0000, 17136.0000, 17120.0000, 14536.0000, 37792.0000, 30144.0000,\n",
      "        22128.0000,  3298.0000, 42560.0000, 55776.0000, 22128.0000, 51232.0000,\n",
      "        60864.0000,        inf, 21936.0000, 14128.0000, 45184.0000, 26288.0000,\n",
      "         7376.0000,        inf,        inf, 16288.0000,  5740.0000, 61632.0000,\n",
      "        31824.0000, 15680.0000, 14616.0000, 50656.0000,  5804.0000,  5580.0000,\n",
      "               inf, 33600.0000, 22320.0000, 15568.0000, 48064.0000,  3232.0000,\n",
      "        23680.0000, 19808.0000,        inf, 33440.0000, 19008.0000, 22512.0000,\n",
      "        42816.0000, 21024.0000, 33984.0000, 61856.0000, 47328.0000, 29968.0000,\n",
      "        62560.0000, 23392.0000, 65312.0000, 17328.0000, 16432.0000, 53248.0000,\n",
      "               inf, 65472.0000, 24688.0000, 31888.0000, 10008.0000,        inf,\n",
      "        35104.0000, 47488.0000, 53600.0000, 51200.0000, 41184.0000, 24032.0000,\n",
      "        37568.0000, 20240.0000, 30400.0000,        inf, 27776.0000, 13776.0000,\n",
      "        52480.0000, 56544.0000,        inf, 50176.0000, 19264.0000, 14792.0000,\n",
      "        57888.0000, 18832.0000, 15672.0000,        inf,        inf, 55712.0000,\n",
      "        57088.0000, 59584.0000, 21936.0000, 53600.0000, 33440.0000, 14168.0000,\n",
      "        29712.0000,        inf,  8512.0000, 22768.0000,  2636.0000, 17744.0000,\n",
      "        41952.0000, 24224.0000, 22320.0000, 30944.0000, 25408.0000, 51168.0000,\n",
      "        32544.0000, 33888.0000, 15512.0000,  2848.0000, 55552.0000, 55168.0000,\n",
      "        19040.0000, 58528.0000, 15184.0000,  9312.0000, 16048.0000, 42752.0000,\n",
      "               inf,  9288.0000, 31616.0000, 22112.0000, 50048.0000, 42048.0000,\n",
      "        14624.0000, 10920.0000,        inf,        inf, 21568.0000, 52448.0000,\n",
      "          719.0000, 49248.0000, 58528.0000, 27568.0000, 65088.0000, 34304.0000,\n",
      "        17632.0000, 46272.0000, 17776.0000, 27584.0000, 28048.0000, 57664.0000,\n",
      "        25376.0000,        inf,        inf, 58816.0000, 27440.0000, 31136.0000,\n",
      "         2718.0000, 17888.0000, 25296.0000, 10520.0000, 54048.0000, 26832.0000,\n",
      "        25712.0000, 45600.0000,        inf, 65408.0000, 21232.0000, 47008.0000,\n",
      "        12872.0000, 30160.0000, 50816.0000, 31504.0000, 10368.0000,  2718.0000,\n",
      "        43520.0000,  2008.0000, 39616.0000, 51808.0000, 28784.0000, 45984.0000,\n",
      "        19040.0000, 21184.0000, 49600.0000, 13216.0000, 24448.0000, 21888.0000,\n",
      "        23888.0000, 64576.0000, 55584.0000, 17312.0000, 38112.0000, 25232.0000,\n",
      "               inf,        inf, 56672.0000, 13408.0000,        inf, 26112.0000,\n",
      "        20032.0000,        inf, 37280.0000, 65408.0000,  1788.0000,  5320.0000,\n",
      "               inf, 61280.0000, 56288.0000,  5332.0000, 22800.0000, 44864.0000,\n",
      "        39168.0000, 38720.0000, 57824.0000, 46880.0000, 17312.0000, 38048.0000,\n",
      "        28256.0000,        inf,  4460.0000, 13936.0000,        inf, 22560.0000,\n",
      "               inf, 29616.0000,        inf, 59136.0000, 11472.0000, 31712.0000,\n",
      "        13696.0000, 34368.0000, 45600.0000,  8968.0000, 10072.0000, 17808.0000,\n",
      "        20272.0000,        inf, 43008.0000,        inf,        inf,  2039.0000,\n",
      "        31728.0000, 16880.0000, 10544.0000,  7456.0000, 41792.0000, 11504.0000,\n",
      "        28912.0000, 35168.0000, 21488.0000, 14048.0000, 38304.0000, 33152.0000,\n",
      "        39200.0000, 42816.0000, 29920.0000, 55712.0000, 23152.0000,        inf,\n",
      "         9264.0000, 59008.0000,        inf, 22976.0000, 41472.0000, 56320.0000,\n",
      "        13288.0000, 34272.0000, 11464.0000, 59904.0000,        inf, 22672.0000,\n",
      "               inf, 64256.0000, 45056.0000, 24144.0000, 19760.0000, 42944.0000,\n",
      "        51328.0000,        inf, 38464.0000, 21648.0000, 57600.0000, 33760.0000,\n",
      "        28832.0000, 62880.0000, 39488.0000, 10552.0000, 24048.0000,        inf,\n",
      "        34176.0000, 20960.0000,        inf,  7488.0000, 22336.0000, 22064.0000,\n",
      "         9088.0000, 21664.0000, 49280.0000, 32928.0000, 30256.0000, 17536.0000,\n",
      "        18736.0000,        inf,        inf, 64448.0000, 63136.0000, 38432.0000,\n",
      "        12288.0000, 15536.0000, 18672.0000, 29408.0000, 16152.0000, 45408.0000,\n",
      "               inf, 20016.0000,  9048.0000, 37664.0000, 12336.0000,        inf,\n",
      "         1691.0000, 33408.0000, 44544.0000,        inf,  9704.0000, 43968.0000,\n",
      "        34176.0000,  5956.0000,  3674.0000, 34400.0000,        inf,  3210.0000,\n",
      "               inf, 53280.0000, 31968.0000,        inf, 62272.0000, 31744.0000,\n",
      "        44512.0000,        inf,  4732.0000, 35872.0000, 28928.0000, 15264.0000,\n",
      "        20512.0000,   759.0000,  5144.0000, 39680.0000, 63488.0000, 18416.0000,\n",
      "        24320.0000,  3690.0000,        inf, 39584.0000,        inf, 16576.0000,\n",
      "        24560.0000, 54016.0000, 31680.0000, 28368.0000, 31072.0000, 42592.0000,\n",
      "         9856.0000,        inf, 31616.0000,        inf,        inf, 34496.0000,\n",
      "        21856.0000,  3040.0000, 36928.0000, 41184.0000,  9064.0000, 18528.0000,\n",
      "         2222.0000, 21040.0000, 65312.0000,  4972.0000, 21344.0000, 10560.0000,\n",
      "        21856.0000, 44032.0000, 27584.0000, 34880.0000, 42464.0000, 37152.0000,\n",
      "        13480.0000, 26864.0000, 27280.0000, 26464.0000, 11432.0000,        inf,\n",
      "        22704.0000, 21104.0000,        inf,  6284.0000, 23968.0000, 49472.0000,\n",
      "          218.6250, 15528.0000, 15752.0000, 12496.0000, 19744.0000, 22288.0000,\n",
      "        12616.0000, 31888.0000,  2796.0000,        inf, 38752.0000, 28256.0000,\n",
      "               inf, 47488.0000, 37440.0000,        inf, 27984.0000, 12464.0000,\n",
      "        50688.0000, 33696.0000, 46432.0000,        inf, 39584.0000,   302.0000,\n",
      "        26576.0000,  6320.0000, 11632.0000, 20224.0000, 57280.0000, 15928.0000,\n",
      "        53280.0000, 46624.0000, 17712.0000, 32400.0000,  7936.0000, 13224.0000,\n",
      "          408.0000, 40032.0000, 52960.0000, 53440.0000,        inf, 22688.0000,\n",
      "        63520.0000, 37152.0000], device='cuda:0', dtype=torch.float16), tensor([35712.0000,  7196.0000,  3146.0000, 25600.0000,  9576.0000, 56928.0000,\n",
      "         8824.0000,  1279.0000, 28240.0000, 10448.0000, 57856.0000,  7576.0000,\n",
      "        26544.0000,  9232.0000, 17536.0000, 25248.0000,  9336.0000, 18896.0000,\n",
      "         1210.0000,  2294.0000,  3166.0000,  2058.0000, 43648.0000,        inf,\n",
      "         3998.0000, 41760.0000, 21008.0000, 16976.0000,  1142.0000, 14256.0000,\n",
      "         4360.0000,  5476.0000,  1732.0000, 25536.0000,  2452.0000,  9760.0000,\n",
      "               inf,  5972.0000, 24032.0000,  3964.0000, 10512.0000,  5012.0000,\n",
      "        10352.0000, 32656.0000,  6428.0000, 11640.0000,        inf, 20176.0000,\n",
      "         1675.0000, 15536.0000,  8888.0000, 18496.0000, 17056.0000,  6636.0000,\n",
      "               inf,   947.0000, 25056.0000,  3226.0000, 31120.0000, 24704.0000,\n",
      "         3338.0000, 43584.0000, 20384.0000,  3564.0000, 18304.0000,  9960.0000,\n",
      "        14592.0000, 52992.0000, 38336.0000, 22384.0000, 12368.0000,        inf,\n",
      "         6488.0000, 20832.0000,  9376.0000, 33696.0000,  5620.0000,  6256.0000,\n",
      "          332.0000,  2086.0000, 18272.0000, 46720.0000,  9656.0000, 11384.0000,\n",
      "        15616.0000, 18256.0000, 15456.0000,  5552.0000,  8528.0000, 35872.0000,\n",
      "         9304.0000, 12848.0000,  7348.0000, 22400.0000, 16448.0000, 45472.0000,\n",
      "         1626.0000, 27632.0000, 32960.0000, 22736.0000,  5464.0000,  2446.0000,\n",
      "         5384.0000, 42080.0000,        inf,  5568.0000,  4300.0000, 36608.0000,\n",
      "        23584.0000, 24912.0000,  3916.0000, 17264.0000,  6720.0000, 19456.0000,\n",
      "        15648.0000,   542.5000, 17904.0000,  2974.0000, 21056.0000,  8368.0000,\n",
      "        13056.0000, 12984.0000,  3252.0000,  2492.0000, 13040.0000,  7340.0000,\n",
      "        27952.0000,  1399.0000,  1726.0000, 30288.0000, 15928.0000,  1079.0000,\n",
      "        26976.0000,  8408.0000, 11912.0000, 18752.0000,  2596.0000, 26720.0000,\n",
      "        10304.0000, 33472.0000, 12680.0000, 26416.0000, 12848.0000, 30192.0000,\n",
      "        26768.0000,  4028.0000,  3098.0000, 48832.0000, 19504.0000, 19040.0000,\n",
      "        14664.0000,  4464.0000, 24112.0000, 12216.0000, 16232.0000,  4592.0000,\n",
      "        18096.0000, 12480.0000, 12328.0000, 21376.0000,  3408.0000,  4984.0000,\n",
      "        19664.0000,  6072.0000,  6172.0000, 29136.0000,        inf, 15712.0000,\n",
      "        15384.0000,        inf, 13408.0000, 34752.0000, 16432.0000,  2054.0000,\n",
      "        18288.0000,        inf,  4540.0000,  2218.0000, 22192.0000, 16592.0000,\n",
      "        43680.0000,  1120.0000, 30768.0000, 13520.0000,  1811.0000, 17296.0000,\n",
      "         9824.0000, 38432.0000,  7876.0000,  8360.0000, 51232.0000, 21888.0000,\n",
      "         4748.0000, 13864.0000,  2892.0000,  2512.0000, 17872.0000, 57152.0000,\n",
      "          303.7500, 30656.0000,   596.0000,  2256.0000, 21280.0000, 45920.0000,\n",
      "          625.5000,   668.5000, 26672.0000, 15120.0000, 48544.0000,  4588.0000,\n",
      "         6040.0000, 43168.0000,  2037.0000,  5072.0000, 46112.0000, 21808.0000,\n",
      "        21664.0000,        inf, 22672.0000, 22576.0000,  7364.0000,  9392.0000,\n",
      "        15832.0000, 19904.0000, 19024.0000,   149.0000, 19552.0000, 31440.0000,\n",
      "         1876.0000,   775.0000,  3726.0000,  4876.0000, 51040.0000, 10344.0000,\n",
      "        55136.0000, 59904.0000, 26608.0000, 24784.0000, 37024.0000, 29920.0000,\n",
      "          280.0000, 30800.0000,  3036.0000, 11832.0000,  4856.0000,   280.5000,\n",
      "         4520.0000,  8496.0000,  9504.0000, 33856.0000, 26640.0000, 17840.0000,\n",
      "        18272.0000, 10816.0000,   582.0000, 17728.0000, 18624.0000, 11448.0000,\n",
      "        45824.0000, 22784.0000, 19520.0000,  8256.0000, 19520.0000, 13624.0000,\n",
      "               inf, 41952.0000,   239.5000,  5752.0000,  9760.0000, 26240.0000,\n",
      "        13072.0000, 33312.0000, 45728.0000,        inf,  8632.0000, 13776.0000,\n",
      "        46656.0000, 46336.0000, 49088.0000, 18992.0000, 21216.0000, 38848.0000,\n",
      "        43744.0000, 33984.0000, 52128.0000, 59744.0000, 16280.0000,  5920.0000,\n",
      "        37120.0000, 25968.0000,  6916.0000, 12120.0000,        inf, 10304.0000,\n",
      "               inf, 20016.0000, 60160.0000, 49600.0000, 19312.0000,   655.5000,\n",
      "        19648.0000, 17712.0000, 32048.0000,  7824.0000, 37824.0000, 41408.0000,\n",
      "        24736.0000,        inf, 42208.0000, 52384.0000,  7288.0000,  1558.0000,\n",
      "        19792.0000, 14016.0000,   488.5000, 16136.0000, 10280.0000, 11072.0000,\n",
      "        16672.0000, 12296.0000, 12392.0000,   808.5000, 26944.0000, 23376.0000,\n",
      "        22192.0000, 30880.0000, 17120.0000, 15216.0000, 22352.0000, 25072.0000,\n",
      "        20048.0000, 33664.0000,        inf,  1492.0000, 11752.0000,  1342.0000,\n",
      "        13080.0000,   490.7500,  2454.0000,  6840.0000, 52320.0000, 30736.0000,\n",
      "        33696.0000, 21808.0000,  2726.0000, 44000.0000,  2634.0000, 11736.0000,\n",
      "               inf, 62080.0000, 19152.0000, 62336.0000, 25968.0000, 38304.0000,\n",
      "        27536.0000, 64544.0000, 14880.0000, 15328.0000,  7528.0000, 18112.0000,\n",
      "        18736.0000,  5528.0000,        inf,  2960.0000,  3478.0000, 10496.0000,\n",
      "        18144.0000,   673.0000, 50048.0000, 33376.0000,  6980.0000, 12912.0000,\n",
      "          933.0000, 65184.0000,        inf,        inf, 30128.0000, 16184.0000,\n",
      "        10296.0000, 10184.0000,  3782.0000, 62208.0000,  5084.0000, 48928.0000,\n",
      "        43168.0000,  4260.0000,  2124.0000, 18576.0000,  2942.0000,        inf,\n",
      "          181.2500, 40320.0000, 34784.0000,        inf, 15848.0000, 58592.0000,\n",
      "        29280.0000,  1917.0000,  6076.0000, 18992.0000,  6972.0000,  5840.0000,\n",
      "               inf, 26384.0000, 60768.0000, 33280.0000, 40288.0000, 36576.0000,\n",
      "        21776.0000,        inf, 10112.0000, 28368.0000, 28240.0000, 16560.0000,\n",
      "         4212.0000, 13888.0000,   146.5000, 23184.0000, 17248.0000,  1404.0000,\n",
      "        14632.0000,   485.5000, 51680.0000,  2912.0000, 27360.0000, 20992.0000,\n",
      "        13816.0000, 16736.0000,  5268.0000,  1445.0000, 10552.0000, 22992.0000,\n",
      "         6940.0000, 47232.0000, 36128.0000,  1846.0000, 57984.0000,  8704.0000,\n",
      "         6884.0000, 11088.0000, 18928.0000, 18176.0000,  9248.0000, 20064.0000,\n",
      "        12400.0000, 38464.0000, 38304.0000, 23952.0000, 21840.0000,  6864.0000,\n",
      "        21840.0000, 41440.0000, 12208.0000, 56928.0000, 63584.0000, 12928.0000,\n",
      "        27152.0000, 29568.0000, 22704.0000, 24976.0000, 20416.0000, 39776.0000,\n",
      "        12464.0000, 15328.0000, 25040.0000,  1192.0000,  9424.0000, 53120.0000,\n",
      "        11048.0000,  5312.0000, 18480.0000, 23536.0000,  8416.0000, 28224.0000,\n",
      "         5476.0000, 25472.0000,  1771.0000, 14448.0000,        inf, 26048.0000,\n",
      "               inf,  1880.0000, 40544.0000, 59776.0000,  4796.0000, 10408.0000,\n",
      "        46656.0000,  9792.0000, 51904.0000, 33600.0000,  5852.0000,  5432.0000,\n",
      "        38976.0000,  5468.0000, 37184.0000, 22192.0000, 37536.0000, 27488.0000,\n",
      "        10688.0000, 39232.0000, 17568.0000, 11984.0000,   801.0000,  2952.0000,\n",
      "         1514.0000,  1643.0000, 16200.0000, 42432.0000, 58944.0000, 31952.0000,\n",
      "        29568.0000,  7152.0000], device='cuda:0', dtype=torch.float16), tensor([23456.0000, 49184.0000,  1842.0000, 11720.0000, 48448.0000, 63168.0000,\n",
      "        49472.0000, 27232.0000, 39584.0000,  5084.0000, 32864.0000, 15416.0000,\n",
      "        11688.0000, 26800.0000, 34304.0000,        inf, 27536.0000, 28640.0000,\n",
      "        25728.0000, 65472.0000, 11680.0000,   612.0000, 64480.0000,        inf,\n",
      "        15144.0000, 50496.0000,  9184.0000, 10576.0000,        inf,  3808.0000,\n",
      "         8000.0000,   357.2500, 15368.0000, 37056.0000,        inf,  6768.0000,\n",
      "        61280.0000, 53376.0000,        inf, 35136.0000, 58720.0000, 58368.0000,\n",
      "        37696.0000, 19904.0000, 12384.0000, 16200.0000, 58560.0000, 25552.0000,\n",
      "        18688.0000,  7972.0000,  5960.0000, 30832.0000, 17392.0000, 24432.0000,\n",
      "               inf, 15184.0000, 29872.0000, 43424.0000,  2128.0000, 37280.0000,\n",
      "        25440.0000,        inf,        inf,  6764.0000, 50912.0000, 18576.0000,\n",
      "        32464.0000, 53568.0000, 10496.0000, 17552.0000, 38400.0000,        inf,\n",
      "        60448.0000, 55616.0000,        inf, 60096.0000,  3628.0000,  5220.0000,\n",
      "        19408.0000, 24384.0000,  6296.0000, 64832.0000,  8044.0000, 53504.0000,\n",
      "        15264.0000, 19424.0000, 19520.0000, 22224.0000, 35616.0000, 33408.0000,\n",
      "        11448.0000, 13376.0000, 35456.0000, 49152.0000, 15744.0000, 48672.0000,\n",
      "        56160.0000,        inf, 20784.0000, 15936.0000, 39968.0000, 25216.0000,\n",
      "        10712.0000,        inf,        inf, 15632.0000,  7512.0000, 59424.0000,\n",
      "        28816.0000, 20608.0000,  6808.0000, 47424.0000,  6016.0000,  7604.0000,\n",
      "               inf, 37280.0000, 23504.0000,  9136.0000, 57280.0000,   359.0000,\n",
      "        24768.0000, 19520.0000, 64128.0000, 21888.0000, 20848.0000, 21424.0000,\n",
      "        38752.0000, 22768.0000, 28352.0000, 64992.0000, 45504.0000, 20464.0000,\n",
      "        63936.0000,  9368.0000, 57024.0000, 14896.0000, 17248.0000, 51936.0000,\n",
      "               inf, 63616.0000, 34912.0000, 25888.0000, 11632.0000,        inf,\n",
      "        33664.0000, 49760.0000, 55104.0000, 53568.0000, 39360.0000, 29552.0000,\n",
      "        40544.0000, 19664.0000, 27952.0000,        inf, 35232.0000, 15184.0000,\n",
      "        54624.0000, 56768.0000,        inf, 51680.0000, 13120.0000, 16000.0000,\n",
      "        59520.0000, 20576.0000, 16240.0000,        inf,        inf, 60224.0000,\n",
      "        55648.0000, 63872.0000, 21440.0000, 56384.0000, 32048.0000, 16720.0000,\n",
      "        27984.0000,        inf, 10800.0000, 26272.0000,  4172.0000, 20816.0000,\n",
      "        38208.0000, 16048.0000, 20208.0000, 33408.0000, 27312.0000, 53664.0000,\n",
      "        27408.0000, 34464.0000, 10328.0000,  3624.0000, 47296.0000, 57152.0000,\n",
      "        17216.0000, 58816.0000, 13424.0000,  4648.0000, 17488.0000, 40256.0000,\n",
      "               inf, 16264.0000, 31392.0000, 25024.0000, 50720.0000, 38848.0000,\n",
      "        10912.0000, 12032.0000,        inf,        inf, 24832.0000, 55104.0000,\n",
      "          923.0000, 54048.0000, 56096.0000, 27776.0000, 61056.0000, 37952.0000,\n",
      "        19968.0000, 54144.0000, 24592.0000, 26496.0000, 23072.0000, 59328.0000,\n",
      "        20192.0000,        inf,        inf,        inf, 21728.0000, 31552.0000,\n",
      "         2376.0000, 25680.0000, 18432.0000,  8624.0000, 58656.0000, 25696.0000,\n",
      "        39296.0000, 51840.0000,        inf,        inf, 25920.0000, 49504.0000,\n",
      "        13248.0000, 34048.0000, 48704.0000, 32512.0000,   278.2500,   150.0000,\n",
      "        47840.0000,  3518.0000, 48672.0000, 44096.0000, 31168.0000, 51008.0000,\n",
      "        19264.0000, 12912.0000, 53568.0000, 18144.0000, 28624.0000, 13440.0000,\n",
      "        32656.0000, 57344.0000, 47488.0000, 20496.0000, 43840.0000, 24352.0000,\n",
      "               inf,        inf, 58624.0000, 20512.0000,        inf, 22672.0000,\n",
      "        11672.0000,        inf, 30048.0000,        inf,  7844.0000,  8632.0000,\n",
      "               inf, 64608.0000, 54144.0000,  2052.0000, 12160.0000, 48864.0000,\n",
      "        34368.0000, 36032.0000, 50464.0000, 51808.0000, 17072.0000, 36896.0000,\n",
      "        30528.0000,        inf,  3428.0000,  7240.0000,        inf, 18912.0000,\n",
      "               inf, 30816.0000,        inf, 58144.0000, 12992.0000, 29840.0000,\n",
      "        28576.0000, 32192.0000, 57888.0000, 17440.0000, 21120.0000, 21184.0000,\n",
      "        23040.0000,        inf, 39008.0000,        inf,        inf,  6208.0000,\n",
      "        30224.0000, 14928.0000,  4070.0000, 17824.0000, 38464.0000,  9152.0000,\n",
      "        38624.0000, 23520.0000, 15968.0000,  5820.0000, 39328.0000, 31184.0000,\n",
      "        43232.0000, 48320.0000, 26096.0000, 53088.0000, 20672.0000, 65152.0000,\n",
      "        12120.0000, 63680.0000,        inf, 17984.0000, 42880.0000, 57536.0000,\n",
      "         8920.0000, 36512.0000,  4800.0000, 59008.0000,        inf, 24160.0000,\n",
      "               inf, 61152.0000, 41664.0000, 19392.0000, 19920.0000, 47840.0000,\n",
      "        50912.0000,        inf, 44800.0000, 27584.0000, 56896.0000, 37600.0000,\n",
      "        27280.0000, 57376.0000, 42496.0000, 15016.0000, 17072.0000,        inf,\n",
      "        22304.0000, 20464.0000,        inf, 10960.0000, 16400.0000, 12456.0000,\n",
      "        17248.0000, 22544.0000, 52576.0000, 34528.0000, 23360.0000, 17104.0000,\n",
      "        16704.0000,        inf,        inf, 57824.0000, 65088.0000, 39840.0000,\n",
      "         9944.0000, 15184.0000,  8432.0000, 34368.0000, 19248.0000, 38048.0000,\n",
      "               inf, 23632.0000,  1555.0000, 24720.0000, 14512.0000,        inf,\n",
      "         7496.0000, 39520.0000, 39744.0000,        inf, 11792.0000, 59712.0000,\n",
      "        34560.0000,  6540.0000,  1546.0000, 37440.0000,        inf,   839.0000,\n",
      "               inf, 52256.0000, 40800.0000, 61056.0000, 60736.0000, 36576.0000,\n",
      "        38400.0000,        inf,  5744.0000, 33152.0000, 35008.0000, 13224.0000,\n",
      "        21168.0000,  3200.0000,  2582.0000, 43200.0000, 64224.0000, 25488.0000,\n",
      "        25872.0000,  2350.0000,        inf, 40832.0000,        inf, 21392.0000,\n",
      "        29712.0000, 53728.0000, 31392.0000, 22928.0000, 36288.0000, 47520.0000,\n",
      "        15672.0000,        inf, 37312.0000,        inf,        inf, 29408.0000,\n",
      "        14504.0000,  7744.0000, 35168.0000, 35008.0000,  9192.0000, 18432.0000,\n",
      "          948.0000, 23280.0000,        inf,  1682.0000, 23840.0000, 11456.0000,\n",
      "        25984.0000, 46400.0000, 26128.0000, 34112.0000, 47104.0000, 37856.0000,\n",
      "        14416.0000, 30048.0000, 35808.0000, 30480.0000, 14856.0000,        inf,\n",
      "        21152.0000, 17360.0000, 63840.0000,  7232.0000, 20848.0000, 44352.0000,\n",
      "         4116.0000, 14680.0000, 10904.0000, 15592.0000, 16640.0000, 24224.0000,\n",
      "        10880.0000, 37056.0000,  1748.0000,        inf, 43808.0000, 17120.0000,\n",
      "               inf, 54976.0000, 41056.0000,        inf, 24832.0000, 14576.0000,\n",
      "        49824.0000, 31584.0000, 42176.0000,        inf, 41344.0000,  1323.0000,\n",
      "        31680.0000,  4116.0000, 20528.0000, 25104.0000, 62464.0000, 11032.0000,\n",
      "        57984.0000, 52544.0000, 16352.0000, 41120.0000,  9024.0000, 12840.0000,\n",
      "         1946.0000, 27664.0000, 46496.0000, 55680.0000,        inf, 28208.0000,\n",
      "               inf, 27264.0000], device='cuda:0', dtype=torch.float16)]\n",
      "{'grad_avg': tensor([[-1.1757e-06,  9.4795e-05,  1.4019e-05,  ...,  9.8646e-06,\n",
      "          2.4090e-04, -6.1631e-06],\n",
      "        [ 1.4389e-03,  3.7174e-04, -5.9891e-04,  ...,  1.3580e-03,\n",
      "         -1.2646e-04,  8.8196e-04],\n",
      "        [-1.5965e-04,  5.7364e-05,  2.3384e-04,  ..., -1.1578e-04,\n",
      "          1.9169e-04,  9.0599e-05],\n",
      "        ...,\n",
      "        [-1.3709e-06, -4.7028e-06,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          1.7250e-05,  3.5048e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0'), 'wd': 0.0, 'lr': 3.019951720402016e-07, 'mom': 0.9, 'hessian_power': 1, 'eps': 1e-05}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hutchinson_trace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-d81c6bd29d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai2/fastai2/callback/schedule.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, start_lr, end_lr, num_it, stop_div, show_plot, suggestions)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mcb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_plot\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_lr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msuggestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/fastcore/utils.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0minit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'init_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_return\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai2/fastai2/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m;\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai2/fastai2/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m;\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m:\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai2/fastai2/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-5f78db9801c3>\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b, create_graph)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_backward'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# <---CHANGED HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m:\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai2/fastai2/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhyper\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-92f18ef6249e>\u001b[0m in \u001b[0;36maverage_sqr_diag_hessian\u001b[0;34m(p, sqr_mom, dampening, sqr_avg_diag_hessian, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maverage_sqr_diag_hessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqr_mom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqr_avg_diag_hessian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhutchinson_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msqr_avg_diag_hessian\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msqr_avg_diag_hessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msqr_mom\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hutchinson_trace' is not defined"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-124-bb1ce3a72eb6>\u001b[0m(7)\u001b[0;36maverage_sqr_diag_hessian\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      4 \u001b[0;31m    \u001b[0mdamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msqr_mom\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0;31m#sqr_avg_diag_hessian.mul_(sqr_mom).addcmul_(p.grad.data, p.grad.data, value=damp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      6 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 7 \u001b[0;31m    \u001b[0msqr_avg_diag_hessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msqr_mom\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msqr_avg_diag_hessian\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdamp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhutchinson_trace\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhutchinson_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'sqr_avg_diag_hessian'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msqr_avg_diag_hessian\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> hutchinson_trace\n",
      "<function hutchinson_trace at 0x7fe2b03a2cb0>\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.one_batch??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - Adam 2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.5248074531555176)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>corpus_bleu</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.559707</td>\n",
       "      <td>1.546616</td>\n",
       "      <td>4.695555</td>\n",
       "      <td>0.357184</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.182653</td>\n",
       "      <td>1.253895</td>\n",
       "      <td>3.503965</td>\n",
       "      <td>0.404492</td>\n",
       "      <td>10:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.885465</td>\n",
       "      <td>1.057526</td>\n",
       "      <td>2.879239</td>\n",
       "      <td>0.445343</td>\n",
       "      <td>10:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.723774</td>\n",
       "      <td>0.930569</td>\n",
       "      <td>2.535953</td>\n",
       "      <td>0.473972</td>\n",
       "      <td>10:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.634351</td>\n",
       "      <td>0.891113</td>\n",
       "      <td>2.437842</td>\n",
       "      <td>0.483517</td>\n",
       "      <td>10:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, 5e-4, div=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.save('v03_exp2_adahess_basline_2_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp1 - AdaHessian 2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-d81c6bd29d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai2/fastai2/callback/schedule.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, start_lr, end_lr, num_it, stop_div, show_plot, suggestions)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mcb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_plot\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_lr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msuggestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/fastcore/utils.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0minit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'init_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_return\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai2/fastai2/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m;\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai2/fastai2/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m;\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m:\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai2/fastai2/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-5f78db9801c3>\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b, create_graph)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_backward'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# <---CHANGED HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m:\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai2/fastai2/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhyper\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-75bd35f17d1a>\u001b[0m in \u001b[0;36mhutchinson_trace\u001b[0;34m(p, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0monly_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         retain_graph=True)\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mhutchinson_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    190\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    191\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>corpus_bleu</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.559707</td>\n",
       "      <td>1.546616</td>\n",
       "      <td>4.695555</td>\n",
       "      <td>0.357184</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.182653</td>\n",
       "      <td>1.253895</td>\n",
       "      <td>3.503965</td>\n",
       "      <td>0.404492</td>\n",
       "      <td>10:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.885465</td>\n",
       "      <td>1.057526</td>\n",
       "      <td>2.879239</td>\n",
       "      <td>0.445343</td>\n",
       "      <td>10:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.723774</td>\n",
       "      <td>0.930569</td>\n",
       "      <td>2.535953</td>\n",
       "      <td>0.473972</td>\n",
       "      <td>10:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.634351</td>\n",
       "      <td>0.891113</td>\n",
       "      <td>2.437842</td>\n",
       "      <td>0.483517</td>\n",
       "      <td>10:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, 5e-4, div=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAeNElEQVR4nO3deXRc5Z3m8e+vSqXdsmRL3iRAosNuFBbhdoJDOywBbMLSuDPOhISQTDgnJAO4QxITQkIIdJNkesIw3UAgQwfSBocYaJKwNJDYOAk2IGNjCxvwgo2FF8nyJlnW/s4fdVWqkmRZS5WurvR8zqlTt966VfWre6ynXr/3vfeacw4REQmekN8FiIjI4CjARUQCSgEuIhJQCnARkYBSgIuIBFTacH5YYWGhKy0tHc6PFBEJvFWrVu1xzhV1bx/WAC8tLaWysnI4P1JEJPDMbFtv7UcdQjGzR8ysxsyq4tommNnLZrbRuy9IZrEiInJ0/RkD/xVwSbe2hcAfnXMnAH/0HouIyDA6aoA755YDe7s1XwE86i0/ClyZ5LpEROQoBjsGPtk5txPAObfTzCYdaUUzux64HuDYY48d5MeJyFjV2tpKdXU1TU1NfpeScpmZmZSUlBCJRPq1fsp3YjrnHgIeAqioqNCJV0RkQKqrqxk3bhylpaWYmd/lpIxzjrq6OqqrqykrK+vXawY7D3y3mU0F8O5rBvk+IiJ9ampqYuLEiaM6vAHMjIkTJw7ofxqDDfDfAdd6y9cCzw7yfUREjmq0h3engX7P/kwjfAJYAZxkZtVm9lXgHuAiM9sIXOQ9TplnVlez6PVep0GKiIxZ/ZmF8nnn3FTnXMQ5V+Kc+3/OuTrn3AXOuRO8++6zVJLqd2t28Js3t6fyI0REerV//37uv//+Ab9uzpw57N+/PwUVdQnEuVBCZrR3aP+niAy/IwV4e3t7n697/vnnyc/PT1VZwDAfSj9YoZCh/BYRPyxcuJDNmzdzxhlnEIlEyM3NZerUqaxZs4b169dz5ZVXsn37dpqamrjpppu4/vrrga5ThzQ0NHDppZcya9YsXnvtNYqLi3n22WfJysoacm3BCHCDDiW4yJj3o9+/w/odB5P6nqdOy+OHnz3tiM/fc889VFVVsWbNGpYtW8bcuXOpqqqKTfV75JFHmDBhAocPH+acc87h6quvZuLEiQnvsXHjRp544gkefvhhPve5z/HUU09xzTXXDLn2QAR4OGR06NqdIjICzJgxI2Ge9n333cczzzwDwPbt29m4cWOPAC8rK+OMM84A4Oyzz2br1q1JqSUQAW5mtCvARca8vnrKwyUnJye2vGzZMl555RVWrFhBdnY2s2fP7nUed0ZGRmw5HA5z+PDhpNQSiJ2YYTOU3yLih3HjxlFfX9/rcwcOHKCgoIDs7GzeffddVq5cOay1BaIHHjI0C0VEfDFx4kTOPfdcpk+fTlZWFpMnT449d8kll/Dggw9SXl7OSSedxMyZM4e1tmAEuMbARcRHjz/+eK/tGRkZvPDCC70+1znOXVhYSFVV7HIK3HLLLUmrKxBDKCEzzUIREekmIAGO5oGLiHQTiADXNEIRkZ4CEeBmCnARke4CEeBh06H0IiLdBSLANY1QRKSnYAS4xsBFJCByc3MB2LFjB/Pmzet1ndmzZ1NZWTnkzwpGgGsaoYgEzLRp01iyZElKPyMQB/KEdTpZEfHJd7/7XY477jhuuOEGAO644w7MjOXLl7Nv3z5aW1u56667uOKKKxJet3XrVi677DKqqqo4fPgw1113HevXr+eUU05J2rlQAhHgZuhkViICLyyEXeuS+55TTodLj3xVyPnz53PzzTfHAvzJJ5/kxRdfZMGCBeTl5bFnzx5mzpzJ5ZdffsRrWj7wwANkZ2ezdu1a1q5dy1lnnZWU0gMR4NGTWSnARWT4nXnmmdTU1LBjxw5qa2spKChg6tSpLFiwgOXLlxMKhfjoo4/YvXs3U6ZM6fU9li9fzo033ghAeXk55eXlSaktEAEeMqNNYygi0kdPOZXmzZvHkiVL2LVrF/Pnz2fRokXU1tayatUqIpEIpaWlvZ5GNt5ArzjfH4HYiZkWjp5OVjsyRcQP8+fPZ/HixSxZsoR58+Zx4MABJk2aRCQSYenSpWzbtq3P15933nksWrQIgKqqKtauXZuUuoIR4KHoL5d64SLih9NOO436+nqKi4uZOnUqX/jCF6isrKSiooJFixZx8skn9/n6r3/96zQ0NFBeXs5Pf/pTZsyYkZS6AjGEEg5Ff2d0MI+I+GXduq6dp4WFhaxYsaLX9RoaGoDoRY07TyOblZXF4sWLk15TIHrgkXBnD7zD50pEREaOQAR4uHMIpV09cBGRToEI8LRwtEyNgYuMTWNlGvFAv2cwAjykIRSRsSozM5O6urpRH+LOOerq6sjMzOz3awKxEzNNQygiY1ZJSQnV1dXU1tb6XUrKZWZmUlJS0u/1gxHg3k5MzUIRGXsikQhlZWV+lzEiBWIIpXMaoYZQRES6BCLAIzqQR0Skh0AEuKYRioj0FIgAj2gaoYhID4EI8K4euMbARUQ6DSnAzWyBmb1jZlVm9oSZ9X8C4wDoZFYiIj0NOsDNrBi4Eahwzk0HwsD8ZBUWr/NITE0jFBHpMtQhlDQgy8zSgGxgx9BL6qlzCKVVQygiIjGDDnDn3EfA/wI+BHYCB5xzL3Vfz8yuN7NKM6sc7JFUER3IIyLSw1CGUAqAK4AyYBqQY2bXdF/POfeQc67COVdRVFQ0qM/q6oErwEVEOg1lCOVC4APnXK1zrhV4GvhkcspKFNEYuIhID0MJ8A+BmWaWbdGrdV4AbEhOWYnCOhuhiEgPQxkDfx1YArwFrPPe66Ek1ZVAZyMUEelpSGcjdM79EPhhkmo5Ik0jFBHpKRBHYnb2wFs1hCIiEhOoAFcPXESkS0ACPFqmphGKiHQJRICHYwfyaAhFRKRTIAJcJ7MSEekpWAGuIRQRkZhABHhYPXARkR4CEeBmRlrIdEEHEZE4gQhwgLSwaRqhiEic4AR4KKRphCIicQIT4OGQaRqhiEicwAR4JGzaiSkiEicwAR4OmaYRiojECUyAp4VC6oGLiMQJToCHTRd0EBGJE5gAD4c0Bi4iEi8wAR4JhWjXGLiISExgAjzaA9cQiohIp8AEuKYRiogkCkyAaxqhiEiiwAR4dBqhhlBERDoFJ8DD6oGLiMQLTIBrGqGISKLABHgkHNLpZEVE4gQmwMMho1UXdBARiQlMgEd0QQcRkQQBCvAQzW3qgYuIdApMgGenp9HY0u53GSIiI0aAAjzM4ZY2v8sQERkxAhXgja3tOKdxcBERCFCAZ6WHcQ6aWjUOLiICAQrwnPQ0ABo1jCIiAgQowLPSwwDakSki4glMgGd7AX64VQEuIgJDDHAzyzezJWb2rpltMLNPJKuw7rLVAxcRSZA2xNf/H+BF59w8M0sHspNQU6+yO8fAmzUGLiICQwhwM8sDzgO+DOCcawFaklNWT+qBi4gkGsoQyvFALfDvZrbazH5pZjlJqquHWIBrDFxEBBhagKcBZwEPOOfOBA4BC7uvZGbXm1mlmVXW1tYO+sOyvCEUHY0pIhI1lACvBqqdc697j5cQDfQEzrmHnHMVzrmKoqKiQX9YjoZQREQSDDrAnXO7gO1mdpLXdAGwPilV9ULzwEVEEg11Fsr/BBZ5M1C2ANcNvaTepYdDhEOmIzFFRDxDCnDn3BqgIkm19MnMyI6E1QMXEfEE5khMiA6jHFaAi4gAAQvwnAxd1EFEpFOgAjwrEtYYuIiIJ1ABnp2uMXARkU6BCvAsBbiISEygAjwnPU07MUVEPIEK8Oh1MTUGLiICAQvwrPQwjc3qgYuIQMACvKG5jbpDKTtjrYhIoAQqwNfvOAhAc5t64SIigQrwr8wqA6CuQb1wEZFABXhhbgYAexqafa5ERMR/AQvwdABq6xXgIiIBC3D1wEVEOgUqwCflRQN890EFuIhIoAI8Iy1MfnZEQygiIgQswCE6jKIhFBGRAAZ4UW6GeuAiIgQxwMepBy4iAgEM8A07D7K1rpGODud3KSIivgpcgJ9TNgFA50QRkTEvcAH+qY8VAlBT3+RzJSIi/gpcgHfOBa/RXHARGeOCF+DjMgF4bMVWX+sQEfFb4AJ8Wn4WAEvfq/W5EhERfwUuwMMh87sEEZERIXABDjDL25EpIjKWBTLA/7JpDwCrP9zncyUiIv4JZIB/+ZOlAKzapgAXkbErkAH+tfOOB+Cu5zb4XImIiH8CGeBT8jL9LkFExHeBDHDNRBERCWiAx6s5qEPqRWRsCmyA//2ZxQC8sXWvz5WIiPhjyAFuZmEzW21mf0hGQf214KITAXhr2/7h/FgRkREjGT3wm4Bhnw4yZXx0R+Yjf/1guD9aRGREGFKAm1kJMBf4ZXLK6b9IuKv06n2Nw/3xIiK+G2oP/F7gO0BHEmoZtFk/Wernx4uI+GLQAW5mlwE1zrlVR1nvejOrNLPK2trknkFw7R2fSer7iYgEyVB64OcCl5vZVmAxcL6Z/Uf3lZxzDznnKpxzFUVFRUP4uJ7yMiN8++KTADjU3JbU9xYRGekGHeDOuVudcyXOuVJgPvAn59w1Sausn171zgt+3a/eHO6PFhHxVWDngXe666rpALzxgeaDi8jYkpaMN3HOLQOWJeO9BurEyePi68BMh9mLyNgQ+B54vLJbn/e7BBGRYTMqAvzRr8yILe891OJjJSIiw2dUBPjfndg1u+VbT67xsRIRkeEzKgIcYPXtFwFQNC7D50pERIbHqAnwgpx0AJ6srPa5EhGR4TFqAjzeD56t8rsEEZGUG1UB/s1PfwyAx1Zs87kSEZHUG1UBfot3WD3A8veTe94VEZGRZlQFeLwvPfKG3yWIiKTUqAvwD/55Tmz5T+/u9rESEZHUGnUBHn8o/Vd+VeljJSIiqTXqAhxg6z1zY8vNbe0+ViIikjqjMsDjnfT9F2lp8/WCQSIiKTFqA/zdH18SW7716XU+ViIikhqjNsAzI+HY8lNv6ehMERl9Rm2AA2z+p64ZKVtqG3ysREQk+UZ1gIdDXTNSzv+XV32sREQk+UZ1gAP8+Tufji2v/nCfj5WIiCTXqA/wkoKs2PJV97/mYyUiIsk16gPczKj60cWxxwebWn2sRkQkeUZ9gAPkZqRxknfx4/I7XvK5GhGR5BgTAQ6w6Gt/G1teuaWOjg7nYzUiIkOX5ncBw6Uwt+tSa/MfWgkkHnIvIhI0Y6YHDvDG9y5IePzEGx/6VImIyNCNqQCflJfJX77bNa1Qh9iLSJCNqQAHKCnITjhn+I79h32sRkRk8MZcgEPiOcM/ec+f+OfnN/hYjYjI4IzJAAeo/P6FseVfLN/Ch3WNPlYjIjJwYzbAC3MzuOS0KbHH5/1sqY/ViIgM3JgNcIAHv3g2G+7sOm/4xt31PlYjIjIwYzrAAbLSu84bftHPl1O68Dn+ummPjxWJiPTPmA9wSLySPcAXfvm6T5WIiPSfApzorJSNd1/K5LyuozVLFz7HM6t1JR8RGbkU4J5IOMTr37swoW3Bb96mqVVXtReRkUkB3k3386OcfPuL7G9s8akaEZEjG3SAm9kxZrbUzDaY2TtmdlMyC/PT1nvm8osvnh17fMadL/PfH15Ja3uHj1WJiCQaSg+8DfiWc+4UYCbwDTM7NTll+e/i06bw8//28djj1zbXccJtL3DvK+/rVLQiMiIMOsCdczudc295y/XABqA4WYWNBFedWZJwxCbAva9s5PjvPa+xcRHxXVLGwM2sFDgT6DH/zsyuN7NKM6usra1NxscNq8LcjB7TDCE6Nv6NRW9RuXUvh5rbfKhMRMY6c25owwFmlgu8CtztnHu6r3UrKipcZWXlkD7PT845ym59vtfndHEIEUkVM1vlnKvo3j6kHriZRYCngEVHC+/RwMzYes9c3rjtgh7PlS58jr2HWhjqD6KISH8Nugdu0XOyPgrsdc7d3J/XBL0HHu/l9bup3LaXX7y6pdfnf3T5aVwz8zjCIev1eRGR/jpSD3woAT4L+DOwDuicX/c951zvYwyMrgCPV7rwuSM+V1KQxV++e/4wViMio82RAnzQFzV2zv0FUPcSuG3OKdx9hItCVO87nBDwD3+pgunFeUwdnzVc5YnIKDXknZgDMVp74PHa2jv4t6Wb+fkr7w/4tcu//WnS00JMGZ+ZgspEJKiSPoQyGGMhwOM1tbZz8u0vDvh1a35wEVUfHWTWCYUpqEpEgkYB7pOODsfq7fuZXpxHU2sHv63cTjhk3PXcBtr7OKJzZmg9zS7C/V+fy1eXbGd9zWF+Nq+cf6g4ZhirF5GRQAE+Qu071MLVD77GltpDCe1/ybiREoteWKLDGXXksdsVsMsVUOMK2OUmsJsCdrsCFlz1d6RPKKauPZdZJxZR39TK3c9tYPGb23nvrkvISAv39tEiEhAK8BFuT0Mz66oP8Im/mcj3nl7HZ6fs5bH/eo3Jto8ptpfJ7POW9zHJ9lFoB3u8R7NLi4a7F+y73QR2u3x2uQnkFJZw7cWf4OQTToT0HB++oYgMlgI8gJxzdLjofVo4xIrNdXz+4ZUApNNKEfuZbPu6Qt72Mzkh7PeSY8093vegy+7qzVPAeWeXkzmhmFterOWmqz7FqSeeDLmTITzoSUoikkQK8FHq9S113L9sMwebWpk+bTy/XrkNgGdu+CRVHx3gJ8++GQv5yXT14KfY3tjyJPYTscSTczmMpoyJtGZN5t3GHM45/TQsbxqMmxq95Xn3WQVgmk0qkkoK8DHq4eVb2NvYwrcuOpFfr9zGj36/vsc6RgcTqY/23r1hmmjg700Ytplo9T1e22bp7OzIJ2/SsbRkTaJoWhmMmwJ506L3nYGfnj0cX1dkVFKAS6+aWttZ/n4t1/961VHXTaeVSba/R7DHevNeDz+7l2EbMsd3hfm4qRzKnERk/DTSC4q7evQ5kzRsI9ILBbgclXOOQy3t5KSH+eqjlfz4yukU52dRU9/EltpDzH9oZX/ehXEc7urNe+Pxk70x+im2NzZsk2aJVzhyFsIy86PDMrFb98cF0Ns64UhqNorICKAAl6RwznH7s1VMnzae5rYOSgtzKM7P5B8eXMG+xtZ+v0+IDiZysNuwzV4KaOCEca20NOxlvB0inwbyrYHx1tj3G6bn9gz8HkHfyw9CJFtj+DLiKcBlWPzvl99nT0Mzd185nea2DlraO7jz9+tZsqo6Yb2vfaqMh//8Qb/fN0QH42gk3xq8UD/EeA4xPu5xvjVw9Sm5cHhf4q2jjx+WcHr/gj4rHzLj2jPHQ0jz62V4KMBlRGrvcIRDRmNLG6f+4L9S8AmOLJo5d1qYj3buIN8auPbMfP52aogCGrpCvml/XOjvj95aeu607WLREO/vEE/847SMFHxPGc0U4BJYT765ne88tRaAL3+ylMaWNp6srD7Kq4bu+W/O5OnX3uHmcwvJaDlApPVgt979/p69/c4fAtdx5DeOZMeFfD5kjIu2ped03SLZ0WGhdK890vmc1x6/fjhdw0CjnAJcRr32DsdFP3+1x2kJUuHGC07gvj9ujD3+/Ixj+Gz5NKaXjCeMI4fDRw76WG/fu29pgJZD0NLo3TeAG8BFs0Np3QL+KIHf3x+ISJZ+GEYIBbiMaQcaW8nNTCMcMpxzPLP6I/7xybdT/rlv/+AzfPzOl5hz+hSeX7cLgKW3zKassI/TGTgH7S1emB+C1kYv5L2Abz0UF/gN3vOHuq3f/bH3+vZepngekfUR+AP4H0L39SM5EErK9dTHDAW4yABtqqnn3V313PG79fzH/5jBJff+OaWfNzkvg90HmykrzGHpLbMTnlu/4yClhdlkpw9xnnx7Wz9/ABoS/0dwtHXaDg+sDguBhaP/ewiFvVtaz7bY47Ro6Pe2zmBfl7BO97a4++5tFhrc6/KPHfT+DwW4SBI55zBveKG9w7F+x0GOL8qhua2Dc+5+hZBBa3vq/rayImFum3sK3//Pqljb+jsvprGlncLcjFhdnf/jsFQPhXS0e6Hejx+F1kboaPNu7d6tLTpsFN8We9xbW39e18/37mt/RTJ9400oOnFQL1WAiwyztvYONtY0MC0/i/FZXQcaLXuvhnd2HOS3ldvZWneU+e0pkJ8dofK2C1n49DouK5/KjU+s5mBTG0tvmc1za3fwzfNPGPaafOVcL8Hf1nub6zjKj0ofrzvhM5CZN6gSFeAiAdDR4dhc28ADr26mqbWdS6dPpbggi10Hmlj0+jZWf7if1vaOlPbuj+TWS09mX2MrX/zEcTQ0tXHshGyy0qNz4Zvb2jnQ2MqkPF0OMBUU4CKjTPehEeccB5vauP0/q/jd2zuA6HVWz/vZUr9K5PbLTuXHf4ieQK3y+xcy4+5X6HDw2FdmMOtjhbxdvZ/yknzCIc126YsCXEQAeGfHAUoKssnNSKO5rZ3DLe1kpYeprW9m8ZvbeWDZZr9L7Jd5Z5f0OML3rGPzuevK0zm+KAcz+GjfYf7lpfcpmZDFggtPJBwyIuGeM2Bq6ptID4cwM8ZlpBEKGS1tHYQM0npZvzep3NegABeRpOi80EjnzJjMSJj2Dsf7u+v5v3/ahAEvrd/td5kjzqvfns1xEwd3NawjBbjO3SkiA2JmhA1OLxkfa4uEobwkn4e/1CNjetW9t7ph50Fu+e3bvLPjIPPOLuHTJ03iG4+/xZzTp+AcvFC1K+nfY7iFUtA7V4CLyLDrPtRwytQ8nrvxUwltc8vnDvr9m9vaae9wRMIhtu9t5PiiXCB6QNfH73wJgG9ffBJT8jL5+7OKY/Ucam5jU00D4ZAxPitCTX0z97ywgTe37uPuq6azp76FLXsaeHbNDh764tnc8tu3mV48ntc21/GpEwr52KRc/v2vWynIjpCfnc5nPz4NA6aMz+SYCcm/qImGUERERrgjDaHoeFYRkYBSgIuIBJQCXEQkoBTgIiIBpQAXEQkoBbiISEApwEVEAkoBLiISUMN6II+Z1QLbBvnyQmBPEssZjbSNjk7b6Oi0jfrmx/Y5zjlX1L1xWAN8KMyssrcjkaSLttHRaRsdnbZR30bS9tEQiohIQCnARUQCKkgB/pDfBQSAttHRaRsdnbZR30bM9gnMGLiIiCQKUg9cRETiKMBFRAIqEAFuZpeY2XtmtsnMFvpdTyqZ2SNmVmNmVXFtE8zsZTPb6N0XeO1mZvd522WtmZ0V95prvfU3mtm1ce1nm9k67zX3WaquwppCZnaMmS01sw1m9o6Z3eS1azt5zCzTzN4ws7e9bfQjr73MzF73vu9vzCzda8/wHm/yni+Ne69bvfb3zOziuPbA/12aWdjMVpvZH7zHwdo+zrkRfQPCwGbgeCAdeBs41e+6Uvh9zwPOAqri2n4KLPSWFwI/8ZbnAC8ABswEXvfaJwBbvPsCb7nAe+4N4BPea14ALvX7Ow9iG00FzvKWxwHvA6dqOyVsIwNyveUI8Lr33Z8E5nvtDwJf95ZvAB70lucDv/GWT/X+5jKAMu9vMTxa/i6BfwQeB/7gPQ7U9glCD3wGsMk5t8U51wIsBq7wuaaUcc4tB/Z2a74CeNRbfhS4Mq79MRe1Esg3s6nAxcDLzrm9zrl9wMvAJd5zec65FS76r++xuPcKDOfcTufcW95yPbABKEbbKcb7rg3ew4h3c8D5wBKvvfs26tx2S4ALvP91XAEsds41O+c+ADYR/ZsM/N+lmZUAc4Ffeo+NgG2fIAR4MbA97nG11zaWTHbO7YRoeAGTvPYjbZu+2qt7aQ8s77+yZxLtYWo7xfGGB9YANUR/nDYD+51zbd4q8d8rti285w8AExn4tguSe4HvAB3e44kEbPsEIcB7G3vU3MeoI22bgbYHkpnlAk8BNzvnDva1ai9to347OefanXNnACVEe4Sn9Laadz+mtpGZXQbUOOdWxTf3suqI3j5BCPBq4Ji4xyXADp9q8ctu77/1ePc1XvuRtk1f7SW9tAeOmUWIhvci59zTXrO2Uy+cc/uBZUTHwPPNLM17Kv57xbaF9/x4okN5A912QXEucLmZbSU6vHE+0R55sLaP3zsR+rGTIY3ozqUyunYGnOZ3XSn+zqUk7sT8GYk7537qLc8lcefcG177BOADojvmCrzlCd5zb3rrdu6cm+P39x3E9jGi49L3dmvXduraFkVAvrecBfwZuAz4LYk76W7wlr9B4k66J73l00jcSbeF6A66UfN3CcymaydmoLaP7xuvnxt4DtGZBpuB2/yuJ8Xf9QlgJ9BK9Ff8q0TH2v4IbPTuO0PGgH/ztss6oCLufb5CdIfKJuC6uPYKoMp7zb/iHY0bpBswi+h/R9cCa7zbHG2nhG1UDqz2tlEV8AOv/XiiM2w2eWGV4bVneo83ec8fH/det3nb4T3iZuOMlr/LbgEeqO2jQ+lFRAIqCGPgIiLSCwW4iEhAKcBFRAJKAS4iElAKcBGRgFKAi4gElAJcRCSg/j9v2WXqbgD6zQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.save('v03_exp2_adahess_basline_2_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Process Translations funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, sentence, vocab, to_cuda=True):    \n",
    "    model=model.eval()\n",
    "    \n",
    "    sentence=learn.dls.tokenizer[0].encodes(sentence)\n",
    "    #sentence=learn.dls.tokenizer[0][1].encodes(sentence)\n",
    "    sentence=learn.dls.numericalize[0].encodes(sentence)\n",
    "    \n",
    "    translated_sentence = [2] \n",
    "    i = 0\n",
    "    while int(translated_sentence[-1]) != 3 and i < 75:   \n",
    "        if to_cuda: output = forward_model(model, sentence, translated_sentence, to_cuda).cuda()\n",
    "        else: output = forward_model(model, sentence, translated_sentence, to_cuda).cpu()\n",
    "        values, indices = torch.topk(output, 5)\n",
    "        translated_sentence.append(int(indices[-1][0]))\n",
    "        i+=1\n",
    "\n",
    "    detok_translated_sentence=detokenize(translated_sentence, vocab)\n",
    "    #print(' '.join(detok_translated_sentence))\n",
    "    return ' '.join(detok_translated_sentence)\n",
    "    \n",
    "\n",
    "def forward_model(model, src, tgt, to_cuda=True):\n",
    "    if to_cuda:\n",
    "        src = torch.as_tensor(src).unsqueeze(0).long().cuda()\n",
    "        tgt = torch.as_tensor(tgt).unsqueeze(0).cuda()\n",
    "        tgt_mask = gen_nopeek_mask(tgt.shape[1]).cuda()\n",
    "        src = to_half(src)\n",
    "        tgt = to_half(tgt)\n",
    "    else:\n",
    "        src = torch.as_tensor(src).unsqueeze(0).long().cpu()\n",
    "        tgt = torch.as_tensor(tgt).unsqueeze(0).cpu()\n",
    "        tgt_mask = gen_nopeek_mask(tgt.shape[1]).cpu()\n",
    "#         src = to_half(src)\n",
    "#         tgt = to_half(tgt)\n",
    "    output = model.forward(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "\n",
    "    #return output.squeeze(0).to('cpu')\n",
    "    return output.squeeze(0).detach()\n",
    "\n",
    "\n",
    "# def tokenize(sentence, freq_list, lang_model):\n",
    "#     punctuation = ['(', ')', ':', '\"', ' ']\n",
    "\n",
    "#     sentence = sentence.lower()\n",
    "#     sentence = [tok.text for tok in lang_model.tokenizer(sentence) if tok.text not in punctuation]\n",
    "#     return [freq_list[word] if word in freq_list else freq_list['[OOV]'] for word in sentence]\n",
    "\n",
    "\n",
    "def detokenize(sentence, vocab):\n",
    "    #freq_list = {v: k for k, v in freq_list.items()}\n",
    "    return [vocab[token] for token in sentence]\n",
    "    #return [freq_list[token] for token in sentence]\n",
    "# def detokenize(sentence, freq_list):\n",
    "#     freq_list = {v: k for k, v in freq_list.items()}\n",
    "#     return [freq_list[token] for token in sentence]\n",
    "\n",
    "\n",
    "def gen_nopeek_mask(length):\n",
    "    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -> w h')\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_trans(trans):\n",
    "    trans_ls=[]\n",
    "    for s in trans: \n",
    "        #print(s)\n",
    "        tmp = s.replace('xxbos','')\n",
    "        tmp = tmp.replace('xxeos','')\n",
    "        tmp = tmp.replace(' .','.')\n",
    "        tmp = tmp.replace(' ,',',')\n",
    "        tmp = tmp.replace(' ?','?')\n",
    "        tmp = tmp.replace(' !','!')\n",
    "        #print(tmp[0])\n",
    "        if tmp.endswith('. '): tmp=tmp[:-1]\n",
    "        if tmp.endswith('? '): tmp=tmp[:-1]\n",
    "        if tmp.endswith('! '): tmp=tmp[:-1]\n",
    "        \n",
    "        for spec in ['xxmaj ', 'xxup ']:\n",
    "            found=[]\n",
    "            for m in re.finditer(spec, tmp):\n",
    "                found.append(m.start())\n",
    "\n",
    "            for f in found:\n",
    "                m = tmp.find(spec)\n",
    "                if m != -1:   \n",
    "                    ml = m+len(spec)\n",
    "                    if m != 0:\n",
    "                        tmp = tmp[:ml] + tmp[ml].upper() + tmp[ml+1:]\n",
    "                        tmp = tmp[:m] + tmp[ml:]\n",
    "                    else: \n",
    "                        tmp = tmp[ml].upper() + tmp[ml+1:]\n",
    "                        tmp = tmp[ml:]\n",
    "        \n",
    "        # Remove space at start\n",
    "        if tmp[0] == ' ': tmp = tmp[1:]\n",
    "            \n",
    "        # Uppercase start of sentence\n",
    "        #tmp = tmp[0].upper() + tmp[1:]\n",
    "            \n",
    "        trans_ls.append(tmp)\n",
    "    return trans_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ga_id</th>\n",
       "      <th>ga</th>\n",
       "      <th>en_id</th>\n",
       "      <th>en</th>\n",
       "      <th>ga_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>557291</td>\n",
       "      <td>Cá bhfuil críochfort na mbus?</td>\n",
       "      <td>35406</td>\n",
       "      <td>Where is the bus terminal?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>557299</td>\n",
       "      <td>Nuair a dhúisigh mé, bhí brón orm.</td>\n",
       "      <td>1361</td>\n",
       "      <td>When I woke up, I was sad.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>557533</td>\n",
       "      <td>Tosaíonn an t-oideachas sa bhaile.</td>\n",
       "      <td>19122</td>\n",
       "      <td>Education starts at home.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>557579</td>\n",
       "      <td>Táim i ngrá leat.</td>\n",
       "      <td>1434</td>\n",
       "      <td>I love you.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>934942</td>\n",
       "      <td>Tá grá agam duit.</td>\n",
       "      <td>1434</td>\n",
       "      <td>I love you.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ga_id                                  ga  en_id  \\\n",
       "0  557291       Cá bhfuil críochfort na mbus?  35406   \n",
       "1  557299  Nuair a dhúisigh mé, bhí brón orm.   1361   \n",
       "2  557533  Tosaíonn an t-oideachas sa bhaile.  19122   \n",
       "3  557579                   Táim i ngrá leat.   1434   \n",
       "4  934942                   Tá grá agam duit.   1434   \n",
       "\n",
       "                           en  ga_len  \n",
       "0  Where is the bus terminal?       5  \n",
       "1  When I woke up, I was sad.       7  \n",
       "2   Education starts at home.       5  \n",
       "3                 I love you.       4  \n",
       "4                 I love you.       4  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_path = Path('data/irish/parallel_corpora/tatoeba')\n",
    "t_fn = 'tatoeba_en-ga.csv'\n",
    "t_df = pd.read_csv(t_path/t_fn)\n",
    "\n",
    "t_df['ga_len'] = t_df.ga.str.split().str.len()\n",
    "t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.dls.test_dl = t_dls.valid\n",
    "# def act_fn(x): return L(F.softmax(o, dim=-1) for o in x)\n",
    "# o = learn.get_preds(dl=t_dls.valid, act=act_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxup tá mé réidh don deireadh seachtaine ! xxeos'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"I'm ready for the weekend!\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxbos xxmaj más é an críochfort bus ? xxeos',\n",
       " 'xxbos xxmaj nuair mé xxunk suas , bhí mé brónach . xxeos',\n",
       " 'xxbos xxmaj tosaíonn oideachas sa bhaile . xxeos',\n",
       " 'xxbos xxmaj grá mé duit . xxeos',\n",
       " 'xxbos xxmaj grá mé duit . xxeos']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_ls=[]\n",
    "for e in t_df.en.values:\n",
    "    trans_ls.append(generate(learn.model, e, dls.vocab[1]))    \n",
    "trans_ls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('tatoeba_raw_translations_exp{4}_20200810.txt','w')\n",
    "for ele in trans_ls:\n",
    "    f.write(ele+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Más é an críochfort bus?',\n",
       " 'Nuair mé xxunk suas, bhí mé brónach.',\n",
       " 'Tosaíonn oideachas sa bhaile.',\n",
       " 'Grá mé duit.',\n",
       " 'Grá mé duit.',\n",
       " 'Tá mé grá le tú.',\n",
       " 'Glan againn ár seomra ranga tar éis na scoile.',\n",
       " 'Nach bhfuil muid bhuail roimh?']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_final = process_trans(trans_ls)\n",
    "t_final[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('tatoeba_processed_translations_exp4_20200810.txt','w')\n",
    "for ele in t_final:\n",
    "    f.write(ele+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df['translation'] = t_final\n",
    "t_df.to_csv('tatoeba_with_translation_exp{exp}_20200810.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ga</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>Is féidearthacht é domhan eile.</td>\n",
       "      <td>Is domhan eile is féidir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Tá feabhas ar an aimsir.</td>\n",
       "      <td>Tá an aimsir feabhas níos fearr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>Níl a fhios agam go díreach.</td>\n",
       "      <td>Ní fhios agam go díreach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>Scríobh chugam.</td>\n",
       "      <td>Scríobh mé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>Cheap cailín a méara sa doras.</td>\n",
       "      <td>a gafa cailín a gafa a xxunk sa doras.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>An bhfuil Béarla aige?</td>\n",
       "      <td>An bhfuil sé labhairt Béarla?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>Ní fhaca Tomás rud ar bith.</td>\n",
       "      <td>Tom chonaic aon rud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Tá na laethanta ag éirí níos faide agus níos faide.</td>\n",
       "      <td>Na laethanta atá ag fáil níos faide agus níos faide.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Táim i ngrá leat.</td>\n",
       "      <td>Grá mé duit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>Ná féach ar Tom! Féach orm.</td>\n",
       "      <td>Ná breathnú ar Tom. Féach mé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>Bhí Tom ag iarraidh mé gar a dhéanamh dó.</td>\n",
       "      <td>Theastaigh dom a dhéanamh dó bhfabhar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>Is féidir le gach duine páirt a ghlacadh.</td>\n",
       "      <td>Is féidir le duine ar bith páirt a ghlacadh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>Déan é as do neart féin</td>\n",
       "      <td>An bhfuil sé amach as do neart féin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>Tá oraibh dul.</td>\n",
       "      <td>Ní mór duit dul.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>Níl a fhios agam rud ar bith.</td>\n",
       "      <td>Ní fhios agam aon rud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Ní chanaim.</td>\n",
       "      <td>Ní féidir liom a shíniú.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>Tá sibh ag cócaireacht.</td>\n",
       "      <td>Tá tú cócaireachta.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>Úsáideann mé an ríomhaire sin.</td>\n",
       "      <td>Bain úsáid mé ríomhaire sin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>Chuir an rud a dúirt Tom fearg ort, nár chuir?</td>\n",
       "      <td>Cad a dúirt Tom a rinne tú feargach, ní raibh sé?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>Ní bhfuair mo bhean bás.</td>\n",
       "      <td>Ní raibh mo bhean chéile bás.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       ga  \\\n",
       "393                       Is féidearthacht é domhan eile.   \n",
       "700                              Tá feabhas ar an aimsir.   \n",
       "914                          Níl a fhios agam go díreach.   \n",
       "1396                                      Scríobh chugam.   \n",
       "1713                       Cheap cailín a méara sa doras.   \n",
       "467                                An bhfuil Béarla aige?   \n",
       "1039                          Ní fhaca Tomás rud ar bith.   \n",
       "100   Tá na laethanta ag éirí níos faide agus níos faide.   \n",
       "3                                       Táim i ngrá leat.   \n",
       "788                           Ná féach ar Tom! Féach orm.   \n",
       "1674            Bhí Tom ag iarraidh mé gar a dhéanamh dó.   \n",
       "844             Is féidir le gach duine páirt a ghlacadh.   \n",
       "1791                              Déan é as do neart féin   \n",
       "1182                                       Tá oraibh dul.   \n",
       "908                         Níl a fhios agam rud ar bith.   \n",
       "409                                           Ní chanaim.   \n",
       "637                               Tá sibh ag cócaireacht.   \n",
       "1956                       Úsáideann mé an ríomhaire sin.   \n",
       "1149       Chuir an rud a dúirt Tom fearg ort, nár chuir?   \n",
       "740                              Ní bhfuair mo bhean bás.   \n",
       "\n",
       "                                               translation  \n",
       "393                              Is domhan eile is féidir.  \n",
       "700                       Tá an aimsir feabhas níos fearr.  \n",
       "914                              Ní fhios agam go díreach.  \n",
       "1396                                           Scríobh mé.  \n",
       "1713                a gafa cailín a gafa a xxunk sa doras.  \n",
       "467                          An bhfuil sé labhairt Béarla?  \n",
       "1039                                  Tom chonaic aon rud.  \n",
       "100   Na laethanta atá ag fáil níos faide agus níos faide.  \n",
       "3                                             Grá mé duit.  \n",
       "788                          Ná breathnú ar Tom. Féach mé.  \n",
       "1674                Theastaigh dom a dhéanamh dó bhfabhar.  \n",
       "844           Is féidir le duine ar bith páirt a ghlacadh.  \n",
       "1791                  An bhfuil sé amach as do neart féin   \n",
       "1182                                      Ní mór duit dul.  \n",
       "908                                 Ní fhios agam aon rud.  \n",
       "409                               Ní féidir liom a shíniú.  \n",
       "637                                    Tá tú cócaireachta.  \n",
       "1956                          Bain úsáid mé ríomhaire sin.  \n",
       "1149     Cad a dúirt Tom a rinne tú feargach, ní raibh sé?  \n",
       "740                          Ní raibh mo bhean chéile bás.  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df[['ga','translation']].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SacreBLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.334313545254766\n"
     ]
    }
   ],
   "source": [
    "refs = [t_df.ga.values.tolist()]\n",
    "sys = t_df.translation.values.tolist()\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(sys, refs)\n",
    "print(bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47239173591863887\n"
     ]
    }
   ],
   "source": [
    "chrf = sacrebleu.corpus_chrf(sys, refs)\n",
    "print(chrf.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'SacreBLEU':bleu.score, 'chrF':chrf.score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect top losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "# at 30k tokens per vocab sometimes this works, sometimes it doesn't\n",
    "\n",
    "# Couldnt process 30k tokens until I added the 'hi' below, it was getting stuck at 94.87%, no idea why\n",
    "@Numericalize\n",
    "def encodes(self, o): \n",
    "    print('hi')\n",
    "    return TensorText(tensor([self.o2i  [o_] for o_ in o]))\n",
    "\n",
    "class floatify_tfm(Transform):\n",
    "    def encodes(self,o): return o.float()\n",
    "    def decodes(self,o): return o.long()\n",
    "\n",
    "max_vocab=30000\n",
    "#splits = ColSplitter()(df) \n",
    "splits = RandomSplitter(valid_pct=0.2, seed=42)(df)\n",
    "splits = (splits[0], splits[1][:2000])\n",
    "\n",
    "tfms = [[Tokenizer.from_df(text_cols='en' , rules=proc_rules), attrgetter(\"text\"), Numericalize(max_vocab=max_vocab)], \n",
    "       [Tokenizer.from_df(text_cols='ga', lang='ga', rules=proc_rules), attrgetter(\"text\"), Numericalize(max_vocab=max_vocab)]]\n",
    "\n",
    "dl = partial(SortedDL, shuffle=True, res=df.ga_len.values)\n",
    "\n",
    "dsets = Datasets(df, tfms, splits=splits, dl_type=dl)\n",
    "\n",
    "# remove the print from Numericalize\n",
    "@Numericalize\n",
    "def encodes(self, o): return TensorText(tensor([self.o2i  [o_] for o_ in o]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos ( i ) the application of any goods of a kind specified in the xxmaj fourth xxmaj schedule by a person for the purposes of his business and treated as delivered in accordance with section 3 ( 1 ) ( e ) , xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos ( i ) le duine do bhaint úsáid chun críocha a ghnó as aon earraí de chineál a shonraítear sa xxmaj cheathrú xxmaj sceideal agus a áirítear mar earraí arna seachadadh de réir alt 3 ( 1 ) ( e ) , xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxup xxunk . xxmaj population aged one year and over , usually resident and present in the xxmaj state , whose usual residence one year previously was outside the xxmaj state , classified by former country of usual residence , sex and distinguishing those with xxmaj irish nationality or other nationality xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos › xxup xxunk . xxmaj daonra xxmaj gnáthchónaitheoirí xxmaj aon xxmaj bliain d'aois agus xxmaj níos xxmaj sine a bhí i xxmaj láthair sa xxmaj stát de réir xxmaj náisiúntacht , xxmaj inscne , xxmaj gnátháit xxmaj chónaithe xxmaj bliain xxmaj roimhe agus bliaindaonáirimh › xxup xxunk . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos ( a ) the constituencies for the election of members to the xxmaj dáil , and xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos ( a ) na dáilcheantair chun comhaltaí a thoghadh chun na xxmaj dála , agus xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos “ but as climate change increases the frequency and severity of droughts and floods and makes food more difficult to produce , we need innovative solutions to support communities on the frontline . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos \" ach mar a ardaíonn an t - athrú aeráide minicíocht agus déine na xxunk agus na dtuilte agus dhéanann sé bia níos deacra le táirgeadh , is gá réitigh nuálacha chun tacú le pobail ar an líne thosaigh . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxmaj member xxmaj states : xxmaj germany , xxmaj france , xxmaj italy , the xxmaj netherlands , xxmaj belgium and xxmaj luxembourg . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos xxmaj ballstáit : xxmaj an xxmaj ghearmáin , an xxmaj fhrainc , an xxmaj iodáil , an ísiltír , an xxmaj bheilg agus xxmaj lucsamburg . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos ( b ) contributions in respect of that service have been returned to him , xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos ( b ) go mbeifear tar éis ranníoca i leith na seirbhíse sin a thabhairt ar ais dó , xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos xxmaj this website is a national shared service for the collection of the charge for xxmaj non xxmaj principal xxmaj private xxmaj residences on behalf of the local authorities . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos xxmaj is seirbhís náisiúnta chomhroinnte an láithreán gréasáin seo chun an muirear d’áiteanna xxmaj cónaithe xxmaj príobháideacha neamhphríomha ar son na n - údarás áitiúil a bhailiú . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos ( 2 ) xxmaj the xxmaj minister may order that a person committed under section 14 be released from custody if the xxmaj minister is of the opinion that a request for the person 's surrender is not being proceeded with . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos ( 2 ) xxmaj féadfaidh an taire a ordú duine a cimíodh faoi alt 14 a scaoileadh saor ó choimeád más é tuairim an xxmaj aire nach bhfuiltear ag dul ar aghaidh le hiarraidh chun an duine a thabhairt suas . xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos xxmaj posted on xxmaj august 28 , 2013 by xxmaj kaia xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos by xxmaj kaia xxmaj postáilte ar 28 xxmaj lúnasa , 2013 ag xxmaj kaia xxeos xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs,sl = 32, 512\n",
    "dls = dsets.dataloaders(bs=bs, seq_len=sl, before_batch=partial(pad_input, pad_fields=[0,1]))\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eos_idx=3\n",
    "dls.valid.bs = 1\n",
    "\n",
    "loss_ls = []\n",
    "en_ls, ga_trg_ls, ga_pred_ls = [], [], []\n",
    "\n",
    "for xb, yb in dls.valid:\n",
    "    yy = yb\n",
    "    eos_mask=(yb!=eos_idx)\n",
    "    sz=torch.tensor(yb.size())\n",
    "    sz[1]=sz[1]-1\n",
    "    yb = yb[eos_mask].view((sz[0],sz[1]))  # drop the last token (\"eos\") for training \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred_logits = model.forward(src=xb, trg=yb)\n",
    "    \n",
    "    yb_loss = yy[:,1:]  # shift target to exclude xxbos\n",
    "    loss = learn.loss_func(y_pred_logits, yb_loss)\n",
    "    loss_ls.append(loss)\n",
    "\n",
    "    y_pred_act = F.softmax(y_pred_logits, dim=-1)\n",
    "    \n",
    "    preds_ls = []\n",
    "    for p in y_pred_act: \n",
    "        preds_ls.append(p.argmax(dim=-1))\n",
    "    \n",
    "    tmp_ls = []\n",
    "    for i in xb[0]:\n",
    "        if (dls.vocab[0][i] != 'xxpad') and (dls.vocab[1][i] != 'xxbos'):\n",
    "            tmp_ls.append(dls.vocab[0][i])\n",
    "    en_ls.append(' '.join(tmp_ls))\n",
    "\n",
    "    tmp_ls = []\n",
    "    for i in yb[0]:\n",
    "        if (dls.vocab[1][i] != 'xxpad') and (dls.vocab[1][i] != 'xxbos'):\n",
    "            tmp_ls.append(dls.vocab[1][i])\n",
    "    ga_trg_ls.append(' '.join(tmp_ls))\n",
    "\n",
    "    tmp_ls = []\n",
    "    for i in preds_ls[0]:\n",
    "        if dls.vocab[1][i] != 'xxpad':\n",
    "            tmp_ls.append(dls.vocab[1][i])\n",
    "    ga_pred_ls.append(' '.join(tmp_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.8346, device='cuda:0'), tensor(0.0034, device='cuda:0'))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_loss_idx = np.argmax(loss_ls)\n",
    "min_loss_idx = np.argmin(loss_ls)\n",
    "loss_ls[max_loss_idx], loss_ls[min_loss_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorted indices by loss, (highest loss to lowest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sorted_idxs = np.argsort(-np.array(loss_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show `n` top losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 4.834611415863037\n",
      "xxmaj we are currently experiencing technical difficulties with our subtitles on some browsers . xxeos\n",
      "\n",
      "xxup tá deacrachtaí le fotheidil ar roinnt ‘ xxunk ’ faoi láthair .\n",
      "\n",
      "xxmaj tá muid teicniúla deacrachtaí teicniúla ár brabhsálaithe brabhsálaithe faoi faoi láthair atá xxeos\n",
      "\n",
      "\n",
      "LOSS: 4.4953813552856445\n",
      "frosted green cosmetic glass bottle with silver … xxeos\n",
      "\n",
      "frosted buidéal dropper gloine glas do cosmaideacha …\n",
      "\n",
      "xxmaj cosmaideacha gloine cosmaideacha cosmaideacha cosmaideacha airgid le xxeos\n",
      "\n",
      "\n",
      "LOSS: 4.347179412841797\n",
      "xxmaj not even all the end of compulsory schooling despite compulsory schooling . xxeos\n",
      "\n",
      "xxup ní fiú go léir a chuaigh amach bhunscoil ainneoin scolaíocht éigeantach .\n",
      "\n",
      "xxmaj ní fiú amháin léir deireadh deireadh deireadh dheireadh éigeantach in éigeantach in xxeos\n",
      "\n",
      "\n",
      "LOSS: 4.304743766784668\n",
      "p - xxmaj depth of xxmaj foundation xxeos\n",
      "\n",
      "p - xxmaj fondúireacht doimhneacht\n",
      "\n",
      "p - xxmaj doimhneacht na na\n",
      "\n",
      "\n",
      "LOSS: 4.188599586486816\n",
      "xxmaj open 7 days including bank holidays xxeos\n",
      "\n",
      "xxmaj samhradh : xxmaj oscailte 7 lá , laethanta saoire bainc san áireamh\n",
      "\n",
      "7 laethanta 7 laethanta laethanta 7 lá lena laethanta saoire bainc lena áireamh laethanta\n",
      "\n",
      "\n",
      "LOSS: 4.172364234924316\n",
      "xxmaj ok . xxmaj then i reboot and cross the border . xxeos\n",
      "\n",
      "xxmaj ansin leithroinnte agus dtrasnaíonn an teorainn .\n",
      "\n",
      "xxup ok i mé tras an teorainn . xxeos\n",
      "\n",
      "\n",
      "LOSS: 4.111687660217285\n",
      "xxmaj this was equivalent to a 4.5 % unemployment rate , up slightly from 4.4 % in 2002 but still well below the xxup eu average of 8.0 % . xxeos\n",
      "\n",
      "xxmaj b’ionann sin agus 4.5 % ráta dífhostaíochta ardú beagán ó 4.4 % in 2002 ach go maith faoi leibhéal an mheán xxup ae ag 8.0 % . xxmaj d’ardaigh an líon daoine ar dhífhostaíocht fhadtéarmach 1,200 .\n",
      "\n",
      "xxmaj bhí seo agus ráta % xxunk dífhostaíochta , , ón 4.4 % i 2002 ach fós maith faoi bhun an xxup an ae de brath % . xxeos bhí % ráta tí is an % . % xxeos\n",
      "\n",
      "\n",
      "LOSS: 4.084066390991211\n",
      "( i ) substitute “ the actuary to or trustees of ” for “ the trustees of ” , xxeos\n",
      "\n",
      "( i ) “ déanfaidh achtúire nó iontaobhaithe scéime nó iontaobhais xxup cbs ” a chur in ionad “ déanfaidh iontaobhaithe scéime nó iontaobhais xxup cbs ” ,\n",
      "\n",
      "( i ) “ an achtúire do iontaobhaithe ” ” iontaobhaithe ” xxunk iontaobhais a chur in ionad “ iontaobhaithe iontaobhaithe ” ” , ” na ” , xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.9471380710601807\n",
      "xxmaj ensure national mail services packages are posted either without a cover or in a cover which can be easily removed for the purpose of examination . xxeos\n",
      "\n",
      "xxmaj déan cinnte de go seoltar pacáistí gan chlúdach nó i gclúdach atá furasta a bhaint le haghaidh scrúdúcháin sa chóras náisiúnta poist .\n",
      "\n",
      "xxmaj cuirtear pacáistí go pacáistí gcuirfear pacáistí seirbhísí chlúdach nó i xxunk ar féidir a aistriú go go an a phost seirbhísí a a xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.885239362716675\n",
      "a sea change in xxmaj europe 's innovation performance is the only way to create lasting and well - paid jobs that withstand the pressures of globalisation . \" xxeos\n",
      "\n",
      "xxmaj beidh gá le feabhas ó bhonn a dhéanamh ar fheidhmíocht nuálaíochta na heorpa má táthar chun poist bhuana ar phá maith a chruthú , poist a bheidh in ann brú an domhandaithe a sheasamh . \"\n",
      "\n",
      "xxmaj is athrú le hathrú farraige thaobh na xxunk ar fheidhmíocht na san heorpa ar tá chun poist buan buan aghaidh agus agus chruthú a go atá sheasamh sheasamh aice a an domhandú . chur . \" xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.8690526485443115\n",
      "xxmaj heat loss or gains from pipes , xxunk and vessels is to be limited and energy efficient lighting systems provided . xxeos\n",
      "\n",
      "xxup ní mór caillteanas nó gnóthachain teasa trí phíopaí , xxunk agus soithigh a mhaolú agus córas éifeachtach soilse a fheistiú .\n",
      "\n",
      "xxup tá mór córas teasa gnóchain ó ó phíopaí , xxunk agus soithí a bhfuil a a fuinnimh fuinnimh fuinnimh sholáthar ar xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.851304292678833\n",
      "xxmaj provisions in relation to reserve values , transfer values and transfers of insurance . xxeos\n",
      "\n",
      "xxmaj forálacha maidir le cúl - luacha , luacha aistriúcháin agus aistrithe árachais .\n",
      "\n",
      "xxmaj forálacha maidir le luachanna - luachanna , luachanna aistrithe d'aistriú aistrithe luachanna d'aistriú xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.835312604904175\n",
      "xxmaj article xxunk in attack xxeos\n",
      "\n",
      "xxmaj airteagal xxunk i gcás ionsaithe\n",
      "\n",
      "xxmaj airteagal xxunk ionsaí ionsaí ionsaí ionsaí\n",
      "\n",
      "\n",
      "LOSS: 3.74434494972229\n",
      "xxmaj xxunk and licences - construction - road safety - traffic xxmaj management and xxmaj parking xxeos\n",
      "\n",
      "xxmaj xxunk xxmaj tógáil bóthair - sábháilteacht bóthair - bainistíocht xxmaj tráchta\n",
      "\n",
      "xxmaj xxunk agus xxunk agus agus sábháilteacht agus agus agus agus tráchta agus\n",
      "\n",
      "\n",
      "LOSS: 3.710318088531494\n",
      "xxmaj standard link building sources xxeos\n",
      "\n",
      "xxmaj foinsí xxmaj standard tógáil nasc\n",
      "\n",
      "xxmaj foinsí nasc caighdeánach nasc xxmaj tógála\n",
      "\n",
      "\n",
      "LOSS: 3.7060744762420654\n",
      "xxmaj after this the delegates late xxmaj louis xxup xvi to the death , by a majority vote of one vote . xxeos\n",
      "\n",
      "xxmaj tar éis seo an xxmaj toscairí déanach xxmaj louis xxup xvi xxmaj chun an xxmaj bás , xxmaj de a xxmaj formhór na xxmaj vóta haon .\n",
      "\n",
      "xxmaj tar éis na na xxmaj louis xxmaj xxmaj louis xxup xvi le louis an bháis bháis , trí vóta vóta vóta vóta vóta vóta vóta amháin vóta xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.6944961547851562\n",
      "17 . a sensible housing policy in xxmaj xxunk . xxeos\n",
      "\n",
      "17 . xxmaj polasaí ceart tithíochta i mbearna .\n",
      "\n",
      "17 . xxmaj polasaí tithíochta tithíochta ciallmhar xxmaj xxmaj xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.6918556690216064\n",
      "xxup eu budget 2014 by financial framework heading xxeos\n",
      "\n",
      "xxmaj buiséad 2014 de réir na gceannteideal atá sa chreat airgeadais\n",
      "\n",
      "xxmaj buiséad an ag réir réigiúin creat airgeadais ann xxup airgeadais xxup\n",
      "\n",
      "\n",
      "LOSS: 3.656128406524658\n",
      "3.1 xxmaj the purpose of this chapter is to give a brief review of the more important or interesting decisions and developments in the area of criminal law in 2007 . xxeos\n",
      "\n",
      "3.1 xxmaj is é cuspóir atá leis an gcaibidil seo cuntas gairid a thabhairt ar chinntí agus ar chora xxunk eile den tábhacht maidir le réimse dhlí na coireachta i rith na bliana 2007 .\n",
      "\n",
      "3.1 xxmaj is é is na leis an gcaibidil seo athbhreithniú gearr ar thabhairt ar na níos forbairtí na is nó nó dlí níos le forbairtí an coiriúil coiriúil in 2007 2007 bliana 2007 . xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.5991711616516113\n",
      "( 5 ) fire or water hose fittings of the following descriptions : xxeos\n",
      "\n",
      "( 5 ) feistisí feadán tóiteáin no feadán uisce de sna xxunk so leanas :\n",
      "\n",
      "( 5 ) xxup xxunk dóiteáin nó uisce uisce de sna saghsanna seo a : xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.5880415439605713\n",
      "xxmaj this play was first performed in 1979 and is a xxunk look at xxmaj irish society , still relevant today . xxeos\n",
      "\n",
      "xxmaj tugann an dráma seo , a léiríodh ar dtús i 1979 , léargas xxunk ar shochaí na héireann , léargas atá fós suntasach inniu .\n",
      "\n",
      "xxmaj an an spraoi seo an ar bhí i dtús i 1979 , agus xxunk ar chumann na héireann , agus ábhartha ábhartha ar sa . xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.5831832885742188\n",
      "xxmaj first elections of members of certain public assistance authorities . xxeos\n",
      "\n",
      "xxmaj céad - toghcháin chomhaltaí údarásanna conganta xxunk áirithe .\n",
      "\n",
      "xxmaj na toghcháin toghcháin do údarás cúnaimh phuiblí áirithe . xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.514817714691162\n",
      "xxmaj withdrawal xxmaj date of v 1 xxmaj assessments xxeos\n",
      "\n",
      "xxmaj dáta xxmaj xxunk xxmaj measúnaithe l 1\n",
      "\n",
      "xxmaj dáta xxmaj siar v measúnachtaí xxup xxeos xxmaj\n",
      "\n",
      "\n",
      "LOSS: 3.4614880084991455\n",
      "( ii ) certifies that the person , although the person ’s condition is not such as to require the person ’s hospitalisation , is unfit for any questioning for the purpose of the investigation for a specified period , xxeos\n",
      "\n",
      "( ii ) go ndeimhníonn sé nach xxunk an duine chun críche an imscrúdaithe go ceann tréimhse sonraithe , d’ainneoin nach gá an duine a chur isteach in ospidéal de dheasca na baile atá air ,\n",
      "\n",
      "( ii ) go ndeimhneoidh an nó bhfuil an duine , a aon imscrúdaithe ar bhfuil tréimhse sonraithe , cé nach amhlaidh don duine chun cheangal i ar imthosca an shórt xxunk xxunk a sheachaint , xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.3972277641296387\n",
      "xxmaj shoot 2 : xxmaj cruise xxmaj control xxeos\n",
      "\n",
      "xxmaj shoot 2 : cúrsála rialaithe\n",
      "\n",
      "xxmaj shoot 2 : xxmaj xxmaj xxmaj\n",
      "\n",
      "\n",
      "LOSS: 3.344511032104492\n",
      "xxmaj the blue flag campaign today flies in 13 different countries having started life in xxmaj france in 1987 . xxeos\n",
      "\n",
      "xxmaj san xxmaj fhrainc a thosaigh an feachtas seo i 1987 agus anois tá an brat gorm ar foluain i 13 thír éagsúla .\n",
      "\n",
      "xxmaj déanann fheachtas eoraip i cuireadh an feachtas bratach caite 1987 , cuileoga i na feachtas gorm i xxmaj i 13 thír éagsúla a xxmaj\n",
      "\n",
      "\n",
      "LOSS: 3.3257603645324707\n",
      "xxmaj contingency allowances given to compensate for the time required by the workers to perform all necessary additional and periodic activities , e.g. reading drawings , cleaning machinery etc . xxeos\n",
      "\n",
      "liúntais teagmhasacha a tugadh mar chúiteamh ar an am is gá ag na hoibrithe chun gach gníomhaíocht breise agus tréimhsiúla gá , m.sh. líníochtaí léamh , glanadh innealra srl\n",
      "\n",
      "xxmaj teagmhais na thabhairt mar chúiteamh don an am a gá de na hoibrithe chun gach gníomhaíocht bhreise agus thréimhsiúil a a m.sh. ag , , innealra etc etc .\n",
      "\n",
      "\n",
      "LOSS: 3.311084747314453\n",
      "relationship , and supply chain processes . xxeos\n",
      "\n",
      "caidreamh , agus slabhra soláthair próisis .\n",
      "\n",
      "le , próisis próisis soláthair próisis slabhra xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.306642532348633\n",
      "xxmaj in 1963 introduced the first helicopters into service in the xxmaj state and within one year provided a daytime xxmaj search and xxmaj rescue service and within a further year established an inter - hospital air ambulance service , the first of its kind in xxmaj europe . xxeos\n",
      "\n",
      "xxmaj na chéad héileacaptair a thionscnamh i mbun seirbhíse sa xxmaj stát sa bhliain 1963 : soláthraíodh seirbhís xxmaj chuardaigh agus xxmaj tarrthála lae faoi cheann bliana agus bunaíodh seirbhís aerárthaigh othar idir - ospidéil faoi cheann bliana eile , an chéad seirbhís dá leithéid san xxmaj eoraip .\n",
      "\n",
      "i thug chéad uair a tugadh i seirbhís seirbhíse sa xxmaj stát agus bhliain 1963 agus xxmaj an xxmaj cuardaigh agus xxmaj tarrthála agus agus cheann bliana agus laistigh seirbhís ospidéil idir idir - ospidéil bhreise láthair dá breise , an chéad cheann dá chineál san xxmaj eoraip . xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.2318711280822754\n",
      "“ in the 25 years since our previous logo was developed , the world has moved from the printed page to an open fluid digital environment where more and more people have access to data . xxeos\n",
      "\n",
      "“ le linn an 25 bliana ó forbraíodh ár sean - lógó , tá an saol mór tar éis bogadh ar aghaidh ó cháipéisí clóite go timpeallacht oscailte solúbtha digiteach ina bhfuil rochtain ar sonraí ag líon méadaithe daoine .\n",
      "\n",
      "“ sa linn an 25 bliana ó rinneadh ár lógó - lógó , tá an domhan bhog ar éis a ón an ar leathanach clóite go dtí digiteach digiteach i i bhfuil rochtain níos níos agus níos daoine agus agus xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.222780704498291\n",
      "xxmaj it takes into account the extent to which individual students ’ characteristics such as gender , age , socio - economic background and prior educational attainment , have an impact on progression . xxeos\n",
      "\n",
      "xxmaj cuireann sé san áireamh an tionchar atá ag éagsúlacht thréithe na mac léinn ar an xxunk , leithéidí inscne , aois , cúlra socheacnamaíoch agus xxunk oideachasúil .\n",
      "\n",
      "xxmaj cuirtear sí san áireamh a méid ar ag mic na na mic léinn aonair leith xxunk , ar , , aois , cúlra eacnamaíoch agus roimh oideachais roimh sula\n",
      "\n",
      "\n",
      "LOSS: 3.1986210346221924\n",
      "xxmaj look at some other items of cutlery . xxmaj can you make drawings of them ? xxeos\n",
      "\n",
      "xxmaj an féidir leat cur síos a dhéanamh air ? xxmaj déan líníocht de . xxmaj amharc ar roinnt míreanna eile sceanra .\n",
      "\n",
      "xxmaj féach féidir leat líníochtaí isteach ar dhéanamh ar ? xxmaj féachaint tú ar roinnt xxmaj is ar roinnt míreanna eile de ? xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.1934149265289307\n",
      "but does not include — xxeos\n",
      "\n",
      "ach ní fholuíonn sí —\n",
      "\n",
      "ach ní fholaíonn sé — ní\n",
      "\n",
      "\n",
      "LOSS: 3.170163154602051\n",
      "xxmaj in all places that were affected by lightning discharges electricity . xxeos\n",
      "\n",
      "i ngach áit xxmaj bhí tionchar ag go tintreach leictreachas sceitheadh .\n",
      "\n",
      "i ngach áiteanna go go tionchar ag scaoileadh raibh scaoileadh scaoilte leictreachais xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.164721727371216\n",
      "xxmaj since 2004 the works programme has focused on preserving the extensive structural remains on the xxmaj south xxmaj peak . xxeos\n",
      "\n",
      "á “ 2004 , xxunk clár na n - oibreacha ar xxunk fairsinge na struchtúr ar an xxunk xxmaj theas a chaomhnú .\n",
      "\n",
      "ó xxup tá tá tá an oibreacha n - oibreacha ar an an atá n mór an xxmaj xxmaj theas a chaomhnú . xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.1545069217681885\n",
      "xxmaj please ensure that you provide complete and accurate information ( e.g. on dates of birth and xxup pps numbers ) as failure to do this will delay processing of your application . xxeos\n",
      "\n",
      "xxmaj cinntigh , le do thoil , go soláthraíonn tú faisnéis shoiléir agus chruinn ( dátaí breithe , uimhreacha xxup psp agus sonraí bainc go háirithe ) . xxup má theipeann ort é seo a dhéanamh , beidh moill ar phróiseáil d’iarratais .\n",
      "\n",
      "xxmaj déan go go do thoil go go xxunk tú faisnéis iomlán ( cruinn ( e.g breithe agus agus xxup psp ) uimhreacha xxup xxup mbeidh ) mar xxmaj má dhéantar ar an seo a dhéanamh tuilleadh xxunk tú ar d’iarratas d . xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.147977828979492\n",
      "xxmaj fan circulation xxunk optimized for higher efficient heat transfer in the low - temperature application . xxeos\n",
      "\n",
      "xxunk le haghaidh níos airde aistriú teasa éifeachtach i gcur i bhfeidhm íseal\n",
      "\n",
      "xxmaj xxmaj scaipeadh aistriú airde aistriú teasa níos ó bhfeidhm i bhfeidhm teocht –\n",
      "\n",
      "\n",
      "LOSS: 3.143120765686035\n",
      "xxmaj but xxmaj plague knocked people from all those different stalls , but not as many suffered by those of them poor . xxeos\n",
      "\n",
      "xxmaj ach leag plague daoine ó siúd go léir stallaí éagsúla , ach bhí tionchar nach mar go leor acu siúd acu bochta .\n",
      "\n",
      "xxmaj ach tá xxmaj daoine ó gach go léir stallaí éagsúla , ach ní nach ag bhfuil chuid leor ag siúd iad bochta . xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.1297659873962402\n",
      "xxmaj poor standards of communication and documentation xxeos\n",
      "\n",
      "droch - chaighdeáin cumarsáide agus doiciméadú\n",
      "\n",
      "xxmaj - caighdeáin na agus doiciméid cumarsáide\n",
      "\n",
      "\n",
      "LOSS: 3.117694139480591\n",
      "xxmaj those who look to achieve the physique of their dreams can look no further than these xxunk , time - tested brands . xxeos\n",
      "\n",
      "xxmaj is féidir leo siúd a thugann aire a bhaint amach an physique a n - aisling breathnú níos faide ná seo xxunk , ama de réir tástála acmhainne brandaí .\n",
      "\n",
      "xxmaj iad féidir leo siúd a breathnú chun a bhaint amach ar dá a bhaint - aisling a a mó ná na xxunk , am - na na - . ama xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.1128244400024414\n",
      "xxmaj this progression is part of the developmental process for the young sailor . xxeos\n",
      "\n",
      "xxmaj is cuid de phróiseas forbartha an mhairnéalaigh óig an dul chun cinn seo .\n",
      "\n",
      "xxmaj is dul den phróiseas forbartha an xxunk do do ea chun cinn seo . xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.0744216442108154\n",
      "xxmaj are another generation of xxmaj xxunk children to miss out on the benefits of an all - irish education or will the xxmaj department of xxmaj education and xxmaj skills do the right thing and support xxmaj gaelscoil xxmaj ráth xxup xxunk by giving it official recognition ? ” xxeos\n",
      "\n",
      "“ an bhfuil glúin eile de pháistí xxmaj ráth xxup tó chun an deis sin a chailliúint arís , nó bhfuil an xxmaj roinn xxmaj oideachais agus xxmaj scileanna chun an rud cheart a dhéanamh agus tacú le xxmaj gaelscoil xxmaj ráth xxup tó trí aitheantas a thabhairt di ?\n",
      "\n",
      "xxmaj tá bhfuil glúin eile de leanaí xxmaj xxunk xxmaj xxunk chun aitheantas t a a chailleann as ar ar go an xxmaj roinn xxmaj oideachais agus xxmaj scileanna ag an ceart ceart agus chur agus tacaíocht le xxmaj creidiúnaithe xxmaj ráth xxup xxunk trí aithint oifigiúil thabhairt dó oifigiúil ”\n",
      "\n",
      "\n",
      "LOSS: 3.034301996231079\n",
      "xxmaj the marathon cycle will take place on 28 xxmaj april , so there 's plenty of time to train for the event . xxeos\n",
      "\n",
      "xxmaj beidh an rothaíocht ar siúl ar 28 xxmaj aibreán ; mar sin tá xxunk ama agat le haghaidh traenála chuige .\n",
      "\n",
      "xxmaj beidh an timthriall xxunk siúl an an xxmaj aibreán , agus sin , neart am go chun haghaidh oiliúint ar . xxeos\n",
      "\n",
      "\n",
      "LOSS: 3.0320944786071777\n",
      "xxmaj of the farmers needed more days to erect large structures , such as temple pyramids . xxeos\n",
      "\n",
      "xxmaj as na feirmeoirí ag teastáil níos mó agus níos mó lá gnó le struchtúir mhóra , ar nós pirimidí teampall in airde .\n",
      "\n",
      "xxmaj as na feirmeoirí a teastáil níos mó lá struchtúir mó struchtúir a a struchtúir móra a mar nós na teampall . áirithe . xxeos\n",
      "\n",
      "\n",
      "LOSS: 2.9994633197784424\n",
      "xxmaj undergraduate , xxmaj master 's or phd students xxeos\n",
      "\n",
      "xxmaj mic léinn bunchéime , xxmaj máistreachta nó phd\n",
      "\n",
      "xxmaj mic léinn xxmaj , xxmaj máistir nó mic ,\n",
      "\n",
      "\n",
      "LOSS: 2.971003532409668\n",
      "xxmaj located in the heart of xxmaj europe , several other xxmaj european capitals are only a couple of hours away ( amsterdam , xxmaj paris , xxmaj london ) . xxeos\n",
      "\n",
      "xxmaj is i gcroílár na heorpa atá siad suite , agus níl roinnt príomhchathracha eile ( amstardam , xxmaj páras , xxmaj londain ) ach cúpla uair an chloig taistil uathu .\n",
      "\n",
      "xxmaj suite lánúin gcroílár na heorpa , roinnt , ach agus tá cúpla xxmaj xxmaj xxmaj xxmaj ) xxmaj amstardam , xxmaj londain ) . cúpla uair an chloig de ar . xxeos\n",
      "\n",
      "\n",
      "LOSS: 2.9318175315856934\n",
      "a key work - stream in this unit relates to assessing the xxmaj irish economic implications of xxmaj brexit . xxeos\n",
      "\n",
      "xxmaj ceann de xxunk oibre an aonaid seo ná na himpleachtaí eacnamaíocha a bheidh ag xxmaj imeacht na xxmaj breataine as an xxmaj aontas xxmaj eorpach do éirinn a mheas .\n",
      "\n",
      "xxmaj baineann de na oibre - t seo , baineann xxunk eacnamaíochta na bhaineann ag xxmaj xxunk xxmaj héireann gaeilge - xxmaj xxmaj aontas xxmaj eorpach a bhaint a mheasúnú . xxeos\n",
      "\n",
      "\n",
      "LOSS: 2.9126083850860596\n",
      "xxmaj home page » xxmaj games » xxmaj mutually xxmaj assured xxmaj destruction ( mad ) xxeos\n",
      "\n",
      "xxmaj leathanach xxmaj baile » xxmaj cluichí » xxmaj scrios roicéad\n",
      "\n",
      "xxmaj leathanach xxmaj baile » xxmaj cluichí » xxmaj cinnte xxmaj xxmaj\n",
      "\n",
      "\n",
      "LOSS: 2.9069716930389404\n",
      "xxmaj automatic recording of sickness and absence . xxeos\n",
      "\n",
      "xxmaj breoiteacht agus as láthair taifeadadh go huathoibríoch .\n",
      "\n",
      "xxmaj taifeadadh xxunk taifeadadh láthair a uathoibríoch huathoibríoch . xxeos\n",
      "\n",
      "\n",
      "LOSS: 2.895787239074707\n",
      "xxmaj it shall be embossed with the stamp of the military authority . xxeos\n",
      "\n",
      "xxmaj beidh stampa an údaráis mhíleata múnlaithe air .\n",
      "\n",
      "xxmaj beidh sé ar údaráis mhíleata uirthi . . xxeos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_losses = 50\n",
    "top_losses = loss_sorted_idxs[:n_losses]\n",
    "\n",
    "for i in top_losses:\n",
    "    print(f'LOSS: {float(loss_ls[i])}')\n",
    "    print(en_ls[i])\n",
    "    print()\n",
    "    print(ga_trg_ls[i])\n",
    "    print()\n",
    "    print(ga_pred_ls[i])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show smallest `n` losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.012944857589900494\n",
      "( 2 ) xxmaj this xxmaj act shall come into operation on the 1st day of xxmaj january , 1937 . xxeos\n",
      "\n",
      "( 2 ) xxmaj tiocfaidh an tacht so i ngníomh an 1adh lá d'eanar , 1937 .\n",
      "\n",
      "( 2 ) xxmaj tiocfaidh an tacht so i ngníomh an 1adh lá d'eanar , 1937 . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.012784866616129875\n",
      "( f ) by substituting the following for paragraph 2 of xxmaj schedule 3 : xxeos\n",
      "\n",
      "( f ) tríd an méid seo a leanas a chur in ionad mhír 2 de xxmaj sceideal 3 :\n",
      "\n",
      "( f ) tríd an méid seo a leanas a chur in ionad mhír 2 de xxmaj sceideal 3 : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.012429873459041119\n",
      "xxmaj amendment of section 96 of xxmaj act of 2001 . xxeos\n",
      "\n",
      "xxmaj leasú ar alt 96 d’acht 2001 .\n",
      "\n",
      "xxmaj leasú ar alt 96 d’acht 2001 . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.012094180099666119\n",
      "11 . — this xxmaj act may be cited as the xxmaj transport ( miscellaneous xxmaj provisions ) xxmaj act , 1979 . xxeos\n",
      "\n",
      "11 . — féadfar an tacht xxmaj iompair ( forálacha xxmaj ilghnéitheacha ) , 1979 , a ghairm den xxmaj acht seo .\n",
      "\n",
      "11 . — féadfar an tacht xxmaj iompair ( forálacha xxmaj ilghnéitheacha ) , 1979 , a ghairm den xxmaj acht seo . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.01209115982055664\n",
      "93 . xxmaj amendment of section 1 ( interpretation ) of xxmaj principal xxmaj act . xxeos\n",
      "\n",
      "93 . xxmaj leasú ar alt 1 ( léiriú ) den phríomh - acht .\n",
      "\n",
      "93 . xxmaj leasú ar alt 1 ( léiriú ) den phríomh - acht . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.011882972903549671\n",
      "[ ga ] ( xxrep 3 i ) by inserting the following after paragraph 2 : xxeos\n",
      "\n",
      "( xxrep 3 i ) tríd an méid seo a leanas a chur isteach i ndiaidh mhír 2 :\n",
      "\n",
      "( xxrep 3 i ) tríd an méid seo a leanas a chur isteach i ndiaidh mhír 2 : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.011682409793138504\n",
      "xxmaj number 32 / 2009 : xxmaj criminal xxmaj justice ( amendment ) xxmaj act 2009 xxmaj home xxeos\n",
      "\n",
      "xxmaj uimhir 32 / 2009 : xxmaj an tacht um xxmaj cheartas xxmaj coiriúil ( leasú ) 2009\n",
      "\n",
      "xxmaj uimhir 32 / 2009 : xxmaj an tacht um xxmaj cheartas xxmaj coiriúil ( leasú ) 2009 xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.011207898147404194\n",
      "xxmaj amendment to section 20 of xxmaj act of 2003 . xxeos\n",
      "\n",
      "xxmaj leasú ar alt 20 d’acht 2003 .\n",
      "\n",
      "xxmaj leasú ar alt 20 d’acht 2003 . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.01114108320325613\n",
      "xxmaj central xxmaj bank xxmaj act , 1971 . xxeos\n",
      "\n",
      "xxmaj acht an xxmaj bhainc xxmaj ceannais , 1971 .\n",
      "\n",
      "xxmaj acht an xxmaj bhainc xxmaj ceannais , 1971 . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.01099395751953125\n",
      "“ the xxmaj act of 1995 ” means the xxmaj industrial xxmaj development xxmaj act , 1995 ; xxeos\n",
      "\n",
      "ciallaíonn “ acht 1995 ” an tacht um xxmaj fhorbairt xxmaj tionscail , 1995 ;\n",
      "\n",
      "ciallaíonn “ acht 1995 ” an tacht um xxmaj fhorbairt xxmaj tionscail , 1995 ; xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.010816644877195358\n",
      "45 . — ( 1 ) xxmaj the xxmaj principal xxmaj act is amended by inserting the following section after section 611 : xxeos\n",
      "\n",
      "45 . — ( 1 ) xxmaj leasaítear an príomh - acht tríd an alt seo a leanas a chur isteach i ndiaidh alt 611 :\n",
      "\n",
      "45 . — ( 1 ) xxmaj leasaítear an príomh - acht tríd an alt seo a leanas a chur isteach i ndiaidh alt 611 : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.010740697383880615\n",
      "( xxrep 3 i ) which is not a scheduled ( part xxup i ) pension , and xxeos\n",
      "\n",
      "( xxrep 3 i ) nach pinsean sceidealta ( cuid xxup i ) , agus\n",
      "\n",
      "( xxrep 3 i ) nach pinsean sceidealta ( cuid xxup i ) , agus xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.010490857996046543\n",
      "( 2 ) xxmaj section 11 of the xxmaj act of 1997 is hereby repealed . xxeos\n",
      "\n",
      "( 2 ) xxmaj aisghairtear leis seo alt 11 d'acht 1997 .\n",
      "\n",
      "( 2 ) xxmaj aisghairtear leis seo alt 11 d'acht 1997 . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.010299509391188622\n",
      "22 . — xxmaj the xxmaj second xxmaj schedule to the xxmaj courts and xxmaj court xxmaj officers xxmaj act 1995 is amended by the addition of the following paragraph : xxeos\n",
      "\n",
      "22 . — xxmaj leasaítear an xxmaj dara xxmaj sceideal a ghabhann le hacht na gcúirteanna agus na noifigeach xxmaj cúirte 1995 tríd an mír seo a leanas a chur leis :\n",
      "\n",
      "22 . — xxmaj leasaítear an xxmaj dara xxmaj sceideal a ghabhann le hacht na gcúirteanna agus na noifigeach xxmaj cúirte 1995 tríd an mír seo a leanas a chur leis : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.010137557983398438\n",
      "xxmaj sex ( 3 ) xxmaj general xxmaj health ( 7 ) xxmaj field of xxmaj study ( 45 ) xxeos\n",
      "\n",
      "xxmaj inscne ( 3 ) xxmaj sláinte xxmaj ginearálta ( 7 ) xxmaj réimse xxmaj staidéir ( 45 )\n",
      "\n",
      "xxmaj inscne ( 3 ) xxmaj sláinte xxmaj ginearálta ( 7 ) xxmaj réimse xxmaj staidéir ( 45 ) xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.010010609403252602\n",
      "( b ) by the insertion of the following subsection after subsection ( 1 ) of section 5 : xxeos\n",
      "\n",
      "( b ) tríd an bhfo - alt seo a leanas a chur isteach i ndiaidh fho - alt ( 1 ) d'alt 5 :\n",
      "\n",
      "( b ) tríd an bhfo - alt seo a leanas a chur isteach i ndiaidh fho - alt ( 1 ) d'alt 5 : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.009908217005431652\n",
      "( g ) in subsection ( 9 ) , by substituting for paragraph ( a ) the following : xxeos\n",
      "\n",
      "( g ) i bhfo - alt ( 9 ) , tríd an méid seo a leanas a chur in ionad mhír ( a ) :\n",
      "\n",
      "( g ) i bhfo - alt ( 9 ) , tríd an méid seo a leanas a chur in ionad mhír ( a ) : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.009814311750233173\n",
      "( a ) in paragraph 1 of xxmaj part xxup i , “ £ 158 10s . ” shall be substituted for “ £ 132 ” ( inserted by section 16 of the xxmaj increase xxmaj act of 1961 ) , xxeos\n",
      "\n",
      "( a ) i mír 1 de xxmaj chuid xxup i , cuirfear “ £ 158 10s . ” in ionad “ £ 132 ” ( a cuireadh isteach le halt 16 d'acht xxmaj mhéadú 1961 ) ,\n",
      "\n",
      "( a ) i mír 1 de xxmaj chuid xxup i , cuirfear “ £ 158 10s . ” in ionad “ £ 132 ” ( a cuireadh isteach le halt 16 d'acht xxmaj mhéadú 1961 ) , xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.00944268237799406\n",
      "48 . — xxmaj the xxmaj taxes xxmaj consolidation xxmaj act 1997 is amended by the insertion of the following section after section xxunk : xxeos\n",
      "\n",
      "48 . — xxmaj leasaítear an tacht xxmaj comhdhlúite xxmaj cánacha 1997 tríd an alt seo a leanas a chur isteach i ndiaidh alt xxunk :\n",
      "\n",
      "48 . — xxmaj leasaítear an tacht xxmaj comhdhlúite xxmaj cánacha 1997 tríd an alt seo a leanas a chur isteach i ndiaidh alt xxunk : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.009387349709868431\n",
      "( ii ) on conviction on indictment , to a fine not exceeding £ 100 , xxrep 3 0 or to imprisonment for a term not exceeding five years , or both . xxeos\n",
      "\n",
      "( ii ) ar é a chiontú ar díotáil , fíneáil nach mó ná £ 100 , xxrep 3 0 nó príosúnacht ar feadh téarma nach faide ná cúig bliana , nó iad araon , a chur air .\n",
      "\n",
      "( ii ) ar é a chiontú ar díotáil , fíneáil nach mó ná £ 100 , xxrep 3 0 nó príosúnacht ar feadh téarma nach faide ná cúig bliana , nó iad araon , a chur air . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.009340242482721806\n",
      "( xxrep 3 i ) by inserting the following after paragraph ( c ) : xxeos\n",
      "\n",
      "( xxrep 3 i ) tríd an méid seo a leanas a chur isteach i ndiaidh mhír ( c ) :\n",
      "\n",
      "( xxrep 3 i ) tríd an méid seo a leanas a chur isteach i ndiaidh mhír ( c ) : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.009309934452176094\n",
      "( e ) the substitution of the following subsection for subsection ( 12 ) : xxeos\n",
      "\n",
      "( e ) tríd an bhfo - alt seo a leanas a chur in ionad fho - alt ( 12 ) :\n",
      "\n",
      "( e ) tríd an bhfo - alt seo a leanas a chur in ionad fho - alt ( 12 ) : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.009225739166140556\n",
      "( a ) in subsection ( 1 ) , by substituting for paragraph ( a ) the following : xxeos\n",
      "\n",
      "( a ) i bhfo - alt ( 1 ) , tríd an méid seo a leanas a chur in ionad mhír ( a ) :\n",
      "\n",
      "( a ) i bhfo - alt ( 1 ) , tríd an méid seo a leanas a chur in ionad mhír ( a ) : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.00919419713318348\n",
      "1 . — this xxmaj act may be cited as the xxmaj petroleum and xxmaj other xxmaj minerals xxmaj development xxmaj act , 1960 . xxeos\n",
      "\n",
      "1 . — féadfar an tacht um xxmaj fhorbairt xxmaj pheitriliaim agus xxmaj mianraí xxmaj eile , 1960 , a ghairm den xxmaj acht seo .\n",
      "\n",
      "1 . — féadfar an tacht um xxmaj fhorbairt xxmaj pheitriliaim agus xxmaj mianraí xxmaj eile , 1960 , a ghairm den xxmaj acht seo . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.00910924281924963\n",
      "“ contractor ” has the meaning assigned to it by section 5 ; xxeos\n",
      "\n",
      "tá le “ conraitheoir ” an bhrí a shanntar dó le halt 5 ;\n",
      "\n",
      "tá le “ conraitheoir ” an bhrí a shanntar dó le halt 5 ; xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.008987490087747574\n",
      "xxmaj number 30 / 2007 : xxup part 1 xxmaj preliminary and xxmaj general xxmaj home xxeos\n",
      "\n",
      "xxmaj uimhir 30 / 2007 : xxup cuid 1 xxmaj réamhráiteach agus xxmaj ginearálta\n",
      "\n",
      "xxmaj uimhir 30 / 2007 : xxup cuid 1 xxmaj réamhráiteach agus xxmaj ginearálta xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.008874460123479366\n",
      "“ the xxmaj act of 1952 ” means the xxmaj housing ( amendment ) xxmaj act , 1952 ( no . 16 of 1952 ) ; xxeos\n",
      "\n",
      "ciallaíonn “ acht 1952 ” xxmaj acht na dtithe ( leasú ) , 1952 ( uimh. 16 de 1952 ) ;\n",
      "\n",
      "ciallaíonn “ acht 1952 ” xxmaj acht na dtithe ( leasú ) , 1952 ( uimh. 16 de 1952 ) ; xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.008825826458632946\n",
      "( 2 ) xxmaj this section shall come into operation on the 6th day of xxmaj april , 1983 . xxeos\n",
      "\n",
      "( 2 ) xxmaj tiocfaidh an t - alt seo i ngníomh an 6ú lá d'aibreán , 1983 .\n",
      "\n",
      "( 2 ) xxmaj tiocfaidh an t - alt seo i ngníomh an 6ú lá d'aibreán , 1983 . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.008588433265686035\n",
      "66 . — ( 1 ) xxmaj section 766 of the xxmaj principal xxmaj act is amended — xxeos\n",
      "\n",
      "66 . — ( 1 ) xxmaj leasaítear alt 766 den phríomh - acht —\n",
      "\n",
      "66 . — ( 1 ) xxmaj leasaítear alt 766 den phríomh - acht — xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.008027076721191406\n",
      "“ the xxmaj act of 1976 ” means the xxmaj finance xxmaj act , 1976 ; xxeos\n",
      "\n",
      "ciallaíonn “ acht 1976 ” an tacht xxmaj airgeadais , 1976 ;\n",
      "\n",
      "ciallaíonn “ acht 1976 ” an tacht xxmaj airgeadais , 1976 ; xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.007852774113416672\n",
      "xxmaj type of xxmaj household ( 8) xxmaj religion ( 11 ) censusyear xxeos\n",
      "\n",
      "xxmaj cineál xxmaj teaghlaigh ( 8) xxmaj creideamh ( 11 ) bliaindaonáirimh\n",
      "\n",
      "xxmaj cineál xxmaj teaghlaigh ( 8) xxmaj creideamh ( 11 ) bliaindaonáirimh xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.007078517694026232\n",
      "“ ( xxrep 3 i ) in the case of a person — xxeos\n",
      "\n",
      "“ ( xxrep 3 i ) i gcás duine —\n",
      "\n",
      "“ ( xxrep 3 i ) i gcás duine — xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.007061743643134832\n",
      "28 . — ( 1 ) xxmaj section 224 of the xxmaj principal xxmaj act is hereby amended by the substitution for subsections ( 1 ) and ( 2 ) of the following subsections : xxeos\n",
      "\n",
      "28 . — ( 1 ) xxmaj leasaítear leis seo alt 224 den phríomh - acht trí na fo - ailt seo a leanas a chur in ionad fho - ailt ( 1 ) agus ( 2 ) :\n",
      "\n",
      "28 . — ( 1 ) xxmaj leasaítear leis seo alt 224 den phríomh - acht trí na fo - ailt seo a leanas a chur in ionad fho - ailt ( 1 ) agus ( 2 ) : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.007009965367615223\n",
      "xxmaj the xxmaj finance ( customs xxmaj duties ) ( no . 4 ) xxmaj act , 1932 ( no . 34 of 1932 ) , section 3 . xxeos\n",
      "\n",
      "xxmaj an tacht xxmaj airgid ( diúitéthe xxmaj custum ) ( uimh. 4 ) , 1932 ( uimh. 34 de 1932 ) , alt 3 .\n",
      "\n",
      "xxmaj an tacht xxmaj airgid ( diúitéthe xxmaj custum ) ( uimh. 4 ) , 1932 ( uimh. 34 de 1932 ) , alt 3 . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.006748199462890625\n",
      "( xxrep 3 i ) in subsection ( 5 ) — xxeos\n",
      "\n",
      "( xxrep 3 i ) i bhfo - alt ( 5 ) —\n",
      "\n",
      "( xxrep 3 i ) i bhfo - alt ( 5 ) — xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.006730107590556145\n",
      "2 . — this xxmaj act may be cited as the xxmaj restrictive xxmaj trade xxmaj practices ( confirmation of xxmaj order ) ( no . 3 ) xxmaj act , 1956 . xxeos\n",
      "\n",
      "2 . — féadfar an tacht um xxmaj chleachtais xxmaj srianta xxmaj trádála ( ordú a xxmaj dhaingniú ) ( uimh. 3 ) , 1956 , a ghairm den xxmaj acht seo .\n",
      "\n",
      "2 . — féadfar an tacht um xxmaj chleachtais xxmaj srianta xxmaj trádála ( ordú a xxmaj dhaingniú ) ( uimh. 3 ) , 1956 , a ghairm den xxmaj acht seo . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.006541644688695669\n",
      "xxmaj nature of xxmaj occupancy ( 8) xxmaj towns by xxmaj size ( 205 ) censusyear xxeos\n",
      "\n",
      "xxmaj cineál xxmaj seilbhe ( 8) xxmaj bailte de réir xxmaj méide ( 205 ) bliaindaonáirimh\n",
      "\n",
      "xxmaj cineál xxmaj seilbhe ( 8) xxmaj bailte de réir xxmaj méide ( 205 ) bliaindaonáirimh xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.006427327636629343\n",
      "( a ) by the substitution of “ £ 150 ” for “ twenty pounds ” in paragraph ( a ) , and xxeos\n",
      "\n",
      "( a ) trí “ £ 150 ” a chur in ionad “ fiche punt ” i mír ( a ) , agus\n",
      "\n",
      "( a ) trí “ £ 150 ” a chur in ionad “ fiche punt ” i mír ( a ) , agus xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.005883284844458103\n",
      "35 . — xxmaj section 90 of the xxmaj corporation xxmaj tax xxmaj act , 1976 , is hereby amended by the addition to subsection ( 4 ) ( as amended by the xxmaj act of 1978 ) of the following proviso : xxeos\n",
      "\n",
      "35 . — leasaítear leis seo alt 90 den xxmaj acht xxmaj cánach xxmaj corparáide , 1976 , tríd an gcoinníoll seo a leanas a chur le fo - alt ( 4 ) ( arna leasú le hacht 1978 ) :\n",
      "\n",
      "35 . — leasaítear leis seo alt 90 den xxmaj acht xxmaj cánach xxmaj corparáide , 1976 , tríd an gcoinníoll seo a leanas a chur le fo - alt ( 4 ) ( arna leasú le hacht 1978 ) : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.005392891820520163\n",
      "81 . — section 12 of the xxmaj principal xxmaj act is hereby amended — xxeos\n",
      "\n",
      "81 . — leasaítear leis seo alt 12 den phríomh - acht —\n",
      "\n",
      "81 . — leasaítear leis seo alt 12 den phríomh - acht — xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.005264571402221918\n",
      "2 . — ( 1 ) xxmaj this xxmaj act may be cited as the xxmaj rent xxmaj restrictions ( temporary xxmaj provisions ) ( continuance ) xxmaj act , 1981 . xxeos\n",
      "\n",
      "2 . — ( 1 ) xxmaj féadfar an tacht xxmaj srianta xxmaj cíosa ( forálacha xxmaj sealadacha ) ( buanú ) , 1981 , a ghairm den xxmaj acht seo .\n",
      "\n",
      "2 . — ( 1 ) xxmaj féadfar an tacht xxmaj srianta xxmaj cíosa ( forálacha xxmaj sealadacha ) ( buanú ) , 1981 , a ghairm den xxmaj acht seo . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.004902585409581661\n",
      "xxmaj sex ( 3 ) xxmaj disability xxmaj type ( 14 ) censusyear xxeos\n",
      "\n",
      "xxmaj inscne ( 3 ) xxmaj cineál an xxmaj mhàchumais ( 14 ) bliaindaonáirimh\n",
      "\n",
      "xxmaj inscne ( 3 ) xxmaj cineál an xxmaj mhàchumais ( 14 ) bliaindaonáirimh xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.004811659920960665\n",
      "4 . — the xxmaj act of 1976 is hereby amended by the substitution of the following section for section 29 : xxeos\n",
      "\n",
      "4 . — leasaítear leis seo xxmaj acht 1976 tríd an alt seo a leanas a chur in ionad alt 29 :\n",
      "\n",
      "4 . — leasaítear leis seo xxmaj acht 1976 tríd an alt seo a leanas a chur in ionad alt 29 : xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.004624366760253906\n",
      "7 . — ( 1 ) xxmaj this xxmaj act may be cited as the xxmaj postal and xxmaj telecommunications xxmaj services ( amendment ) xxmaj act , 1984 . xxeos\n",
      "\n",
      "7 . — ( 1 ) xxmaj féadfar an tacht xxmaj seirbhísí xxmaj poist agus xxmaj teileachumarsáide ( leasú ) , 1984 , a ghairm den xxmaj acht seo .\n",
      "\n",
      "7 . — ( 1 ) xxmaj féadfar an tacht xxmaj seirbhísí xxmaj poist agus xxmaj teileachumarsáide ( leasú ) , 1984 , a ghairm den xxmaj acht seo . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.00429901946336031\n",
      "xxmaj sex ( 3 ) xxmaj aggregate xxmaj town or xxmaj rural xxmaj area ( 3 ) xxmaj province xxmaj county or xxmaj city ( 44 ) xxeos\n",
      "\n",
      "xxmaj inscne ( 3 ) xxmaj ceantar xxmaj iomlán xxmaj bhaile nó xxmaj ceantar xxmaj iomlán xxmaj tuaithe ( 3 ) xxmaj cúige , xxmaj contae nó xxmaj cathair ( 44 )\n",
      "\n",
      "xxmaj inscne ( 3 ) xxmaj ceantar xxmaj iomlán xxmaj bhaile nó xxmaj ceantar xxmaj iomlán xxmaj tuaithe ( 3 ) xxmaj cúige , xxmaj contae nó xxmaj cathair ( 44 ) xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.0040727341547608376\n",
      "101 . — ( 1 ) xxmaj the xxmaj principal xxmaj act is amended — xxeos\n",
      "\n",
      "101 . — ( 1 ) xxmaj leasaítear an príomh - acht —\n",
      "\n",
      "101 . — ( 1 ) xxmaj leasaítear an príomh - acht — xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.0039478447288274765\n",
      "56 . — xxmaj section 208 of the xxmaj principal xxmaj act is amended — xxeos\n",
      "\n",
      "56 . — xxmaj leasaítear alt 208 den phríomh - acht —\n",
      "\n",
      "56 . — xxmaj leasaítear alt 208 den phríomh - acht — xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.003833770751953125\n",
      "xxmaj number 5 / 2005 : xxup part 5 xxmaj capital xxmaj acquisitions xxmaj tax xxmaj home xxeos\n",
      "\n",
      "xxmaj uimhir 5 / 2005 : xxup cuid 5 xxmaj cáin xxmaj fháltas xxmaj caipitiúil\n",
      "\n",
      "xxmaj uimhir 5 / 2005 : xxup cuid 5 xxmaj cáin xxmaj fháltas xxmaj caipitiúil xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.0036608653608709574\n",
      "12 . — ( 1 ) xxmaj this xxmaj act may be cited as the xxmaj adoption xxmaj act , 1964 . xxeos\n",
      "\n",
      "12 . — ( 1 ) xxmaj féadfar an tacht xxmaj uchtála , 1964 , a ghairm den xxmaj acht seo .\n",
      "\n",
      "12 . — ( 1 ) xxmaj féadfar an tacht xxmaj uchtála , 1964 , a ghairm den xxmaj acht seo . xxeos\n",
      "\n",
      "\n",
      "LOSS: 0.0034489380195736885\n",
      "xxmaj number 2 / 1 xxrep 3 9 : xxup part 5 xxmaj residential xxmaj property xxmaj tax xxmaj home xxeos\n",
      "\n",
      "xxmaj uimhir 2 / 1 xxrep 3 9 : xxup cuid 5 xxmaj cáin xxmaj mhaoine xxmaj cónaithe\n",
      "\n",
      "xxmaj uimhir 2 / 1 xxrep 3 9 : xxup cuid 5 xxmaj cáin xxmaj mhaoine xxmaj cónaithe xxeos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_losses = 50\n",
    "top_losses = loss_sorted_idxs[-n_losses:]\n",
    "\n",
    "for i in top_losses:\n",
    "    print(f'LOSS: {float(loss_ls[i])}')\n",
    "    print(en_ls[i])\n",
    "    print()\n",
    "    print(ga_trg_ls[i])\n",
    "    print()\n",
    "    print(ga_pred_ls[i])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model=pt_Transformer(src_vcbsz=n_x_vocab, trg_vcbsz=n_y_vocab, d_model=d_model, d_inner=d_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_quantized_model = torch.quantization.quantize_dynamic(tmp_model.to('cpu'), {torch.nn.Linear}, dtype=torch.qint8)\n",
    "#print(tmp_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model.to('cpu'), \n",
    "    {torch.nn.Linear}, \n",
    "    dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_pth_q8 = 'models/paracrawl_en_ga_5e_5e-4_5e_1e-5_v0.2_exp4_no_opt_quantized'\n",
    "state_dict=torch.load(mod_pth_q8, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_quantized_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 346.679325\n",
      "Size (MB): 225.109791\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(learn.model)\n",
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.model=quantized_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model=quantized_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.dls.to('cpu')\n",
    "learn.dls.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos dia dia duit , conas atá tú ? xxeos'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model.cpu(), \"hello, how are you?\", dls.vocab[1], to_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-ec7698cc6331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hello, how are you?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-a365c4034e4f>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, sentence, vocab)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_sentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtranslated_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-a365c4034e4f>\u001b[0m in \u001b[0;36mforward_model\u001b[0;34m(model, src, tgt)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#return output.squeeze(0).to('cpu')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-5c9c38cb5d20>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m                         src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0menc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_tfmr_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_tfmr_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Test whether fp16 is being used or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-cf166625c2a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#pos = torch.arange(0, inp.size(1), device=inp.device).float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_sz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "generate(learn.model, \"hello, how are you?\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/morgan/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m(1814)\u001b[0;36membedding\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1812 \u001b[0;31m        \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1813 \u001b[0;31m        \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1814 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1815 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1816 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> weight.device\n",
      "device(type='cpu')\n",
      "ipdb> input.device\n",
      "device(type='cuda', index=0)\n",
      "ipdb> u\n",
      "> \u001b[0;32m/home/morgan/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m(126)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    124 \u001b[0;31m        return F.embedding(\n",
      "\u001b[0m\u001b[0;32m    125 \u001b[0;31m            \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 126 \u001b[0;31m            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0m\u001b[0;32m    127 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    128 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/home/morgan/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m(722)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    720 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    721 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 722 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    723 \u001b[0;31m        for hook in itertools.chain(\n",
      "\u001b[0m\u001b[0;32m    724 \u001b[0;31m                \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> input.device\n",
      "*** AttributeError: 'tuple' object has no attribute 'device'\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-32-cf166625c2a7>\u001b[0m(13)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m        \u001b[0;31m#pos = torch.arange(0, inp.size(1), device=inp.device).float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_sz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> inp.device\n",
      "device(type='cuda', index=0)\n",
      "ipdb> u\n",
      "> \u001b[0;32m/home/morgan/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m(722)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    720 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    721 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 722 \u001b[0;31m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    723 \u001b[0;31m        for hook in itertools.chain(\n",
      "\u001b[0m\u001b[0;32m    724 \u001b[0;31m                \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-63-5c9c38cb5d20>\u001b[0m(20)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     18 \u001b[0;31m                        src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 20 \u001b[0;31m        \u001b[0menc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_tfmr_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_tfmr_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m        \u001b[0;31m# Test whether fp16 is being used or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> src.device\n",
      "device(type='cuda', index=0)\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-82-a365c4034e4f>\u001b[0m(27)\u001b[0;36mforward_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     25 \u001b[0;31m    \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m    \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m    \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m    \u001b[0;31m#return output.squeeze(0).to('cpu')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> tgt.device\n",
      "device(type='cuda', index=0)\n",
      "ipdb> src.device\n",
      "device(type='cuda', index=0)\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-82-a365c4034e4f>\u001b[0m(11)\u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m    \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m    \u001b[0;32mwhile\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_sentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 11 \u001b[0;31m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m        \u001b[0mtranslated_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> sentence.device\n",
      "device(type='cpu')\n",
      "ipdb> model.device\n",
      "*** torch.nn.modules.module.ModuleAttributeError: 'pt_Transformer' object has no attribute 'device'\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj an féidir leat a insint dúinn i gcás go bhfuil an stáisiún bus le do thoil ? xxeos'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"Can you tell we where the bus station is please?\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj inné xxunk sé , ach amárach beidh amárach a bheith an - xxunk xxeos'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"Yesterday it rained, but tomorrow will be very sunny\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj bhí mé lá mór , xxmaj is é mo aistritheoir ag obair xxeos'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"I had a great day, my translator is working\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj mar sin , tá sé seo scéal ar fad faoi conas mo shaol fuair smeach xxunk síos , mar sin mhaith liom a ghlacadh nóiméad díreach suí ceart go bhfuil , beidh mé go léir faoi conas a tháinig mé an xxunk úr xxeos'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"So this is a story all about how my life got flip turned \\\n",
    "upside down, so I'd like to take a minute just sit right there, I'll you all about how I became the fresh prince\\\n",
    "of belair\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos madra xxeos'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"dog\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos cat cat cat xxeos'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"cat\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos crann crann xxeos'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"tree\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos foirgneamh tógála xxeos'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"building\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj cathair cathair cathair xxeos'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"city\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos bean xxeos'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"woman\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos fear xxeos'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"man\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos seacláide xxeos'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"chocolate\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos spásárthach spásárthach xxeos'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(learn.model, \"spaceship\", dls.vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://forums.fast.ai/t/fastai-v2-text/53529/334\n",
    "from fastai2.text.all import *\n",
    "\n",
    "defaults.device = torch.device('cpu')\n",
    "path = Path('.')\n",
    "learner = load_learner(\"./export.pkl\")\n",
    "\n",
    "f = open(\"/tmp/test.txt\", \"r\")\n",
    "test_file_contents = f.read()\n",
    "\n",
    "_, _, losses = learner.predict(test_file_contents)\n",
    "cats = [learner.dls.categorize.decode(i) for i in range(len(losses))]\n",
    "\n",
    "predictions = sorted(\n",
    "    zip(cats, map(float, losses)),\n",
    "    key=lambda p: p[1],\n",
    "    reverse=True\n",
    ")\n",
    "print(predictions)\n",
    "\n",
    "# OR\n",
    "\n",
    "items = pd.read_csv(\"/tmp/test.txt\", sep = '\\t')\n",
    "test_dl = learner.dls.test_dl(items.values)\n",
    "\n",
    "learner.get_preds(dl=test_dl, with_decoded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
